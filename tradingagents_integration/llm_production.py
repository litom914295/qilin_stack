"""
TradingAgentsç”Ÿäº§çº§LLMçœŸå®é›†æˆ
è§£å†³å½“å‰Mockè°ƒç”¨é—®é¢˜,æ”¯æŒå¤šç§LLMæä¾›å•†

é—®é¢˜: 
- å½“å‰tradingagents_integration/real_integration.pyä¸­LLMè°ƒç”¨ä¸ºMock
- é»˜è®¤API keyä¸ºç©º,å¯¼è‡´client=None,è¿”å›å›ºå®šå­—ç¬¦ä¸²
- å¤šæ™ºèƒ½ä½“åä½œçš„æ ¸å¿ƒä»·å€¼æ— æ³•å‘æŒ¥

è§£å†³æ–¹æ¡ˆ:
1. çœŸå®LLM APIè°ƒç”¨ (OpenAI/Anthropic/Azure)
2. Tokenä½¿ç”¨ç»Ÿè®¡å’Œæˆæœ¬è¿½è¸ª
3. é”™è¯¯å¤„ç†å’Œè‡ªåŠ¨é‡è¯•
4. ç¼“å­˜æœºåˆ¶å‡å°‘é‡å¤è°ƒç”¨

ä½¿ç”¨ç¤ºä¾‹:
    from tradingagents_integration.llm_production import ProductionLLMManager
    
    # åˆ›å»ºLLMç®¡ç†å™¨
    manager = ProductionLLMManager()
    
    # Agentè°ƒç”¨LLM
    result = await manager.call_agent(
        agent_name="sentiment",
        task="åˆ†æå¸‚åœºæƒ…ç»ª",
        context={"symbol": "000001.SZ", "date": "2024-01-15"}
    )
    
    # æŸ¥çœ‹ç»Ÿè®¡
    stats = manager.get_usage_report()
    print(f"æ€»æˆæœ¬: ${stats['total_cost_usd']}")
"""

import os
import json
import asyncio
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field
from datetime import datetime
import logging
from pathlib import Path
import hashlib

logger = logging.getLogger(__name__)


# ============================================================================
# LLMæä¾›å•†åŸºç±»
# ============================================================================

class LLMProvider:
    """LLMæä¾›å•†åŸºç±»"""
    
    def __init__(self, model: str):
        self.model = model
        self.call_count = 0
    
    async def call(self, 
                  system_prompt: str,
                  user_prompt: str,
                  **kwargs) -> str:
        """è°ƒç”¨LLM"""
        raise NotImplementedError()
    
    def get_cost(self, input_tokens: int, output_tokens: int) -> float:
        """è®¡ç®—æˆæœ¬"""
        raise NotImplementedError()


# ============================================================================
# OpenAIæä¾›å•†
# ============================================================================

class OpenAIProvider(LLMProvider):
    """OpenAI APIæä¾›å•†"""
    
    # å®šä»· (ç¾å…ƒ/1K tokens) - 2024å¹´ä»·æ ¼
    PRICING = {
        "gpt-4-turbo": {"input": 0.01, "output": 0.03},
        "gpt-4": {"input": 0.03, "output": 0.06},
        "gpt-4-32k": {"input": 0.06, "output": 0.12},
        "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015},
        "gpt-3.5-turbo-16k": {"input": 0.001, "output": 0.002},
    }
    
    def __init__(self, api_key: str, model: str = "gpt-4-turbo", api_base: Optional[str] = None):
        super().__init__(model)
        
        try:
            from openai import AsyncOpenAI
            self.client = AsyncOpenAI(
                api_key=api_key,
                base_url=api_base
            )
            logger.info(f"âœ… OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ: {model}")
        except ImportError:
            raise ImportError("è¯·å®‰è£…openaiåŒ…: pip install openai")
    
    async def call(self,
                  system_prompt: str,
                  user_prompt: str,
                  temperature: float = 0.7,
                  max_tokens: int = 1500,
                  **kwargs) -> tuple[str, int, int]:
        """
        è°ƒç”¨OpenAI API
        
        Returns:
            (response_text, input_tokens, output_tokens)
        """
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs
            )
            
            self.call_count += 1
            
            content = response.choices[0].message.content
            input_tokens = response.usage.prompt_tokens
            output_tokens = response.usage.completion_tokens
            
            return content, input_tokens, output_tokens
            
        except Exception as e:
            logger.error(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def get_cost(self, input_tokens: int, output_tokens: int) -> float:
        """è®¡ç®—æˆæœ¬"""
        pricing = self.PRICING.get(self.model, self.PRICING["gpt-4-turbo"])
        input_cost = (input_tokens / 1000) * pricing["input"]
        output_cost = (output_tokens / 1000) * pricing["output"]
        return input_cost + output_cost


# ============================================================================
# Anthropicæä¾›å•†
# ============================================================================

class AnthropicProvider(LLMProvider):
    """Anthropic Claude APIæä¾›å•†"""
    
    PRICING = {
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
    }
    
    def __init__(self, api_key: str, model: str = "claude-3-sonnet-20240229"):
        super().__init__(model)
        
        try:
            import anthropic
            self.client = anthropic.AsyncAnthropic(api_key=api_key)
            logger.info(f"âœ… Anthropicå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ: {model}")
        except ImportError:
            raise ImportError("è¯·å®‰è£…anthropicåŒ…: pip install anthropic")
    
    async def call(self,
                  system_prompt: str,
                  user_prompt: str,
                  temperature: float = 0.7,
                  max_tokens: int = 1500,
                  **kwargs) -> tuple[str, int, int]:
        """è°ƒç”¨Anthropic API"""
        try:
            response = await self.client.messages.create(
                model=self.model,
                max_tokens=max_tokens,
                temperature=temperature,
                system=system_prompt,
                messages=[
                    {"role": "user", "content": user_prompt}
                ]
            )
            
            self.call_count += 1
            
            content = response.content[0].text
            input_tokens = response.usage.input_tokens
            output_tokens = response.usage.output_tokens
            
            return content, input_tokens, output_tokens
            
        except Exception as e:
            logger.error(f"Anthropic APIè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def get_cost(self, input_tokens: int, output_tokens: int) -> float:
        """è®¡ç®—æˆæœ¬"""
        pricing = self.PRICING.get(self.model, self.PRICING["claude-3-sonnet-20240229"])
        input_cost = (input_tokens / 1000) * pricing["input"]
        output_cost = (output_tokens / 1000) * pricing["output"]
        return input_cost + output_cost


# ============================================================================
# ç”Ÿäº§çº§LLMç®¡ç†å™¨
# ============================================================================

@dataclass
class UsageStats:
    """ä½¿ç”¨ç»Ÿè®¡"""
    total_calls: int = 0
    total_input_tokens: int = 0
    total_output_tokens: int = 0
    total_cost_usd: float = 0.0
    agent_stats: Dict[str, Dict] = field(default_factory=dict)
    start_time: datetime = field(default_factory=datetime.now)


class ProductionLLMManager:
    """
    ç”Ÿäº§çº§LLMç®¡ç†å™¨
    
    åŠŸèƒ½:
    - æ”¯æŒå¤šç§LLMæä¾›å•† (OpenAI/Anthropic/Azure)
    - Tokenä½¿ç”¨ç»Ÿè®¡å’Œæˆæœ¬è¿½è¸ª
    - ç®€å•ç¼“å­˜å‡å°‘é‡å¤è°ƒç”¨
    - é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
    - Agentä¸“ç”¨Promptæ¨¡æ¿
    """
    
    def __init__(self,
                 provider: Optional[str] = None,
                 model: Optional[str] = None,
                 api_key: Optional[str] = None,
                 api_base: Optional[str] = None,
                 cache_enabled: bool = True):
        """
        åˆå§‹åŒ–LLMç®¡ç†å™¨
        
        Args:
            provider: LLMæä¾›å•† (openai/anthropic/azure),é»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–
            model: æ¨¡å‹åç§°,é»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–
            api_key: APIå¯†é’¥,é»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–
            api_base: APIåŸºç¡€URL (å¯é€‰)
            cache_enabled: æ˜¯å¦å¯ç”¨ç¼“å­˜
        """
        # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
        self.provider_name = provider or os.getenv("LLM_PROVIDER", "openai")
        self.api_key = api_key or self._get_api_key()
        self.api_base = api_base or os.getenv("OPENAI_API_BASE")
        
        # æ£€æŸ¥API key
        if not self.api_key:
            raise ValueError(
                "æœªæ‰¾åˆ°LLM APIå¯†é’¥!\n"
                "è¯·è®¾ç½®ç¯å¢ƒå˜é‡: OPENAI_API_KEY æˆ– ANTHROPIC_API_KEY\n"
                "æˆ–åœ¨åˆå§‹åŒ–æ—¶ä¼ å…¥ api_key å‚æ•°"
            )
        
        # åˆ›å»ºæä¾›å•†
        self.provider = self._create_provider(model)
        
        # ä½¿ç”¨ç»Ÿè®¡
        self.stats = UsageStats()
        
        # ç¼“å­˜ (ç®€å•çš„å†…å­˜ç¼“å­˜)
        self.cache_enabled = cache_enabled
        self.cache: Dict[str, tuple[str, datetime]] = {}
        self.cache_ttl = 3600  # ç¼“å­˜1å°æ—¶
        
        logger.info(f"âœ… LLMç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ: {self.provider_name} ({self.provider.model})")
    
    def _get_api_key(self) -> Optional[str]:
        """ä»ç¯å¢ƒå˜é‡è·å–API key"""
        if self.provider_name == "openai":
            return os.getenv("OPENAI_API_KEY")
        elif self.provider_name == "anthropic":
            return os.getenv("ANTHROPIC_API_KEY")
        elif self.provider_name == "azure":
            return os.getenv("AZURE_API_KEY")
        return None
    
    def _create_provider(self, model: Optional[str]) -> LLMProvider:
        """åˆ›å»ºLLMæä¾›å•†"""
        if self.provider_name == "openai":
            default_model = "gpt-4-turbo"
            model = model or os.getenv("LLM_MODEL", default_model)
            return OpenAIProvider(self.api_key, model, self.api_base)
        
        elif self.provider_name == "anthropic":
            default_model = "claude-3-sonnet-20240229"
            model = model or os.getenv("LLM_MODEL", default_model)
            return AnthropicProvider(self.api_key, model)
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {self.provider_name}")
    
    def _get_cache_key(self, agent_name: str, task: str, context_hash: str) -> str:
        """ç”Ÿæˆç¼“å­˜key"""
        key = f"{agent_name}:{task}:{context_hash}"
        return hashlib.md5(key.encode()).hexdigest()
    
    def _check_cache(self, cache_key: str) -> Optional[str]:
        """æ£€æŸ¥ç¼“å­˜"""
        if not self.cache_enabled:
            return None
        
        if cache_key in self.cache:
            content, timestamp = self.cache[cache_key]
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if (datetime.now() - timestamp).total_seconds() < self.cache_ttl:
                logger.debug(f"âœ… ç¼“å­˜å‘½ä¸­: {cache_key[:8]}...")
                return content
            else:
                # æ¸…é™¤è¿‡æœŸç¼“å­˜
                del self.cache[cache_key]
        
        return None
    
    def _set_cache(self, cache_key: str, content: str):
        """è®¾ç½®ç¼“å­˜"""
        if self.cache_enabled:
            self.cache[cache_key] = (content, datetime.now())
    
    async def call_agent(self,
                        agent_name: str,
                        task: str,
                        context: Dict[str, Any],
                        use_cache: bool = True,
                        **llm_kwargs) -> str:
        """
        Agentä¸“ç”¨LLMè°ƒç”¨
        
        Args:
            agent_name: Agentåç§° (sentiment/macroeconomic/etc)
            task: ä»»åŠ¡æè¿°
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ (symbol, date, market_dataç­‰)
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            **llm_kwargs: LLMå‚æ•° (temperature, max_tokensç­‰)
            
        Returns:
            LLMå“åº”æ–‡æœ¬
        """
        # æ£€æŸ¥ç¼“å­˜
        context_str = json.dumps(context, sort_keys=True)
        context_hash = hashlib.md5(context_str.encode()).hexdigest()
        cache_key = self._get_cache_key(agent_name, task, context_hash)
        
        if use_cache:
            cached = self._check_cache(cache_key)
            if cached:
                return cached
        
        # æ„å»ºPrompt
        system_prompt = self._get_agent_system_prompt(agent_name)
        user_prompt = self._format_agent_task(agent_name, task, context)
        
        # è°ƒç”¨LLM
        try:
            response, input_tokens, output_tokens = await self.provider.call(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                **llm_kwargs
            )
            
            # æ›´æ–°ç»Ÿè®¡
            cost = self.provider.get_cost(input_tokens, output_tokens)
            self._update_stats(agent_name, input_tokens, output_tokens, cost)
            
            # è®¾ç½®ç¼“å­˜
            self._set_cache(cache_key, response)
            
            return response
            
        except Exception as e:
            logger.error(f"âŒ Agent {agent_name} LLMè°ƒç”¨å¤±è´¥: {e}")
            # è¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸,ä¿è¯ç³»ç»Ÿå¯ç”¨
            return f"[LLMè°ƒç”¨å¤±è´¥: {str(e)}]"
    
    def _update_stats(self, agent_name: str, input_tokens: int, output_tokens: int, cost: float):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats.total_calls += 1
        self.stats.total_input_tokens += input_tokens
        self.stats.total_output_tokens += output_tokens
        self.stats.total_cost_usd += cost
        
        # Agentçº§ç»Ÿè®¡
        if agent_name not in self.stats.agent_stats:
            self.stats.agent_stats[agent_name] = {
                "calls": 0,
                "input_tokens": 0,
                "output_tokens": 0,
                "cost": 0.0
            }
        
        agent_stat = self.stats.agent_stats[agent_name]
        agent_stat["calls"] += 1
        agent_stat["input_tokens"] += input_tokens
        agent_stat["output_tokens"] += output_tokens
        agent_stat["cost"] += cost
    
    def _get_agent_system_prompt(self, agent_name: str) -> str:
        """è·å–Agentçš„ç³»ç»ŸPrompt"""
        prompts = {
            "sentiment": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¸‚åœºæƒ…ç»ªåˆ†æå¸ˆã€‚
ä½ çš„ä»»åŠ¡æ˜¯åˆ†ææ–°é—»ã€ç¤¾äº¤åª’ä½“å’Œå¸‚åœºæ•°æ®,è¯„ä¼°æŠ•èµ„è€…å¯¹ç‰¹å®šè‚¡ç¥¨çš„æƒ…ç»ªå€¾å‘ã€‚

åˆ†æç»´åº¦:
1. æ–°é—»æƒ…ç»ª: æ­£é¢/è´Ÿé¢/ä¸­æ€§
2. ç¤¾äº¤åª’ä½“çƒ­åº¦
3. å¸‚åœºååº” (ä»·æ ¼ã€æˆäº¤é‡)
4. æƒ…ç»ªå¼ºåº¦ (0-1)

è¾“å‡ºæ ¼å¼: JSON
{"sentiment": "positive/negative/neutral", "confidence": 0.0-1.0, "signal": "BUY/SELL/HOLD", "reasoning": "è¯¦ç»†åˆ†æ"}
""",

            "macroeconomic": """ä½ æ˜¯ä¸€ä¸ªå®è§‚ç»æµåˆ†æä¸“å®¶ã€‚
ä½ çš„ä»»åŠ¡æ˜¯åˆ†æå®è§‚ç»æµæ•°æ®å’Œæ”¿ç­–,è¯„ä¼°å¯¹è‚¡å¸‚å’Œç‰¹å®šè¡Œä¸šçš„å½±å“ã€‚

åˆ†æç»´åº¦:
1. è´§å¸æ”¿ç­– (åˆ©ç‡ã€æµåŠ¨æ€§)
2. è´¢æ”¿æ”¿ç­– (è´¢æ”¿æ”¯å‡ºã€ç¨æ”¶)
3. ç»æµæŒ‡æ ‡ (GDPã€CPIã€PMI)
4. å›½é™…ç¯å¢ƒ (è´¸æ˜“ã€æ±‡ç‡)

è¾“å‡ºæ ¼å¼: JSON
{"signal": "bullish/bearish/neutral", "confidence": 0.0-1.0, "key_factors": [...], "reasoning": "è¯¦ç»†åˆ†æ"}
""",

            "market_ecology": """ä½ æ˜¯ä¸€ä¸ªå¸‚åœºç”Ÿæ€åˆ†æä¸“å®¶ã€‚
ä½ çš„ä»»åŠ¡æ˜¯åˆ†ææ¿å—è½®åŠ¨ã€å¸‚åœºçƒ­ç‚¹å’Œèµ„é‡‘æµå‘ã€‚

åˆ†æç»´åº¦:
1. æ¿å—å¼ºå¼± (æ¶¨è·Œå®¶æ•°ã€èµ„é‡‘æµå‘)
2. å¸‚åœºçƒ­ç‚¹ (æ¦‚å¿µã€é¢˜æ)
3. é¾™å¤´è‚¡è¡¨ç°
4. å¸‚åœºæƒ…ç»ª (å¤šç©ºæ¯”ã€åŒ—å‘èµ„é‡‘)

è¾“å‡ºæ ¼å¼: JSON
{"ecology_status": "strong/weak/neutral", "signal": "BUY/SELL/HOLD", "confidence": 0.0-1.0, "reasoning": "è¯¦ç»†åˆ†æ"}
""",

            "auction_game": """ä½ æ˜¯ä¸€ä¸ªç«ä»·åšå¼ˆåˆ†æä¸“å®¶ã€‚
ä½ çš„ä»»åŠ¡æ˜¯åˆ†æé›†åˆç«ä»·é˜¶æ®µçš„ä¸»åŠ›æ„å›¾å’Œèµ„é‡‘åšå¼ˆã€‚

åˆ†æç»´åº¦:
1. ç«ä»·é‡ä»·å…³ç³»
2. å¤§å•åˆ†å¸ƒ
3. ä¸»åŠ›æ„å›¾åˆ¤æ–­
4. å¼€ç›˜é¢„æœŸ

è¾“å‡ºæ ¼å¼: JSON
{"intent": "ç§¯æ/æ¶ˆæ/è§‚æœ›", "signal": "BUY/SELL/HOLD", "confidence": 0.0-1.0, "reasoning": "è¯¦ç»†åˆ†æ"}
""",

            "pattern": """ä½ æ˜¯ä¸€ä¸ªKçº¿å½¢æ€è¯†åˆ«ä¸“å®¶ã€‚
ä½ çš„ä»»åŠ¡æ˜¯è¯†åˆ«ç»å…¸Kçº¿å½¢æ€å¹¶åˆ¤æ–­è¶‹åŠ¿å»¶ç»­æˆ–åè½¬ä¿¡å·ã€‚

åˆ†æç»´åº¦:
1. å•æ ¹Kçº¿å½¢æ€ (é”¤å­çº¿ã€åå­—æ˜Ÿç­‰)
2. ç»„åˆå½¢æ€ (æ—©æ™¨ä¹‹æ˜Ÿã€é»„æ˜ä¹‹æ˜Ÿç­‰)
3. è¶‹åŠ¿åˆ¤æ–­
4. æ”¯æ’‘/é˜»åŠ›ä½

è¾“å‡ºæ ¼å¼: JSON
{"pattern": "å½¢æ€åç§°", "signal": "BUY/SELL/HOLD", "confidence": 0.0-1.0, "reasoning": "è¯¦ç»†åˆ†æ"}
""",

            "arbitrage": """ä½ æ˜¯ä¸€ä¸ªå¥—åˆ©æœºä¼šåˆ†æä¸“å®¶ã€‚
ä½ çš„ä»»åŠ¡æ˜¯è¯†åˆ«ç»Ÿè®¡å¥—åˆ©å’Œäº‹ä»¶é©±åŠ¨å¥—åˆ©æœºä¼šã€‚

åˆ†æç»´åº¦:
1. ä»·æ ¼åç¦» (ä¸å…¬å…ä»·å€¼)
2. è·¨å¸‚åœºä»·å·®
3. é…å¯¹äº¤æ˜“æœºä¼š
4. äº‹ä»¶é©±åŠ¨ (é‡ç»„ã€åˆ†çº¢ç­‰)

è¾“å‡ºæ ¼å¼: JSON
{"opportunity": "å¥—åˆ©ç±»å‹", "signal": "BUY/SELL/HOLD", "expected_return": 0.0, "confidence": 0.0-1.0, "reasoning": "è¯¦ç»†åˆ†æ"}
"""
        }
        
        # é»˜è®¤Prompt
        default_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡åŒ–äº¤æ˜“åˆ†æå¸ˆã€‚
è¯·æ ¹æ®æä¾›çš„å¸‚åœºæ•°æ®å’Œä»»åŠ¡è¦æ±‚,ç»™å‡ºä¸“ä¸šçš„åˆ†æå’Œå»ºè®®ã€‚

è¾“å‡ºæ ¼å¼: JSON
{"signal": "BUY/SELL/HOLD", "confidence": 0.0-1.0, "reasoning": "è¯¦ç»†åˆ†æ"}
"""
        
        return prompts.get(agent_name, default_prompt)
    
    def _format_agent_task(self, agent_name: str, task: str, context: Dict) -> str:
        """æ ¼å¼åŒ–Agentä»»åŠ¡ä¸ºPrompt"""
        # æå–ä¸Šä¸‹æ–‡ä¿¡æ¯
        symbol = context.get("symbol", "N/A")
        date = context.get("date", "N/A")
        market_data = context.get("market_data", {})
        
        # æ ¼å¼åŒ–å¸‚åœºæ•°æ®
        market_data_str = json.dumps(market_data, indent=2, ensure_ascii=False)
        
        return f"""
ã€è‚¡ç¥¨ä»£ç ã€‘{symbol}
ã€åˆ†ææ—¥æœŸã€‘{date}
ã€ä»»åŠ¡ã€‘{task}

ã€å¸‚åœºæ•°æ®ã€‘
{market_data_str}

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯è¿›è¡Œä¸“ä¸šåˆ†æ,å¹¶ä»¥JSONæ ¼å¼è¾“å‡ºç»“æœã€‚
"""
    
    def get_usage_report(self) -> Dict[str, Any]:
        """
        è·å–ä½¿ç”¨ç»Ÿè®¡æŠ¥å‘Š
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        runtime = (datetime.now() - self.stats.start_time).total_seconds()
        
        return {
            "provider": self.provider_name,
            "model": self.provider.model,
            "runtime_seconds": runtime,
            "total_calls": self.stats.total_calls,
            "total_input_tokens": self.stats.total_input_tokens,
            "total_output_tokens": self.stats.total_output_tokens,
            "total_tokens": self.stats.total_input_tokens + self.stats.total_output_tokens,
            "total_cost_usd": round(self.stats.total_cost_usd, 4),
            "avg_cost_per_call": round(
                self.stats.total_cost_usd / max(1, self.stats.total_calls), 4
            ),
            "cache_enabled": self.cache_enabled,
            "cache_size": len(self.cache),
            "agent_stats": self.stats.agent_stats
        }
    
    def print_usage_report(self):
        """æ‰“å°ä½¿ç”¨ç»Ÿè®¡æŠ¥å‘Š"""
        report = self.get_usage_report()
        
        print("\n" + "=" * 60)
        print("ğŸ“Š LLMä½¿ç”¨ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 60)
        print(f"\næä¾›å•†: {report['provider']} ({report['model']})")
        print(f"è¿è¡Œæ—¶é—´: {report['runtime_seconds']:.1f}ç§’")
        print(f"\næ€»è°ƒç”¨æ¬¡æ•°: {report['total_calls']}")
        print(f"è¾“å…¥Tokens: {report['total_input_tokens']:,}")
        print(f"è¾“å‡ºTokens: {report['total_output_tokens']:,}")
        print(f"æ€»è®¡Tokens: {report['total_tokens']:,}")
        print(f"\nğŸ’° æ€»æˆæœ¬: ${report['total_cost_usd']}")
        print(f"å¹³å‡æ¯æ¬¡è°ƒç”¨: ${report['avg_cost_per_call']}")
        
        if report['cache_enabled']:
            print(f"\nğŸ“¦ ç¼“å­˜çŠ¶æ€: å·²å¯ç”¨ (ç¼“å­˜æ•°: {report['cache_size']})")
        
        if report['agent_stats']:
            print("\n" + "-" * 60)
            print("å„Agentç»Ÿè®¡:")
            print("-" * 60)
            for agent, stats in sorted(report['agent_stats'].items(),
                                       key=lambda x: x[1]['cost'],
                                       reverse=True):
                print(f"\n{agent}:")
                print(f"  è°ƒç”¨: {stats['calls']}æ¬¡")
                print(f"  Tokens: {stats['input_tokens'] + stats['output_tokens']:,}")
                print(f"  æˆæœ¬: ${stats['cost']:.4f}")
        
        print("\n" + "=" * 60 + "\n")
    
    def save_usage_report(self, filepath: str):
        """ä¿å­˜ä½¿ç”¨ç»Ÿè®¡æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        report = self.get_usage_report()
        report["timestamp"] = datetime.now().isoformat()
        
        output_path = Path(filepath)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… ä½¿ç”¨æŠ¥å‘Šå·²ä¿å­˜: {output_path}")


# ============================================================================
# ä¾¿æ·å‡½æ•°
# ============================================================================

async def test_llm_integration():
    """æµ‹è¯•LLMé›†æˆ"""
    print("\nğŸ§ª æµ‹è¯•LLMé›†æˆ...\n")
    
    try:
        # åˆ›å»ºç®¡ç†å™¨
        manager = ProductionLLMManager()
        
        # æµ‹è¯•æƒ…ç»ªåˆ†æAgent
        result = await manager.call_agent(
            agent_name="sentiment",
            task="åˆ†æå¸‚åœºæƒ…ç»ª",
            context={
                "symbol": "000001.SZ",
                "date": "2024-01-15",
                "market_data": {
                    "price": 15.5,
                    "change_pct": 0.03,
                    "volume": 1000000,
                    "news": ["å…¬å¸å‘å¸ƒä¸šç»©é¢„å‘Š", "è¡Œä¸šæ”¿ç­–åˆ©å¥½"]
                }
            }
        )
        
        print(f"âœ… LLMå“åº”:\n{result}\n")
        
        # æ‰“å°ç»Ÿè®¡
        manager.print_usage_report()
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_llm_integration())
