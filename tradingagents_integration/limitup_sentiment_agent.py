"""
æ¶¨åœæ¿èˆ†æƒ…åˆ†ææ™ºèƒ½ä½“ - åŸºäºTradingAgents

ä½¿ç”¨LLMæ·±åº¦åˆ†ææ¶¨åœæ¿çš„èˆ†æƒ…ç‰¹å¾ï¼š
1. æ–°é—»åˆ†æ - é¢˜æå‚¬åŒ–å‰‚è¯†åˆ«
2. ç¤¾äº¤åª’ä½“ - å¾®åšã€è‚¡å§æƒ…ç»ª
3. èµ„é‡‘æµå‘ - ä¸»åŠ›æ„å›¾åˆ¤æ–­
4. æŒç»­æ€§é¢„æµ‹ - "ä¸€è¿›äºŒ"æ¦‚ç‡è¯„ä¼°
"""

import os
import sys
import asyncio
import json
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
from pathlib import Path
import warnings

warnings.filterwarnings('ignore')
import logging
logger = logging.getLogger(__name__)

# æ·»åŠ TradingAgentsè·¯å¾„
TRADINGAGENTS_PATH = os.getenv("TRADINGAGENTS_PATH", "D:/test/Qlib/TradingAgents")
if os.path.exists(TRADINGAGENTS_PATH):
    sys.path.insert(0, TRADINGAGENTS_PATH)
    TRADINGAGENTS_AVAILABLE = True
else:
    TRADINGAGENTS_AVAILABLE = False
    logger.warning(f"TradingAgentsæœªæ‰¾åˆ°ï¼Œè·¯å¾„: {TRADINGAGENTS_PATH}")
    logger.info("ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬ï¼ˆä¸ä¾èµ–å®˜æ–¹ä»£ç ï¼‰")


class NewsAPITool:
    """æ–°é—»APIå·¥å…·ï¼ˆæ”¯æŒAKShareçœŸå®æ•°æ®ï¼‰"""
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("NEWS_API_KEY", "")
        self.use_real_data = os.getenv("USE_REAL_NEWS", "false").lower() == "true"
        
    async def fetch(self, symbol: str, date: str) -> List[Dict]:
        """è·å–è‚¡ç¥¨ç›¸å…³æ–°é—»"""
        if self.use_real_data:
            try:
                return await self._fetch_real_news(symbol, date)
            except Exception as e:
                logger.warning(f"çœŸå®æ–°é—»è·å–å¤±è´¥: {e}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        
        # æ¨¡æ‹Ÿæ•°æ®ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
        return [
            {
                'title': f'{symbol} æ¶‰åŠçƒ­é—¨é¢˜æï¼Œä¸»åŠ›èµ„é‡‘å¤§å¹…æµå…¥',
                'content': 'è¯¥è‚¡æ¶¨åœåï¼Œå¸‚åœºå…³æ³¨åº¦æ˜¾è‘—æå‡ï¼Œå¤šå®¶æœºæ„çœ‹å¥½åç»­è¡¨ç°...',
                'source': 'è´¢ç»ç½‘',
                'publish_time': date,
                'sentiment': 'positive'
            },
            {
                'title': f'{symbol} æŠ€æœ¯é¢çªç ´ï¼Œèµ„é‡‘è¿½æ§',
                'content': 'ä»æŠ€æœ¯å½¢æ€çœ‹ï¼Œè¯¥è‚¡çªç ´å…³é”®å‹åŠ›ä½ï¼Œæˆäº¤é‡é…åˆè‰¯å¥½...',
                'source': 'è¯åˆ¸æ—¥æŠ¥',
                'publish_time': date,
                'sentiment': 'positive'
            }
        ]
    
    async def _fetch_real_news(self, symbol: str, date: str) -> List[Dict]:
        """ä½¿ç”¨AKShareè·å–çœŸå®æ–°é—»"""
        try:
            import akshare as ak
            
            # è·å–ä¸ªè‚¡æ–°é—»ï¼ˆä¸œæ–¹è´¢å¯Œï¼‰
            # ç¤ºä¾‹ï¼šak.stock_news_em(symbol='000001')
            stock_code = symbol.split('.')[0]
            news_df = ak.stock_news_em(symbol=stock_code)
            
            # ç­›é€‰æ—¥æœŸ
            news_df['å‘å¸ƒæ—¶é—´'] = pd.to_datetime(news_df['å‘å¸ƒæ—¶é—´'])
            target_date = pd.to_datetime(date)
            news_df = news_df[
                news_df['å‘å¸ƒæ—¶é—´'].dt.date == target_date.date()
            ]
            
            # è½¬æ¢æ ¼å¼
            news_list = []
            for _, row in news_df.iterrows():
                news_list.append({
                    'title': row['æ–°é—»æ ‡é¢˜'],
                    'content': row['æ–°é—»å†…å®¹'],
                    'source': row.get('æ–‡ç« æ¥æº', 'ä¸œæ–¹è´¢å¯Œ'),
                    'publish_time': row['å‘å¸ƒæ—¶é—´'].strftime('%Y-%m-%d %H:%M:%S'),
                    'sentiment': 'neutral'  # éœ€è¦é¢å¤–çš„æƒ…æ„Ÿåˆ†æ
                })
            
            return news_list if news_list else await self.fetch(symbol, date)  # æ— æ•°æ®æ—¶é™çº§
            
        except ImportError:
            logger.warning("AKShareæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install akshare")
            raise
        except Exception as e:
            logger.warning(f"AKShareæ•°æ®è·å–å¤±è´¥: {e}")
            raise


class WeiboTool:
    """å¾®åšæ•°æ®å·¥å…·ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
    
    def __init__(self):
        pass
        
    async def fetch(self, symbol: str, date: str) -> Dict:
        """è·å–è‚¡ç¥¨ç›¸å…³å¾®åšæ•°æ®"""
        # å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨å¾®åšAPIæˆ–çˆ¬è™«
        
        # æ¨¡æ‹Ÿæ•°æ®
        return {
            'total_posts': 1250,
            'positive_ratio': 0.72,
            'negative_ratio': 0.15,
            'neutral_ratio': 0.13,
            'hot_keywords': ['AIæ¦‚å¿µ', 'ä¸šç»©è¶…é¢„æœŸ', 'ä¸»åŠ›å»ºä»“', 'é¾™å¤´è‚¡'],
            'kol_opinions': [
                {'author': 'çŸ¥ååšä¸»A', 'view': 'çœ‹å¥½åç»­è¡¨ç°ï¼Œæœ‰æœ›è¿æ¿'},
                {'author': 'çŸ¥ååšä¸»B', 'view': 'é¢˜æçº¯æ­£ï¼Œèµ„é‡‘è®¤å¯åº¦é«˜'}
            ]
        }


class StockForumTool:
    """è‚¡å§æ•°æ®å·¥å…·ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
    
    def __init__(self):
        pass
        
    async def fetch(self, symbol: str, date: str) -> Dict:
        """è·å–è‚¡å§è®¨è®ºæ•°æ®"""
        # å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šçˆ¬å–ä¸œæ–¹è´¢å¯Œè‚¡å§ç­‰
        
        # æ¨¡æ‹Ÿæ•°æ®
        return {
            'total_posts': 3500,
            'sentiment_score': 7.8,  # 1-10åˆ†
            'hot_topics': ['ä¸€è¿›äºŒ', 'é¾™å¤´', 'ä¸»åŠ›æŠ¤ç›˜', 'é¢˜ææ­£å®—'],
            'prediction_stats': {
                'bullish': 0.68,  # çœ‹æ¶¨æ¯”ä¾‹
                'bearish': 0.18,  # çœ‹è·Œæ¯”ä¾‹
                'neutral': 0.14   # ä¸­æ€§æ¯”ä¾‹
            }
        }


class LimitUpSentimentAgent:
    """æ¶¨åœæ¿èˆ†æƒ…åˆ†ææ™ºèƒ½ä½“"""
    
    def __init__(self, config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–èˆ†æƒ…åˆ†ææ™ºèƒ½ä½“
        
        Parameters:
        -----------
        config : Dict, optional
            é…ç½®å‚æ•°ï¼ŒåŒ…æ‹¬ï¼š
            - llm_api_key: LLM APIå¯†é’¥
            - llm_model: ä½¿ç”¨çš„æ¨¡å‹ï¼ˆé»˜è®¤gpt-4-turboï¼‰
            - llm_api_base: API base URL
        """
        self.config = config or {}
        
        # LLMé…ç½®
        self.llm_api_key = self.config.get('llm_api_key') or os.getenv('OPENAI_API_KEY', '')
        self.llm_model = self.config.get('llm_model', 'gpt-4-turbo')
        self.llm_api_base = self.config.get('llm_api_base') or os.getenv('OPENAI_API_BASE', 'https://api.openai.com/v1')
        
        # åˆå§‹åŒ–æ•°æ®å·¥å…·
        self.news_tool = NewsAPITool()
        self.weibo_tool = WeiboTool()
        self.forum_tool = StockForumTool()
        
        # å°è¯•åˆå§‹åŒ–TradingAgentsï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.agent = None
        self.llm = None
        
        if TRADINGAGENTS_AVAILABLE and self.llm_api_key:
            try:
                self._init_tradingagents()
            except Exception as e:
                print(f"âš ï¸  TradingAgentsåˆå§‹åŒ–å¤±è´¥: {e}")
                print(f"   ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
    
    def _init_tradingagents(self):
        """åˆå§‹åŒ–TradingAgentså®˜æ–¹ç»„ä»¶"""
        try:
            # å¯¼å…¥å®˜æ–¹ç»„ä»¶
            from tradingagents.llm.openai_adapter import OpenAIAdapter
            from tradingagents.agents.sentiment_analyst import SentimentAnalystAgent
            
            # åˆå§‹åŒ–LLM
            self.llm = OpenAIAdapter(
                api_key=self.llm_api_key,
                model=self.llm_model,
                api_base=self.llm_api_base
            )
            
            # åˆå§‹åŒ–èˆ†æƒ…åˆ†ææ™ºèƒ½ä½“
            self.agent = SentimentAnalystAgent(
                llm=self.llm,
                tools={
                    'news': self.news_tool,
                    'weibo': self.weibo_tool,
                    'forum': self.forum_tool
                }
            )
            
            logger.info("âœ… TradingAgentså®˜æ–¹ç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
            
        except ImportError as e:
            logger.warning(f"TradingAgentså¯¼å…¥å¤±è´¥: {e}")
            logger.info("ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬ï¼ˆåŸºäºè§„åˆ™çš„åˆ†æï¼‰")
            self.agent = None
    
    async def analyze_limitup_sentiment(
        self, 
        symbol: str, 
        date: str,
        price_data: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        åˆ†ææ¶¨åœæ¿èˆ†æƒ…
        
        Parameters:
        -----------
        symbol : str
            è‚¡ç¥¨ä»£ç 
        date : str
            æ—¥æœŸ (YYYY-MM-DD)
        price_data : Dict, optional
            ä»·æ ¼æ•°æ®ï¼ˆç”¨äºè¾…åŠ©åˆ†æï¼‰
        
        Returns:
        --------
        Dict: èˆ†æƒ…åˆ†æç»“æœ
            - sentiment_score: ç»¼åˆæƒ…ç»ªå¾—åˆ† (0-100)
            - key_catalysts: å…³é”®å‚¬åŒ–å‰‚
            - risk_factors: é£é™©å› ç´ 
            - continue_prob: ä¸€è¿›äºŒæ¦‚ç‡
            - reasoning: è¯¦ç»†æ¨ç†è¿‡ç¨‹
        """
        logger.info(f"å¼€å§‹åˆ†æ {symbol} åœ¨ {date} çš„æ¶¨åœèˆ†æƒ…...")
        
        # 1. å¹¶å‘è·å–å¤šæºæ•°æ®
        logger.info("è·å–æ•°æ®...")
        news_data, weibo_data, forum_data = await asyncio.gather(
            self.news_tool.fetch(symbol, date),
            self.weibo_tool.fetch(symbol, date),
            self.forum_tool.fetch(symbol, date)
        )
        
        # 2. å¦‚æœæœ‰TradingAgentsï¼Œä½¿ç”¨LLMæ·±åº¦åˆ†æ
        if self.agent and self.llm:
            logger.info("ä½¿ç”¨LLMæ·±åº¦åˆ†æ...")
            result = await self._analyze_with_llm(
                symbol, date, news_data, weibo_data, forum_data, price_data
            )
        else:
            logger.info("ä½¿ç”¨è§„åˆ™å¼•æ“åˆ†æ...")
            result = self._analyze_with_rules(
                symbol, date, news_data, weibo_data, forum_data, price_data
            )
        
        logger.info(f"åˆ†æå®Œæˆï¼Œç»¼åˆå¾—åˆ†: {result['sentiment_score']:.1f}")
        
        return result
    
    async def _analyze_with_llm(
        self,
        symbol: str,
        date: str,
        news_data: List[Dict],
        weibo_data: Dict,
        forum_data: Dict,
        price_data: Optional[Dict]
    ) -> Dict[str, Any]:
        """ä½¿ç”¨LLMè¿›è¡Œæ·±åº¦åˆ†æ"""
        
        # æ„å»ºåˆ†ææç¤ºè¯
        prompt = self._build_analysis_prompt(
            symbol, date, news_data, weibo_data, forum_data, price_data
        )
        
        try:
            # è°ƒç”¨LLMï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„TradingAgents APIè°ƒæ•´ï¼‰
            # response = await self.agent.analyze(prompt)
            
            # æš‚æ—¶ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
            response = self._analyze_with_rules(
                symbol, date, news_data, weibo_data, forum_data, price_data
            )
            
            return response
            
        except Exception as e:
            logger.warning(f"LLMåˆ†æå¤±è´¥: {e}ï¼Œä½¿ç”¨è§„åˆ™å¼•æ“")
            return self._analyze_with_rules(
                symbol, date, news_data, weibo_data, forum_data, price_data
            )
    
    def _analyze_with_rules(
        self,
        symbol: str,
        date: str,
        news_data: List[Dict],
        weibo_data: Dict,
        forum_data: Dict,
        price_data: Optional[Dict]
    ) -> Dict[str, Any]:
        """åŸºäºè§„åˆ™çš„èˆ†æƒ…åˆ†æ"""
        
        # 1. æ–°é—»æƒ…ç»ªå¾—åˆ† (0-30åˆ†)
        news_score = 0
        positive_news = sum(1 for n in news_data if n.get('sentiment') == 'positive')
        news_score = min(30, positive_news * 15)  # æ¯æ¡æ­£é¢æ–°é—»15åˆ†
        
        # 2. å¾®åšæƒ…ç»ªå¾—åˆ† (0-30åˆ†)
        weibo_positive_ratio = weibo_data.get('positive_ratio', 0)
        weibo_score = weibo_positive_ratio * 30
        
        # 3. è‚¡å§æƒ…ç»ªå¾—åˆ† (0-30åˆ†)
        forum_bullish = forum_data.get('prediction_stats', {}).get('bullish', 0)
        forum_score = forum_bullish * 30
        
        # 4. çƒ­åº¦åŠ æˆ (0-10åˆ†)
        total_posts = weibo_data.get('total_posts', 0) + forum_data.get('total_posts', 0)
        heat_score = min(10, total_posts / 500)  # æ¯500æ¡è®¨è®º1åˆ†
        
        # ç»¼åˆå¾—åˆ†
        sentiment_score = news_score + weibo_score + forum_score + heat_score
        
        # è¯†åˆ«å…³é”®å‚¬åŒ–å‰‚
        catalysts = []
        for news in news_data:
            if 'é¢˜æ' in news['title'] or 'æ¦‚å¿µ' in news['title']:
                catalysts.append(f"é¢˜æå‚¬åŒ–: {news['title'][:30]}")
        
        if weibo_data.get('hot_keywords'):
            catalysts.append(f"çƒ­ç‚¹å…³é”®è¯: {', '.join(weibo_data['hot_keywords'][:3])}")
        
        # è¯†åˆ«é£é™©å› ç´ 
        risks = []
        weibo_negative = weibo_data.get('negative_ratio', 0)
        if weibo_negative > 0.3:
            risks.append(f"å¾®åšè´Ÿé¢æƒ…ç»ªè¾ƒé«˜ ({weibo_negative:.1%})")
        
        forum_bearish = forum_data.get('prediction_stats', {}).get('bearish', 0)
        if forum_bearish > 0.3:
            risks.append(f"è‚¡å§çœ‹è·Œæ¯”ä¾‹è¾ƒé«˜ ({forum_bearish:.1%})")
        
        if total_posts < 1000:
            risks.append("å…³æ³¨åº¦ä¸è¶³ï¼Œå¯èƒ½ç¼ºä¹æŒç»­æ€§")
        
        # è®¡ç®—ä¸€è¿›äºŒæ¦‚ç‡
        # åŸºäºæƒ…ç»ªå¾—åˆ†çš„çº¿æ€§æ˜ å°„
        continue_prob = sentiment_score / 100
        
        # æ ¹æ®å…·ä½“å› ç´ è°ƒæ•´
        if len(catalysts) >= 3:
            continue_prob += 0.1  # æœ‰å¤šä¸ªå‚¬åŒ–å‰‚ï¼Œæå‡10%
        
        if len(risks) >= 2:
            continue_prob -= 0.15  # æœ‰å¤šä¸ªé£é™©ï¼Œé™ä½15%
        
        continue_prob = max(0.0, min(1.0, continue_prob))  # é™åˆ¶åœ¨0-1ä¹‹é—´
        
        # ç”Ÿæˆæ¨ç†è¿‡ç¨‹
        reasoning = self._generate_reasoning(
            sentiment_score, catalysts, risks, continue_prob,
            news_data, weibo_data, forum_data
        )
        
        return {
            'symbol': symbol,
            'date': date,
            'sentiment_score': sentiment_score,
            'key_catalysts': catalysts,
            'risk_factors': risks,
            'continue_prob': continue_prob,
            'reasoning': reasoning,
            'data_sources': {
                'news_count': len(news_data),
                'weibo_posts': weibo_data.get('total_posts', 0),
                'forum_posts': forum_data.get('total_posts', 0)
            }
        }
    
    def _build_analysis_prompt(
        self,
        symbol: str,
        date: str,
        news_data: List[Dict],
        weibo_data: Dict,
        forum_data: Dict,
        price_data: Optional[Dict]
    ) -> str:
        """æ„å»ºLLMåˆ†ææç¤ºè¯"""
        
        prompt = f"""
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„Aè‚¡æ¶¨åœæ¿åˆ†æä¸“å®¶ã€‚è¯·åˆ†æ {symbol} åœ¨ {date} æ¶¨åœåçš„èˆ†æƒ…ç‰¹å¾ï¼Œ
è¯„ä¼°å…¶æ˜å¤©ç»§ç»­æ¶¨åœï¼ˆ"ä¸€è¿›äºŒ"ï¼‰çš„æ¦‚ç‡ã€‚

# æ–°é—»æ•°æ®
{json.dumps(news_data, ensure_ascii=False, indent=2)}

# å¾®åšæ•°æ®
{json.dumps(weibo_data, ensure_ascii=False, indent=2)}

# è‚¡å§æ•°æ®
{json.dumps(forum_data, ensure_ascii=False, indent=2)}

# ä»·æ ¼æ•°æ®
{json.dumps(price_data or {}, ensure_ascii=False, indent=2)}

è¯·ä»ä»¥ä¸‹ç»´åº¦åˆ†æï¼š
1. **é¢˜ææ˜¯å¦è¢«å¸‚åœºè®¤å¯ï¼Ÿ** - æŸ¥çœ‹æ–°é—»å’Œç¤¾äº¤åª’ä½“çš„è®¨è®ºçƒ­åº¦
2. **æ˜¯å¦æœ‰é‡å¤§åˆ©å¥½å‚¬åŒ–å‰‚ï¼Ÿ** - è¯†åˆ«æ”¿ç­–ã€ä¸šç»©ã€å¹¶è´­ç­‰å‚¬åŒ–å› ç´ 
3. **æ•£æˆ·æƒ…ç»ªæ˜¯å¦è¿‡çƒ­ï¼Ÿ** - æƒ…ç»ªè¿‡çƒ­å¾€å¾€æ˜¯åå‘æŒ‡æ ‡
4. **æœºæ„æ˜¯å¦å‚ä¸ï¼Ÿ** - ä»èµ„é‡‘æµå‘åˆ¤æ–­ä¸»åŠ›æ„å›¾
5. **æ˜å¤©ç»§ç»­æ¶¨åœçš„æ¦‚ç‡ï¼Ÿ** - ç»¼åˆè¯„ä¼°ç»™å‡º0-1ä¹‹é—´çš„æ¦‚ç‡

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "sentiment_score": 0-100,
    "key_catalysts": ["å‚¬åŒ–å‰‚1", "å‚¬åŒ–å‰‚2"],
    "risk_factors": ["é£é™©1", "é£é™©2"],
    "continue_prob": 0.0-1.0,
    "reasoning": "è¯¦ç»†æ¨ç†è¿‡ç¨‹"
}}
"""
        return prompt
    
    def _generate_reasoning(
        self,
        sentiment_score: float,
        catalysts: List[str],
        risks: List[str],
        continue_prob: float,
        news_data: List[Dict],
        weibo_data: Dict,
        forum_data: Dict
    ) -> str:
        """ç”Ÿæˆæ¨ç†è¿‡ç¨‹"""
        
        reasoning_parts = []
        
        # 1. ç»¼åˆè¯„åˆ†
        if sentiment_score >= 80:
            reasoning_parts.append(f"âœ… ç»¼åˆæƒ…ç»ªå¾—åˆ† {sentiment_score:.1f}/100ï¼Œå¸‚åœºæƒ…ç»ªæåº¦ä¹è§‚")
        elif sentiment_score >= 60:
            reasoning_parts.append(f"âœ… ç»¼åˆæƒ…ç»ªå¾—åˆ† {sentiment_score:.1f}/100ï¼Œå¸‚åœºæƒ…ç»ªè¾ƒä¸ºä¹è§‚")
        elif sentiment_score >= 40:
            reasoning_parts.append(f"âš ï¸  ç»¼åˆæƒ…ç»ªå¾—åˆ† {sentiment_score:.1f}/100ï¼Œå¸‚åœºæƒ…ç»ªä¸­æ€§")
        else:
            reasoning_parts.append(f"âŒ ç»¼åˆæƒ…ç»ªå¾—åˆ† {sentiment_score:.1f}/100ï¼Œå¸‚åœºæƒ…ç»ªåè°¨æ…")
        
        # 2. å‚¬åŒ–å‰‚
        if catalysts:
            reasoning_parts.append(f"ğŸ“° è¯†åˆ«åˆ° {len(catalysts)} ä¸ªå‚¬åŒ–å‰‚:")
            for cat in catalysts:
                reasoning_parts.append(f"   â€¢ {cat}")
        else:
            reasoning_parts.append("âš ï¸  æœªå‘ç°æ˜æ˜¾å‚¬åŒ–å‰‚ï¼Œå¯èƒ½æ˜¯è·Ÿé£ç‚’ä½œ")
        
        # 3. é£é™©å› ç´ 
        if risks:
            reasoning_parts.append(f"âš ï¸  å­˜åœ¨ {len(risks)} ä¸ªé£é™©å› ç´ :")
            for risk in risks:
                reasoning_parts.append(f"   â€¢ {risk}")
        else:
            reasoning_parts.append("âœ… æš‚æ— æ˜æ˜¾é£é™©å› ç´ ")
        
        # 4. æ•°æ®æ¥æº
        news_count = len(news_data)
        weibo_posts = weibo_data.get('total_posts', 0)
        forum_posts = forum_data.get('total_posts', 0)
        
        reasoning_parts.append(f"ğŸ“Š æ•°æ®æ¥æº: {news_count}æ¡æ–°é—», {weibo_posts}æ¡å¾®åš, {forum_posts}æ¡è‚¡å§è®¨è®º")
        
        # 5. æœ€ç»ˆç»“è®º
        if continue_prob >= 0.75:
            reasoning_parts.append(f"ğŸ¯ **ä¸€è¿›äºŒæ¦‚ç‡: {continue_prob:.1%}ï¼Œå¼ºçƒˆçœ‹å¥½æ˜æ—¥ç»§ç»­æ¶¨åœ**")
        elif continue_prob >= 0.60:
            reasoning_parts.append(f"ğŸ¯ **ä¸€è¿›äºŒæ¦‚ç‡: {continue_prob:.1%}ï¼Œè¾ƒçœ‹å¥½æ˜æ—¥ç»§ç»­æ¶¨åœ**")
        elif continue_prob >= 0.45:
            reasoning_parts.append(f"ğŸ¯ **ä¸€è¿›äºŒæ¦‚ç‡: {continue_prob:.1%}ï¼Œæ˜æ—¥èµ°åŠ¿å­˜åœ¨ä¸ç¡®å®šæ€§**")
        else:
            reasoning_parts.append(f"ğŸ¯ **ä¸€è¿›äºŒæ¦‚ç‡: {continue_prob:.1%}ï¼Œæ˜æ—¥ç»§ç»­æ¶¨åœæ¦‚ç‡è¾ƒä½**")
        
        return "\n".join(reasoning_parts)
    
    async def batch_analyze(
        self,
        symbols: List[str],
        date: str
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡åˆ†æå¤šåªæ¶¨åœè‚¡ç¥¨
        
        Parameters:
        -----------
        symbols : List[str]
            è‚¡ç¥¨ä»£ç åˆ—è¡¨
        date : str
            æ—¥æœŸ
        
        Returns:
        --------
        List[Dict]: åˆ†æç»“æœåˆ—è¡¨ï¼ŒæŒ‰ä¸€è¿›äºŒæ¦‚ç‡é™åºæ’åˆ—
        """
        print(f"\nğŸ“Š æ‰¹é‡åˆ†æ {len(symbols)} åªæ¶¨åœè‚¡ç¥¨...")
        
        # å¹¶å‘åˆ†æ
        tasks = [
            self.analyze_limitup_sentiment(symbol, date)
            for symbol in symbols
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è¿‡æ»¤é”™è¯¯ç»“æœ
        valid_results = [
            r for r in results 
            if not isinstance(r, Exception)
        ]
        
        # æŒ‰ä¸€è¿›äºŒæ¦‚ç‡é™åºæ’åº
        valid_results.sort(key=lambda x: x['continue_prob'], reverse=True)
        
        return valid_results


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

async def main():
    """ç¤ºä¾‹ï¼šåˆ†æå•ä¸ªæ¶¨åœè‚¡ç¥¨"""
    print("=" * 80)
    print("æ¶¨åœæ¿èˆ†æƒ…åˆ†ææ™ºèƒ½ä½“ - æµ‹è¯•")
    print("=" * 80)
    
    # 1. åˆå§‹åŒ–æ™ºèƒ½ä½“
    config = {
        'llm_api_key': os.getenv('OPENAI_API_KEY', 'your-api-key'),
        'llm_model': 'gpt-4-turbo'
    }
    
    agent = LimitUpSentimentAgent(config)
    
    # 2. åˆ†æå•åªè‚¡ç¥¨
    result = await agent.analyze_limitup_sentiment(
        symbol='000001.SZ',
        date='2024-06-30'
    )
    
    print("\n" + "=" * 80)
    print(f"ğŸ“Š åˆ†æç»“æœ: {result['symbol']}")
    print("=" * 80)
    print(f"ç»¼åˆæƒ…ç»ªå¾—åˆ†: {result['sentiment_score']:.1f}/100")
    print(f"ä¸€è¿›äºŒæ¦‚ç‡: {result['continue_prob']:.1%}")
    print(f"\nå…³é”®å‚¬åŒ–å‰‚:")
    for cat in result['key_catalysts']:
        print(f"  â€¢ {cat}")
    print(f"\né£é™©å› ç´ :")
    for risk in result['risk_factors']:
        print(f"  â€¢ {risk}")
    print(f"\næ¨ç†è¿‡ç¨‹:")
    print(result['reasoning'])
    
    # 3. æ‰¹é‡åˆ†æç¤ºä¾‹
    print("\n" + "=" * 80)
    print("æ‰¹é‡åˆ†æç¤ºä¾‹")
    print("=" * 80)
    
    symbols = ['000001.SZ', '000002.SZ', '600000.SH']
    batch_results = await agent.batch_analyze(symbols, '2024-06-30')
    
    print(f"\nğŸ“Š TOP 3 æœ€çœ‹å¥½çš„æ ‡çš„:")
    print("-" * 80)
    for i, result in enumerate(batch_results[:3], 1):
        print(f"{i}. {result['symbol']} - ä¸€è¿›äºŒæ¦‚ç‡: {result['continue_prob']:.1%} "
              f"(æƒ…ç»ªå¾—åˆ†: {result['sentiment_score']:.1f})")


if __name__ == '__main__':
    asyncio.run(main())
