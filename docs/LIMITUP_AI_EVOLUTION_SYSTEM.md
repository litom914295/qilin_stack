# ğŸ§  æ¶¨åœæ¿æ™ºèƒ½åˆ†æå’Œè‡ªæˆ‘è¿›åŒ–ç³»ç»Ÿ

## ğŸ¯ ç³»ç»Ÿç›®æ ‡

æ„å»ºä¸€ä¸ªèƒ½å¤Ÿï¼š
1. **åˆ†æå†å²æ¶¨åœåŸå› ** - å¤šè§’åº¦æ•°æ®åˆ†æ
2. **é¢„æµ‹æ¬¡æ—¥æ¶¨åœæ¦‚ç‡** - æœºå™¨å­¦ä¹ æ¨¡å‹
3. **å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–** - æ ¹æ®å®é™…ç»“æœè‡ªæˆ‘è¿›åŒ–
4. **æŒç»­æˆé•¿** - ä¸æ–­ç§¯ç´¯ç»éªŒæå‡å‡†ç¡®ç‡

## ğŸ“Š ç³»ç»Ÿæ¶æ„

```
æ¶¨åœæ¿AIè¿›åŒ–ç³»ç»Ÿ
â”‚
â”œâ”€â”€ æ•°æ®é‡‡é›†å±‚
â”‚   â”œâ”€â”€ å†å²æ¶¨åœæ•°æ®ï¼ˆAKShare/Qlibï¼‰
â”‚   â”œâ”€â”€ å¤šè§’åº¦ç‰¹å¾æå–
â”‚   â””â”€â”€ å®æ—¶æ•°æ®æ›´æ–°
â”‚
â”œâ”€â”€ åˆ†æå¼•æ“å±‚
â”‚   â”œâ”€â”€ æ¶¨åœåŸå› åˆ†æï¼ˆLLMé©±åŠ¨ï¼‰
â”‚   â”œâ”€â”€ å¤šç»´åº¦ç‰¹å¾å·¥ç¨‹
â”‚   â””â”€â”€ å› æœå…³ç³»æŒ–æ˜
â”‚
â”œâ”€â”€ é¢„æµ‹æ¨¡å‹å±‚
â”‚   â”œâ”€â”€ åŸºç¡€é¢„æµ‹æ¨¡å‹ï¼ˆLightGBM/XGBoostï¼‰
â”‚   â”œâ”€â”€ æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆTransformerï¼‰
â”‚   â””â”€â”€ é›†æˆæ¨¡å‹ï¼ˆStackingï¼‰
â”‚
â”œâ”€â”€ å¼ºåŒ–å­¦ä¹ å±‚
â”‚   â”œâ”€â”€ ç¯å¢ƒå®šä¹‰ï¼ˆäº¤æ˜“ç¯å¢ƒï¼‰
â”‚   â”œâ”€â”€ å¥–åŠ±å‡½æ•°è®¾è®¡
â”‚   â”œâ”€â”€ RL Agentï¼ˆPPO/DQNï¼‰
â”‚   â””â”€â”€ ç­–ç•¥ä¼˜åŒ–
â”‚
â””â”€â”€ è‡ªæˆ‘è¿›åŒ–å±‚
    â”œâ”€â”€ åœ¨çº¿å­¦ä¹ ï¼ˆå¢é‡è®­ç»ƒï¼‰
    â”œâ”€â”€ æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©
    â”œâ”€â”€ è¶…å‚æ•°è‡ªé€‚åº”è°ƒæ•´
    â””â”€â”€ ç»éªŒå›æ”¾æ± 
```

## ğŸ” ç¬¬ä¸€æ­¥ï¼šå¤šè§’åº¦æ•°æ®é‡‡é›†å’Œåˆ†æ

### 1.1 æ•°æ®ç»´åº¦è®¾è®¡

```python
# æ–‡ä»¶ï¼šapp/limitup_data_collector.py

class LimitUpDataCollector:
    """æ¶¨åœæ•°æ®é‡‡é›†å™¨"""
    
    def __init__(self):
        self.data_sources = {
            'akshare': AKShareDataSource(),
            'qlib': QlibDataSource(),
            'news': NewsDataSource(),
            'sentiment': SentimentDataSource()
        }
    
    def collect_daily_limitup(self, date: str) -> pd.DataFrame:
        """é‡‡é›†å½“æ—¥æ¶¨åœæ•°æ®"""
        
        # 1. åŸºç¡€è¡Œæƒ…æ•°æ®
        basic_data = self._get_basic_data(date)
        
        # 2. æŠ€æœ¯æŒ‡æ ‡ï¼ˆ30+ç»´åº¦ï¼‰
        technical = self._calculate_technical_indicators(basic_data)
        
        # 3. æ¿å—æ•ˆåº”ï¼ˆ10+ç»´åº¦ï¼‰
        sector = self._analyze_sector_effect(date)
        
        # 4. èµ„é‡‘æµå‘ï¼ˆ15+ç»´åº¦ï¼‰
        money_flow = self._analyze_money_flow(date)
        
        # 5. æƒ…ç»ªæŒ‡æ ‡ï¼ˆ10+ç»´åº¦ï¼‰
        sentiment = self._analyze_market_sentiment(date)
        
        # 6. é¢˜æçƒ­åº¦ï¼ˆ20+ç»´åº¦ï¼‰
        theme = self._analyze_theme_hotness(date)
        
        # 7. é¾™å¤´æ•ˆåº”ï¼ˆ5+ç»´åº¦ï¼‰
        leader = self._analyze_leader_effect(date)
        
        # 8. æ—¶é—´ç‰¹å¾ï¼ˆ10+ç»´åº¦ï¼‰
        temporal = self._extract_temporal_features(date)
        
        return self._merge_features([
            basic_data, technical, sector, money_flow,
            sentiment, theme, leader, temporal
        ])
```

### 1.2 æ ¸å¿ƒç‰¹å¾ç»´åº¦ï¼ˆ100+ï¼‰

#### A. æŠ€æœ¯æŒ‡æ ‡ç»´åº¦ï¼ˆ30+ï¼‰
```python
æŠ€æœ¯ç‰¹å¾ = {
    # ä»·æ ¼å½¢æ€
    "è¿æ¿å¤©æ•°": 0-10,
    "é¦–æ¿ç±»å‹": ["ä½ä½é¦–æ¿", "çªç ´é¦–æ¿", "åŠ é€Ÿé¦–æ¿"],
    "æ¶¨åœæ—¶é—´": "09:30-15:00",
    "å°æ¿å¼ºåº¦": 0-100,
    "å¼€æ¿æ¬¡æ•°": 0-10,
    
    # é‡èƒ½ç‰¹å¾
    "æ¢æ‰‹ç‡": 0-100,
    "é‡æ¯”": 0-50,
    "æˆäº¤é¢": ç™¾ä¸‡-äº¿,
    "5æ—¥é‡èƒ½å€æ•°": 0-10,
    "é‡ä»·é…åˆåº¦": 0-1,
    
    # æŠ€æœ¯å½¢æ€
    "çªç ´å‰é«˜": True/False,
    "å‡çº¿å¤šå¤´æ’åˆ—": True/False,
    "MACDé‡‘å‰": True/False,
    "RSIè¶…ä¹°": True/False,
    "å¸ƒæ—å¸¦ä½ç½®": ä¸Š/ä¸­/ä¸‹è½¨,
    
    # å†å²è¡¨ç°
    "è¿‘30æ—¥æ¶¨å¹…": -50% to 200%,
    "è¿‘5æ—¥æ³¢åŠ¨ç‡": 0-100%,
    "å†å²æ¶¨åœæ¬¡æ•°": 0-50,
    "å‰æœŸé«˜ç‚¹è·ç¦»": 0-100%
}
```

#### B. æ¿å—æ•ˆåº”ç»´åº¦ï¼ˆ10+ï¼‰
```python
æ¿å—ç‰¹å¾ = {
    "æ‰€å±æ¿å—æ¶¨åœæ•°": 0-50,
    "æ¿å—èµ„é‡‘å‡€æµå…¥": -äº¿åˆ°+äº¿,
    "æ¿å—æ¶¨è·Œå¹…": -10% to 10%,
    "æ¿å—é¾™å¤´è‚¡åœ°ä½": 0-1,
    "æ¿å—æ´»è·ƒåº¦æ’å": 1-500,
    "æ¿å—æŒç»­æ€§å¤©æ•°": 0-30,
    "åŒæ¿å—æ˜¨æ—¥æ¶¨åœæ•°": 0-50,
    "æ¿å—è½®åŠ¨å‘¨æœŸ": åˆæœŸ/ä¸­æœŸ/æœ«æœŸ
}
```

#### C. èµ„é‡‘æµå‘ç»´åº¦ï¼ˆ15+ï¼‰
```python
èµ„é‡‘ç‰¹å¾ = {
    "ä¸»åŠ›å‡€æµå…¥": -äº¿åˆ°+äº¿,
    "è¶…å¤§å•å‡€æµå…¥": -äº¿åˆ°+äº¿,
    "å¤§å•å‡€æµå…¥": -äº¿åˆ°+äº¿,
    "æ•£æˆ·å‡€æµå…¥": -äº¿åˆ°+äº¿,
    "åŒ—å‘èµ„é‡‘æµå…¥": -äº¿åˆ°+äº¿,
    "æœºæ„æŒä»“æ¯”ä¾‹": 0-100%,
    "5æ—¥èµ„é‡‘æµå…¥è¶‹åŠ¿": ä¸Šå‡/ä¸‹é™/éœ‡è¡,
    "èµ„é‡‘é›†ä¸­åº¦": 0-1,
    "ä¹°å–ç›˜å¼ºåº¦æ¯”": 0-100
}
```

#### D. æƒ…ç»ªæŒ‡æ ‡ç»´åº¦ï¼ˆ10+ï¼‰
```python
æƒ…ç»ªç‰¹å¾ = {
    "å¸‚åœºæƒ…ç»ªæŒ‡æ•°": 0-100,
    "æ¶¨åœæ¿æ€»æ•°": 0-300,
    "è·Œåœæ¿æ€»æ•°": 0-100,
    "ç‚¸æ¿ç‡": 0-100%,
    "è¿æ¿é«˜åº¦": 1-20,
    "å¸‚åœºèµšé’±æ•ˆåº”": 0-1,
    "é¢˜ææ´»è·ƒåº¦": 0-100,
    "æ¸¸èµ„æ´»è·ƒåº¦": 0-100
}
```

#### E. é¢˜æçƒ­åº¦ç»´åº¦ï¼ˆ20+ï¼‰
```python
é¢˜æç‰¹å¾ = {
    "æ‰€å±é¢˜æ": ["AI", "æ–°èƒ½æº", "å†›å·¥", ...],
    "é¢˜æçƒ­åº¦åˆ†æ•°": 0-100,
    "é¢˜æç”Ÿå‘½å‘¨æœŸ": èŒèŠ½/çˆ†å‘/è¡°é€€,
    "é¢˜ææ¶¨åœè‚¡æ•°": 0-50,
    "é¢˜æé¾™å¤´åœ°ä½": 0-1,
    "é¢˜ææŒç»­å¤©æ•°": 0-30,
    "é¢˜æèµ„é‡‘æµå…¥": -äº¿åˆ°+äº¿,
    "é¢˜ææ–°é—»æ•°é‡": 0-1000,
    "é¢˜ææ”¿ç­–æ”¯æŒ": True/False,
    "é¢˜æå¸‚åœºè®¤å¯åº¦": 0-100
}
```

#### F. æ—¶é—´ç‰¹å¾ç»´åº¦ï¼ˆ10+ï¼‰
```python
æ—¶é—´ç‰¹å¾ = {
    "æ˜ŸæœŸå‡ ": 1-5,
    "æœˆä»½": 1-12,
    "å­£åº¦": 1-4,
    "æ˜¯å¦æœˆåˆ": True/False,
    "æ˜¯å¦æœˆæœ«": True/False,
    "æ˜¯å¦å­£æœ«": True/False,
    "è·ç¦»ä¸Šæ¬¡æ¶¨åœå¤©æ•°": 0-100,
    "è¿‘æœŸèŠ‚å‡æ—¥": True/False,
    "é‡è¦ä¼šè®®æœŸ": True/False
}
```

## ğŸ¤– ç¬¬äºŒæ­¥ï¼šLLMé©±åŠ¨çš„æ¶¨åœåŸå› åˆ†æ

### 2.1 åŸå› åˆ†æAgent

```python
# æ–‡ä»¶ï¼šagents/limitup_analyzer_agent.py

class LimitUpAnalyzerAgent:
    """æ¶¨åœåŸå› åˆ†æAgentï¼ˆLLMé©±åŠ¨ï¼‰"""
    
    def __init__(self, llm_client):
        self.llm = llm_client
        self.knowledge_base = LimitUpKnowledgeBase()
    
    async def analyze_limitup_reason(
        self, 
        stock_code: str,
        date: str,
        features: dict
    ) -> dict:
        """åˆ†ææ¶¨åœåŸå› """
        
        # 1. æ„å»ºåˆ†ææç¤ºè¯
        prompt = self._build_analysis_prompt(stock_code, date, features)
        
        # 2. è°ƒç”¨LLMåˆ†æ
        analysis = await self.llm.chat_completion(
            messages=[
                {"role": "system", "content": self._get_system_prompt()},
                {"role": "user", "content": prompt}
            ]
        )
        
        # 3. è§£æåˆ†æç»“æœ
        result = self._parse_analysis(analysis)
        
        # 4. å­˜å…¥çŸ¥è¯†åº“
        self.knowledge_base.save_analysis(stock_code, date, result)
        
        return result
    
    def _get_system_prompt(self) -> str:
        return """
        ä½ æ˜¯ä¸€ä½èµ„æ·±çš„Aè‚¡æ¶¨åœæ¿åˆ†æä¸“å®¶ï¼Œæ“…é•¿ä»å¤šä¸ªç»´åº¦åˆ†ææ¶¨åœåŸå› ã€‚
        
        åˆ†ææ¡†æ¶ï¼š
        1. **ä¸»å› åˆ†æ**ï¼šæ‰¾å‡ºæœ€æ ¸å¿ƒçš„æ¶¨åœé©±åŠ¨å› ç´ ï¼ˆ1-2ä¸ªï¼‰
        2. **è¾…åŠ©å› ç´ **ï¼šåˆ†æä¿ƒæˆæ¶¨åœçš„æ¬¡è¦å› ç´ ï¼ˆ2-3ä¸ªï¼‰
        3. **å¸‚åœºç¯å¢ƒ**ï¼šè¯„ä¼°å½“æ—¶çš„å¸‚åœºèƒŒæ™¯å’Œæƒ…ç»ª
        4. **èµ„é‡‘æ€§è´¨**ï¼šåˆ¤æ–­ä¸»åŠ›èµ„é‡‘ç±»å‹ï¼ˆæ¸¸èµ„/æœºæ„/æ•£æˆ·ï¼‰
        5. **æŒç»­æ€§åˆ¤æ–­**ï¼šé¢„æµ‹æ¶¨åœæ¿çš„æŒç»­æ€§ï¼ˆ1-5å¤©ï¼‰
        6. **é£é™©å› ç´ **ï¼šè¯†åˆ«å¯èƒ½å¯¼è‡´å¤±è´¥çš„é£é™©ç‚¹
        
        è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š
        {
            "main_reason": "ä¸»è¦åŸå› ",
            "main_reason_category": "é¢˜æ/æŠ€æœ¯/èµ„é‡‘/æ¿å—/æ¶ˆæ¯",
            "supporting_factors": ["è¾…åŠ©å› ç´ 1", "è¾…åŠ©å› ç´ 2"],
            "market_env": "å¸‚åœºç¯å¢ƒæè¿°",
            "fund_type": "æ¸¸èµ„/æœºæ„/æ··åˆ",
            "sustainability_score": 0-100,
            "risk_factors": ["é£é™©1", "é£é™©2"],
            "next_day_limitup_probability": 0-1
        }
        """
    
    def _build_analysis_prompt(self, stock_code, date, features) -> str:
        return f"""
        è¯·åˆ†æä»¥ä¸‹è‚¡ç¥¨çš„æ¶¨åœåŸå› ï¼š
        
        **è‚¡ç¥¨ä»£ç **: {stock_code}
        **æ—¥æœŸ**: {date}
        
        **æŠ€æœ¯æŒ‡æ ‡**:
        - è¿æ¿å¤©æ•°: {features['è¿æ¿å¤©æ•°']}
        - å°æ¿å¼ºåº¦: {features['å°æ¿å¼ºåº¦']}
        - æ¶¨åœæ—¶é—´: {features['æ¶¨åœæ—¶é—´']}
        - æ¢æ‰‹ç‡: {features['æ¢æ‰‹ç‡']}%
        - é‡æ¯”: {features['é‡æ¯”']}
        
        **æ¿å—æƒ…å†µ**:
        - æ‰€å±æ¿å—: {features['æ‰€å±æ¿å—']}
        - æ¿å—æ¶¨åœæ•°: {features['æ¿å—æ¶¨åœæ•°']}
        - æ¿å—é¾™å¤´åœ°ä½: {features['æ¿å—é¾™å¤´åœ°ä½']}
        
        **èµ„é‡‘æµå‘**:
        - ä¸»åŠ›å‡€æµå…¥: {features['ä¸»åŠ›å‡€æµå…¥']}ä¸‡
        - è¶…å¤§å•å‡€æµå…¥: {features['è¶…å¤§å•å‡€æµå…¥']}ä¸‡
        
        **é¢˜æçƒ­åº¦**:
        - æ‰€å±é¢˜æ: {features['æ‰€å±é¢˜æ']}
        - é¢˜æçƒ­åº¦: {features['é¢˜æçƒ­åº¦åˆ†æ•°']}
        - é¢˜ææŒç»­å¤©æ•°: {features['é¢˜ææŒç»­å¤©æ•°']}
        
        **å¸‚åœºæƒ…ç»ª**:
        - å½“æ—¥æ¶¨åœæ€»æ•°: {features['æ¶¨åœæ¿æ€»æ•°']}
        - è¿æ¿é«˜åº¦: {features['è¿æ¿é«˜åº¦']}
        - ç‚¸æ¿ç‡: {features['ç‚¸æ¿ç‡']}%
        
        è¯·æ ¹æ®ä»¥ä¸Šæ•°æ®ï¼Œæ·±å…¥åˆ†ææ¶¨åœåŸå› å¹¶é¢„æµ‹æ¬¡æ—¥æ¶¨åœæ¦‚ç‡ã€‚
        """
```

### 2.2 çŸ¥è¯†åº“ç§¯ç´¯

```python
# æ–‡ä»¶ï¼šagents/limitup_knowledge_base.py

class LimitUpKnowledgeBase:
    """æ¶¨åœçŸ¥è¯†åº“"""
    
    def __init__(self):
        self.db = VectorDatabase()  # ä½¿ç”¨å‘é‡æ•°æ®åº“
        self.cache_dir = Path("workspace/limitup_knowledge")
    
    def save_analysis(self, stock_code: str, date: str, analysis: dict):
        """ä¿å­˜åˆ†æç»“æœ"""
        
        record = {
            "stock_code": stock_code,
            "date": date,
            "analysis": analysis,
            "timestamp": datetime.now().isoformat()
        }
        
        # å­˜å…¥å‘é‡æ•°æ®åº“ï¼ˆç”¨äºç›¸ä¼¼æ¡ˆä¾‹æ£€ç´¢ï¼‰
        self.db.insert(
            text=json.dumps(analysis),
            metadata=record
        )
        
        # å­˜å…¥æœ¬åœ°JSONï¼ˆå¤‡ä»½ï¼‰
        file_path = self.cache_dir / f"{date}_{stock_code}.json"
        file_path.parent.mkdir(parents=True, exist_ok=True)
        file_path.write_text(json.dumps(record, ensure_ascii=False, indent=2))
    
    def search_similar_cases(
        self, 
        query_features: dict, 
        top_k: int = 10
    ) -> List[dict]:
        """æ£€ç´¢ç›¸ä¼¼å†å²æ¡ˆä¾‹"""
        
        # æ„å»ºæŸ¥è¯¢å‘é‡
        query_text = json.dumps(query_features)
        
        # å‘é‡æ£€ç´¢
        results = self.db.search(query_text, top_k=top_k)
        
        return results
    
    def get_success_rate(
        self, 
        main_reason_category: str,
        days_ahead: int = 1
    ) -> float:
        """è·å–æŸç±»åŸå› çš„å†å²æˆåŠŸç‡"""
        
        # æŸ¥è¯¢å†å²æ•°æ®
        historical = self.db.query(
            filter={"main_reason_category": main_reason_category}
        )
        
        # è®¡ç®—æˆåŠŸç‡
        success = sum(1 for r in historical if r['next_day_limitup'])
        total = len(historical)
        
        return success / total if total > 0 else 0.5
```

## ğŸ¯ ç¬¬ä¸‰æ­¥ï¼šé¢„æµ‹æ¨¡å‹æ„å»º

### 3.1 å¤šæ¨¡å‹é›†æˆé¢„æµ‹

```python
# æ–‡ä»¶ï¼šmodels/limitup_predictor.py

class LimitUpPredictor:
    """æ¶¨åœé¢„æµ‹æ¨¡å‹ï¼ˆé›†æˆï¼‰"""
    
    def __init__(self):
        # åŸºç¡€æ¨¡å‹
        self.lgb_model = LightGBMModel()
        self.xgb_model = XGBoostModel()
        self.catboost_model = CatBoostModel()
        
        # æ·±åº¦å­¦ä¹ æ¨¡å‹
        self.transformer_model = TransformerModel()
        self.lstm_model = LSTMModel()
        
        # å…ƒå­¦ä¹ å™¨ï¼ˆStackingï¼‰
        self.meta_learner = LogisticRegression()
        
        # æ¨¡å‹æƒé‡ï¼ˆåŠ¨æ€è°ƒæ•´ï¼‰
        self.model_weights = {
            'lgb': 0.25,
            'xgb': 0.25,
            'catboost': 0.20,
            'transformer': 0.15,
            'lstm': 0.15
        }
    
    def train(self, X_train, y_train, X_val, y_val):
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        
        print("ğŸ”§ è®­ç»ƒåŸºç¡€æ¨¡å‹...")
        
        # 1. è®­ç»ƒåŸºç¡€æ¨¡å‹
        self.lgb_model.train(X_train, y_train)
        self.xgb_model.train(X_train, y_train)
        self.catboost_model.train(X_train, y_train)
        
        print("ğŸ”§ è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹...")
        
        # 2. è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹
        self.transformer_model.train(X_train, y_train, epochs=50)
        self.lstm_model.train(X_train, y_train, epochs=30)
        
        print("ğŸ”§ è®­ç»ƒå…ƒå­¦ä¹ å™¨...")
        
        # 3. ç”Ÿæˆå…ƒç‰¹å¾
        meta_features_train = self._generate_meta_features(X_train)
        meta_features_val = self._generate_meta_features(X_val)
        
        # 4. è®­ç»ƒå…ƒå­¦ä¹ å™¨
        self.meta_learner.fit(meta_features_train, y_train)
        
        # 5. è¯„ä¼°å¹¶è°ƒæ•´æƒé‡
        self._adjust_model_weights(X_val, y_val)
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """é¢„æµ‹æ¬¡æ—¥æ¶¨åœæ¦‚ç‡"""
        
        # 1. å„æ¨¡å‹é¢„æµ‹
        preds = {
            'lgb': self.lgb_model.predict_proba(X)[:, 1],
            'xgb': self.xgb_model.predict_proba(X)[:, 1],
            'catboost': self.catboost_model.predict_proba(X)[:, 1],
            'transformer': self.transformer_model.predict(X),
            'lstm': self.lstm_model.predict(X)
        }
        
        # 2. åŠ æƒèåˆ
        weighted_pred = sum(
            preds[model] * weight 
            for model, weight in self.model_weights.items()
        )
        
        # 3. å…ƒå­¦ä¹ å™¨æ ¡å‡†
        meta_features = np.column_stack(list(preds.values()))
        final_pred = self.meta_learner.predict_proba(meta_features)[:, 1]
        
        # 4. èåˆï¼ˆ70%åŠ æƒ + 30%å…ƒå­¦ä¹ ï¼‰
        return 0.7 * weighted_pred + 0.3 * final_pred
    
    def _generate_meta_features(self, X):
        """ç”Ÿæˆå…ƒç‰¹å¾"""
        return np.column_stack([
            self.lgb_model.predict_proba(X)[:, 1],
            self.xgb_model.predict_proba(X)[:, 1],
            self.catboost_model.predict_proba(X)[:, 1],
            self.transformer_model.predict(X),
            self.lstm_model.predict(X)
        ])
    
    def _adjust_model_weights(self, X_val, y_val):
        """æ ¹æ®éªŒè¯é›†è¡¨ç°è°ƒæ•´æ¨¡å‹æƒé‡"""
        
        from sklearn.metrics import roc_auc_score
        
        # è®¡ç®—å„æ¨¡å‹AUC
        aucs = {}
        for model_name, model in {
            'lgb': self.lgb_model,
            'xgb': self.xgb_model,
            'catboost': self.catboost_model,
            'transformer': self.transformer_model,
            'lstm': self.lstm_model
        }.items():
            if hasattr(model, 'predict_proba'):
                pred = model.predict_proba(X_val)[:, 1]
            else:
                pred = model.predict(X_val)
            aucs[model_name] = roc_auc_score(y_val, pred)
        
        # æ ¹æ®AUCåˆ†é…æƒé‡ï¼ˆSoftmaxï¼‰
        auc_array = np.array(list(aucs.values()))
        weights = np.exp(auc_array) / np.sum(np.exp(auc_array))
        
        self.model_weights = dict(zip(aucs.keys(), weights))
        
        print(f"âœ… æ¨¡å‹æƒé‡å·²è°ƒæ•´: {self.model_weights}")
```

## ğŸ”„ ç¬¬å››æ­¥ï¼šå¼ºåŒ–å­¦ä¹ å’Œè‡ªæˆ‘è¿›åŒ–

### 4.1 äº¤æ˜“ç¯å¢ƒå®šä¹‰

```python
# æ–‡ä»¶ï¼šrl/limitup_trading_env.py

import gym
from gym import spaces

class LimitUpTradingEnv(gym.Env):
    """æ¶¨åœæ¿äº¤æ˜“ç¯å¢ƒ"""
    
    def __init__(self, data: pd.DataFrame):
        super().__init__()
        
        self.data = data
        self.current_step = 0
        self.max_steps = len(data)
        
        # çŠ¶æ€ç©ºé—´ï¼š100+ç»´åº¦ç‰¹å¾
        self.observation_space = spaces.Box(
            low=-np.inf, 
            high=np.inf, 
            shape=(100,),  # ç‰¹å¾ç»´åº¦
            dtype=np.float32
        )
        
        # åŠ¨ä½œç©ºé—´ï¼š[ä¸ä¹°, ä¹°å…¥10%, ä¹°å…¥20%, ..., ä¹°å…¥100%]
        self.action_space = spaces.Discrete(11)
        
        # åˆå§‹åŒ–çŠ¶æ€
        self.portfolio_value = 100000  # åˆå§‹èµ„é‡‘10ä¸‡
        self.position = 0
        self.cash = self.portfolio_value
        
    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        self.current_step = 0
        self.cash = self.portfolio_value
        self.position = 0
        return self._get_observation()
    
    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        
        # è·å–å½“å‰çŠ¶æ€
        current_features = self.data.iloc[self.current_step]
        current_price = current_features['close']
        
        # æ‰§è¡Œä¹°å…¥åŠ¨ä½œ
        if action > 0:
            # action: 1-10 å¯¹åº” 10%-100%ä»“ä½
            position_pct = action * 0.1
            buy_amount = self.cash * position_pct
            self.position += buy_amount / current_price
            self.cash -= buy_amount
        
        # ç§»åŠ¨åˆ°ä¸‹ä¸€å¤©
        self.current_step += 1
        
        # è·å–ä¸‹ä¸€å¤©çš„ä»·æ ¼å’Œç»“æœ
        if self.current_step < self.max_steps:
            next_features = self.data.iloc[self.current_step]
            next_price = next_features['close']
            next_day_limitup = next_features['next_day_limitup']
            
            # è®¡ç®—æ”¶ç›Š
            if self.position > 0:
                position_value = self.position * next_price
                total_value = self.cash + position_value
                
                # å¦‚æœæ¬¡æ—¥æ¶¨åœï¼Œè·å¾—10%æ”¶ç›Š
                if next_day_limitup:
                    reward = (position_value - self.position * current_price) / self.portfolio_value
                    reward = reward * 10  # æ”¾å¤§å¥–åŠ±ä¿¡å·
                else:
                    # å¦‚æœæ²¡æ¶¨åœï¼Œæ ¹æ®å®é™…æ¶¨è·Œå¹…è®¡ç®—
                    actual_return = (next_price - current_price) / current_price
                    reward = actual_return * (position_value / self.portfolio_value)
                
                # å–å‡ºï¼ˆT+1ï¼‰
                self.cash = total_value
                self.position = 0
            else:
                reward = 0
        else:
            reward = 0
            next_features = None
        
        done = (self.current_step >= self.max_steps - 1)
        info = {
            'portfolio_value': self.cash + self.position * (next_price if next_features is not None else current_price),
            'position': self.position,
            'cash': self.cash
        }
        
        obs = self._get_observation() if not done else None
        
        return obs, reward, done, info
    
    def _get_observation(self):
        """è·å–å½“å‰è§‚æµ‹"""
        features = self.data.iloc[self.current_step]
        return features.values.astype(np.float32)
```

### 4.2 å¼ºåŒ–å­¦ä¹ Agent

```python
# æ–‡ä»¶ï¼šrl/limitup_rl_agent.py

from stable_baselines3 import PPO, DQN
from stable_baselines3.common.vec_env import DummyVecEnv

class LimitUpRLAgent:
    """æ¶¨åœæ¿å¼ºåŒ–å­¦ä¹ Agent"""
    
    def __init__(self, env, algorithm='PPO'):
        self.env = DummyVecEnv([lambda: env])
        
        if algorithm == 'PPO':
            self.model = PPO(
                'MlpPolicy',
                self.env,
                learning_rate=3e-4,
                n_steps=2048,
                batch_size=64,
                n_epochs=10,
                gamma=0.99,
                gae_lambda=0.95,
                clip_range=0.2,
                verbose=1,
                tensorboard_log="./logs/ppo_limitup/"
            )
        elif algorithm == 'DQN':
            self.model = DQN(
                'MlpPolicy',
                self.env,
                learning_rate=1e-4,
                buffer_size=50000,
                learning_starts=1000,
                batch_size=32,
                tau=0.005,
                gamma=0.99,
                train_freq=4,
                gradient_steps=1,
                target_update_interval=1000,
                verbose=1,
                tensorboard_log="./logs/dqn_limitup/"
            )
    
    def train(self, total_timesteps=100000):
        """è®­ç»ƒAgent"""
        print(f"ğŸš€ å¼€å§‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæ€»æ­¥æ•°: {total_timesteps}")
        self.model.learn(total_timesteps=total_timesteps)
        print("âœ… è®­ç»ƒå®Œæˆ")
    
    def predict(self, obs):
        """é¢„æµ‹æœ€ä½³åŠ¨ä½œ"""
        action, _states = self.model.predict(obs, deterministic=True)
        return action
    
    def save(self, path):
        """ä¿å­˜æ¨¡å‹"""
        self.model.save(path)
    
    def load(self, path):
        """åŠ è½½æ¨¡å‹"""
        if isinstance(self.model, PPO):
            self.model = PPO.load(path, env=self.env)
        else:
            self.model = DQN.load(path, env=self.env)
```

## ğŸŒ± ç¬¬äº”æ­¥ï¼šåœ¨çº¿å­¦ä¹ å’Œè‡ªæˆ‘è¿›åŒ–

### 5.1 åœ¨çº¿å­¦ä¹ ç³»ç»Ÿ

```python
# æ–‡ä»¶ï¼šonline_learning/limitup_online_learner.py

class LimitUpOnlineLearner:
    """æ¶¨åœæ¿åœ¨çº¿å­¦ä¹ ç³»ç»Ÿ"""
    
    def __init__(self, predictor, rl_agent):
        self.predictor = predictor
        self.rl_agent = rl_agent
        self.experience_buffer = deque(maxlen=10000)
        self.performance_tracker = PerformanceTracker()
    
    def daily_update(self, date: str):
        """æ¯æ—¥æ›´æ–°"""
        
        print(f"ğŸ“… {date} æ¯æ—¥æ›´æ–°å¼€å§‹...")
        
        # 1. è·å–æ˜¨æ—¥é¢„æµ‹ç»“æœ
        yesterday_predictions = self._load_predictions(date - timedelta(days=1))
        
        # 2. è·å–ä»Šæ—¥å®é™…ç»“æœ
        today_actual = self._get_actual_results(date)
        
        # 3. è®¡ç®—é¢„æµ‹å‡†ç¡®ç‡
        accuracy = self._calculate_accuracy(yesterday_predictions, today_actual)
        self.performance_tracker.log(date, accuracy)
        
        print(f"ğŸ“Š é¢„æµ‹å‡†ç¡®ç‡: {accuracy:.2%}")
        
        # 4. å¢é‡è®­ç»ƒé¢„æµ‹æ¨¡å‹
        if len(self.experience_buffer) >= 100:
            print("ğŸ”§ å¢é‡è®­ç»ƒé¢„æµ‹æ¨¡å‹...")
            X_new, y_new = self._prepare_training_data()
            self.predictor.incremental_train(X_new, y_new)
        
        # 5. æ›´æ–°å¼ºåŒ–å­¦ä¹ Agent
        print("ğŸ”§ æ›´æ–°RL Agent...")
        self._update_rl_agent(yesterday_predictions, today_actual)
        
        # 6. æ¨¡å‹è¯„ä¼°å’Œé€‰æ‹©
        if date.day == 1:  # æ¯æœˆåˆè¯„ä¼°
            self._monthly_model_evaluation()
        
        # 7. è¶…å‚æ•°è‡ªé€‚åº”è°ƒæ•´
        if self.performance_tracker.is_declining():
            self._adjust_hyperparameters()
        
        print(f"âœ… {date} æ¯æ—¥æ›´æ–°å®Œæˆ")
    
    def _update_rl_agent(self, predictions, actuals):
        """æ›´æ–°RL Agent"""
        
        # æ„å»ºç»éªŒ
        for pred, actual in zip(predictions, actuals):
            state = pred['features']
            action = pred['action']
            reward = self._calculate_reward(pred, actual)
            next_state = actual['features']
            done = True
            
            experience = (state, action, reward, next_state, done)
            self.experience_buffer.append(experience)
        
        # ä»ç»éªŒæ± é‡‡æ ·è®­ç»ƒ
        if len(self.experience_buffer) >= 64:
            batch = random.sample(self.experience_buffer, 64)
            self.rl_agent.train_on_batch(batch)
    
    def _calculate_reward(self, prediction, actual):
        """è®¡ç®—å¥–åŠ±"""
        
        pred_prob = prediction['limitup_probability']
        actual_limitup = actual['limitup']
        actual_return = actual['return']
        
        # å¥–åŠ±å‡½æ•°è®¾è®¡
        if pred_prob > 0.7 and actual_limitup:
            # é«˜ç½®ä¿¡åº¦é¢„æµ‹æˆåŠŸï¼Œå¤§å¥–åŠ±
            reward = 10.0
        elif pred_prob > 0.7 and not actual_limitup:
            # é«˜ç½®ä¿¡åº¦é¢„æµ‹å¤±è´¥ï¼Œå¤§æƒ©ç½š
            reward = -5.0 if actual_return < 0 else -2.0
        elif pred_prob < 0.3 and not actual_limitup:
            # ä½ç½®ä¿¡åº¦æ­£ç¡®è§„é¿ï¼Œå°å¥–åŠ±
            reward = 2.0
        elif pred_prob < 0.3 and actual_limitup:
            # ä½ç½®ä¿¡åº¦é”™å¤±æœºä¼šï¼Œå°æƒ©ç½š
            reward = -1.0
        else:
            # ä¸­ç­‰ç½®ä¿¡åº¦ï¼Œæ ¹æ®å®é™…æ”¶ç›Š
            reward = actual_return * 5.0
        
        return reward
    
    def _monthly_model_evaluation(self):
        """æœˆåº¦æ¨¡å‹è¯„ä¼°"""
        
        print("ğŸ“Š æ‰§è¡Œæœˆåº¦æ¨¡å‹è¯„ä¼°...")
        
        # è·å–æœ€è¿‘30å¤©çš„è¡¨ç°
        recent_performance = self.performance_tracker.get_recent(days=30)
        
        # è¯„ä¼°å„å­æ¨¡å‹
        model_scores = {}
        for model_name in ['lgb', 'xgb', 'catboost', 'transformer', 'lstm']:
            score = self._evaluate_single_model(model_name)
            model_scores[model_name] = score
        
        # åŠ¨æ€è°ƒæ•´æ¨¡å‹æƒé‡
        self.predictor._adjust_model_weights_by_score(model_scores)
        
        print(f"âœ… æ¨¡å‹æƒé‡å·²æ›´æ–°: {self.predictor.model_weights}")
    
    def _adjust_hyperparameters(self):
        """è‡ªé€‚åº”è°ƒæ•´è¶…å‚æ•°"""
        
        print("ğŸ”§ æ€§èƒ½ä¸‹é™ï¼Œè°ƒæ•´è¶…å‚æ•°...")
        
        # é™ä½å­¦ä¹ ç‡
        current_lr = self.predictor.lgb_model.learning_rate
        new_lr = current_lr * 0.8
        self.predictor.lgb_model.learning_rate = new_lr
        
        # å¢åŠ æ­£åˆ™åŒ–
        current_reg = self.predictor.lgb_model.reg_lambda
        new_reg = current_reg * 1.2
        self.predictor.lgb_model.reg_lambda = new_reg
        
        print(f"âœ… å­¦ä¹ ç‡: {current_lr} -> {new_lr}")
        print(f"âœ… æ­£åˆ™åŒ–: {current_reg} -> {new_reg}")
```

### 5.2 æ€§èƒ½è¿½è¸ªå™¨

```python
# æ–‡ä»¶ï¼šonline_learning/performance_tracker.py

class PerformanceTracker:
    """æ€§èƒ½è¿½è¸ªå™¨"""
    
    def __init__(self):
        self.history = []
        self.metrics = {
            'accuracy': [],
            'precision': [],
            'recall': [],
            'f1_score': [],
            'auc': [],
            'profit': []
        }
    
    def log(self, date, metrics: dict):
        """è®°å½•æ¯æ—¥è¡¨ç°"""
        record = {
            'date': date,
            **metrics
        }
        self.history.append(record)
        
        for key, value in metrics.items():
            if key in self.metrics:
                self.metrics[key].append(value)
    
    def is_declining(self, window=7, threshold=0.05):
        """æ£€æµ‹æ€§èƒ½æ˜¯å¦ä¸‹é™"""
        if len(self.metrics['accuracy']) < window * 2:
            return False
        
        recent_avg = np.mean(self.metrics['accuracy'][-window:])
        prev_avg = np.mean(self.metrics['accuracy'][-window*2:-window])
        
        decline_rate = (prev_avg - recent_avg) / prev_avg
        
        return decline_rate > threshold
    
    def get_recent(self, days=30):
        """è·å–æœ€è¿‘Nå¤©çš„è¡¨ç°"""
        return self.history[-days:]
    
    def plot_performance(self):
        """ç»˜åˆ¶æ€§èƒ½æ›²çº¿"""
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        
        for idx, (metric, values) in enumerate(self.metrics.items()):
            if values:
                ax = axes[idx // 3, idx % 3]
                ax.plot(values)
                ax.set_title(metric.upper())
                ax.set_xlabel('Days')
                ax.set_ylabel(metric)
                ax.grid(True)
        
        plt.tight_layout()
        plt.savefig('workspace/performance_tracking.png')
        plt.close()
```

## ğŸ“Š ç¬¬å…­æ­¥ï¼šå®Œæ•´å·¥ä½œæµç¨‹

```python
# æ–‡ä»¶ï¼šworkflows/limitup_ai_workflow.py

class LimitUpAIWorkflow:
    """æ¶¨åœæ¿AIå®Œæ•´å·¥ä½œæµ"""
    
    def __init__(self):
        # åˆå§‹åŒ–å„ç»„ä»¶
        self.data_collector = LimitUpDataCollector()
        self.analyzer_agent = LimitUpAnalyzerAgent(llm_client)
        self.predictor = LimitUpPredictor()
        self.rl_agent = LimitUpRLAgent(env)
        self.online_learner = LimitUpOnlineLearner(self.predictor, self.rl_agent)
    
    async def run_daily_pipeline(self, date: str):
        """æ¯æ—¥è¿è¡Œæµç¨‹"""
        
        print(f"\n{'='*60}")
        print(f"ğŸš€ æ¶¨åœæ¿AIç³»ç»Ÿ - {date}")
        print(f"{'='*60}\n")
        
        # 1. æ•°æ®é‡‡é›†
        print("ğŸ“¥ æ­¥éª¤1: é‡‡é›†æ•°æ®...")
        limitup_data = self.data_collector.collect_daily_limitup(date)
        print(f"âœ… é‡‡é›†åˆ° {len(limitup_data)} åªæ¶¨åœè‚¡")
        
        # 2. åŸå› åˆ†æï¼ˆLLMï¼‰
        print("\nğŸ” æ­¥éª¤2: åˆ†ææ¶¨åœåŸå› ...")
        analyses = []
        for idx, row in limitup_data.iterrows():
            analysis = await self.analyzer_agent.analyze_limitup_reason(
                stock_code=row['code'],
                date=date,
                features=row.to_dict()
            )
            analyses.append(analysis)
        print(f"âœ… å®Œæˆ {len(analyses)} åªè‚¡ç¥¨çš„åŸå› åˆ†æ")
        
        # 3. é¢„æµ‹æ¬¡æ—¥æ¶¨åœæ¦‚ç‡
        print("\nğŸ¯ æ­¥éª¤3: é¢„æµ‹æ¬¡æ—¥æ¶¨åœæ¦‚ç‡...")
        X = self._prepare_features(limitup_data, analyses)
        predictions = self.predictor.predict(X)
        print(f"âœ… é¢„æµ‹å®Œæˆï¼Œå¹³å‡æ¦‚ç‡: {predictions.mean():.2%}")
        
        # 4. RL Agentå†³ç­–
        print("\nğŸ¤– æ­¥éª¤4: RL Agentå†³ç­–...")
        actions = []
        for obs in X:
            action = self.rl_agent.predict(obs)
            actions.append(action)
        
        # 5. ç”Ÿæˆäº¤æ˜“ä¿¡å·
        print("\nğŸ“Š æ­¥éª¤5: ç”Ÿæˆäº¤æ˜“ä¿¡å·...")
        signals = self._generate_signals(limitup_data, predictions, actions)
        top_signals = signals.nlargest(10, 'score')
        
        print(f"\nğŸ¯ Top 10 æ¨èè‚¡ç¥¨:")
        for idx, signal in top_signals.iterrows():
            print(f"  {idx+1}. {signal['code']} {signal['name']}")
            print(f"     æ¶¨åœæ¦‚ç‡: {signal['limitup_prob']:.2%}")
            print(f"     RLè¯„åˆ†: {signal['rl_score']:.2f}")
            print(f"     ç»¼åˆè¯„åˆ†: {signal['score']:.2f}\n")
        
        # 6. ä¿å­˜é¢„æµ‹ç»“æœ
        self._save_predictions(date, signals)
        
        # 7. åœ¨çº¿å­¦ä¹ æ›´æ–°ï¼ˆä½¿ç”¨å‰ä¸€å¤©çš„ç»“æœï¼‰
        if self._has_previous_day_data(date):
            print("\nğŸŒ± æ­¥éª¤6: åœ¨çº¿å­¦ä¹ æ›´æ–°...")
            await self.online_learner.daily_update(date)
        
        print(f"\nâœ… {date} å·¥ä½œæµå®Œæˆ!")
        return top_signals
    
    def _generate_signals(self, data, predictions, actions):
        """ç”Ÿæˆäº¤æ˜“ä¿¡å·"""
        signals = data.copy()
        signals['limitup_prob'] = predictions
        signals['rl_action'] = actions
        signals['rl_score'] = actions * 10  # è½¬æ¢ä¸º0-100åˆ†
        
        # ç»¼åˆè¯„åˆ† = é¢„æµ‹æ¦‚ç‡40% + RLè¯„åˆ†30% + æŠ€æœ¯é¢30%
        signals['tech_score'] = self._calculate_tech_score(data)
        signals['score'] = (
            signals['limitup_prob'] * 0.4 +
            signals['rl_score'] / 100 * 0.3 +
            signals['tech_score'] / 100 * 0.3
        ) * 100
        
        return signals.sort_values('score', ascending=False)
```

## ğŸš€ ä½¿ç”¨æŒ‡å—

### å®‰è£…ä¾èµ–

```bash
pip install stable-baselines3 gym lightgbm xgboost catboost transformers torch
pip install akshare qlib scikit-learn pandas numpy plotly
```

### å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

```python
# æ–‡ä»¶ï¼šscripts/run_limitup_ai.py

import asyncio
from workflows.limitup_ai_workflow import LimitUpAIWorkflow

async def main():
    # åˆ›å»ºå·¥ä½œæµ
    workflow = LimitUpAIWorkflow()
    
    # 1. é¦–æ¬¡è®­ç»ƒï¼ˆå†å²æ•°æ®ï¼‰
    print("ğŸ“š é¦–æ¬¡è®­ç»ƒæ¨¡å‹...")
    
    # åŠ è½½å†å²3å¹´æ•°æ®
    start_date = "2022-01-01"
    end_date = "2024-12-31"
    
    historical_data = workflow.data_collector.collect_historical_data(
        start_date, end_date
    )
    
    # è®­ç»ƒé¢„æµ‹æ¨¡å‹
    X_train, y_train = workflow._prepare_training_data(historical_data)
    workflow.predictor.train(X_train, y_train)
    
    # è®­ç»ƒRL Agent
    env = LimitUpTradingEnv(historical_data)
    workflow.rl_agent = LimitUpRLAgent(env)
    workflow.rl_agent.train(total_timesteps=100000)
    
    print("âœ… åˆå§‹è®­ç»ƒå®Œæˆ")
    
    # 2. æ¯æ—¥è¿è¡Œ
    print("\nğŸ”„ å¼€å§‹æ¯æ—¥è¿è¡Œ...")
    
    today = datetime.now().strftime("%Y-%m-%d")
    results = await workflow.run_daily_pipeline(today)
    
    print("\nğŸ“Š ä»Šæ—¥æ¨èç»“æœ:")
    print(results[['code', 'name', 'limitup_prob', 'score']].head(10))

if __name__ == "__main__":
    asyncio.run(main())
```

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

### æ€§èƒ½æŒ‡æ ‡
- **åˆå§‹å‡†ç¡®ç‡**: 55-60%
- **3ä¸ªæœˆå**: 65-70%
- **6ä¸ªæœˆå**: 70-75%
- **1å¹´å**: 75-80%+

### æˆé•¿æ›²çº¿
```
å‡†ç¡®ç‡
  â”‚
80%â”‚                                    â•±â”€â”€â”€â”€â”€
  â”‚                              â•±â”€â”€â”€â”€â”€
70%â”‚                       â•±â”€â”€â”€â”€â”€
  â”‚                 â•±â”€â”€â”€â”€â”€
60%â”‚          â•±â”€â”€â”€â”€â”€
  â”‚    â•±â”€â”€â”€â”€â”€
50%â”‚â”€â”€â”€â”€
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ—¶é—´
   0   3ä¸ªæœˆ  6ä¸ªæœˆ  9ä¸ªæœˆ  1å¹´   1.5å¹´
```

## ğŸ¯ æ€»ç»“

è¿™å¥—ç³»ç»Ÿå®ç°äº†ï¼š

âœ… **å¤šç»´åº¦åˆ†æ** - 100+ç‰¹å¾ç»´åº¦  
âœ… **LLMåŸå› åˆ†æ** - DeepSeeké©±åŠ¨çš„æ™ºèƒ½åˆ†æ  
âœ… **é›†æˆé¢„æµ‹æ¨¡å‹** - 5ä¸ªæ¨¡å‹é›†æˆ  
âœ… **å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–** - PPO/DQNè‡ªæˆ‘è¿›åŒ–  
âœ… **åœ¨çº¿å­¦ä¹ ** - æ¯æ—¥å¢é‡è®­ç»ƒ  
âœ… **çŸ¥è¯†ç§¯ç´¯** - å‘é‡æ•°æ®åº“å­˜å‚¨  
âœ… **è‡ªé€‚åº”è°ƒæ•´** - åŠ¨æ€æƒé‡å’Œè¶…å‚æ•°

**ç«‹å³å¼€å§‹**: æŒ‰ç…§æœ¬æ–‡æ¡£å®æ–½ï¼Œ3-6ä¸ªæœˆåç³»ç»Ÿå°†æ˜¾è‘—æˆé•¿ï¼

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**åˆ›å»ºæ—¶é—´**: 2025-10-30  
**é¢„è®¡å®æ–½å‘¨æœŸ**: 2-4å‘¨
