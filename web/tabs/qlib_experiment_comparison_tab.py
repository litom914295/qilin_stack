"""
Qlibå®éªŒå¯¹æ¯”åŠŸèƒ½
æ”¯æŒå¤šå®éªŒé€‰æ‹©ã€æŒ‡æ ‡å¯¹æ¯”ã€å¯è§†åŒ–åˆ†æå’Œç»Ÿè®¡æ£€éªŒ
"""
import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import logging
from scipy import stats
from pathlib import Path

logger = logging.getLogger(__name__)

# Qlibå¯¼å…¥
try:
    import qlib
    from qlib.workflow import R
    from qlib.constant import REG_CN
    QLIB_AVAILABLE = True
except ImportError as e:
    logger.warning(f"Qlibå¯¼å…¥å¤±è´¥: {e}")
    QLIB_AVAILABLE = False


def render_qlib_experiment_comparison_tab():
    """æ¸²æŸ“Qlibå®éªŒå¯¹æ¯”é¡µé¢"""
    st.header("ğŸ”¬ å®éªŒå¯¹æ¯”åˆ†æ")
    
    if not QLIB_AVAILABLE:
        st.error("âŒ Qlibæœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥")
        st.info("è¯·å…ˆå®‰è£…Qlib: `pip install pyqlib`")
        return
    
    st.markdown("""
    **å®éªŒå¯¹æ¯”åˆ†æ**å·¥å…·å¸®åŠ©æ‚¨ï¼š
    - ğŸ“Š æ¨ªå‘å¯¹æ¯”å¤šä¸ªå®éªŒçš„æ€§èƒ½æŒ‡æ ‡
    - ğŸ“ˆ å¯è§†åŒ–å¯¹æ¯”è®­ç»ƒæ›²çº¿å’Œå›æµ‹ç»“æœ
    - ğŸ” å‚æ•°å·®å¼‚åˆ†æå’Œå½±å“è¯„ä¼°
    - ğŸ“‰ ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒå’Œç›¸å…³æ€§åˆ†æ
    - ğŸ† æ™ºèƒ½æ’åå’Œæ¨¡å‹é€‰æ‹©å»ºè®®
    """)
    
    # åˆ›å»ºé€‰é¡¹å¡
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ“‹ å®éªŒé€‰æ‹©",
        "ğŸ“Š æŒ‡æ ‡å¯¹æ¯”",
        "ğŸ“ˆ å¯è§†åŒ–åˆ†æ",
        "ğŸ”¬ ç»Ÿè®¡åˆ†æ"
    ])
    
    with tab1:
        render_experiment_selector()
    
    with tab2:
        render_metrics_comparison()
    
    with tab3:
        render_visualization_comparison()
    
    with tab4:
        render_statistical_analysis()


def render_experiment_selector():
    """æ¸²æŸ“å®éªŒé€‰æ‹©å™¨"""
    st.subheader("ğŸ“‹ é€‰æ‹©è¦å¯¹æ¯”çš„å®éªŒ")
    
    # è·å–æ‰€æœ‰å¯ç”¨å®éªŒ
    try:
        all_experiments = get_all_experiments()
        
        if not all_experiments:
            st.warning("æš‚æ— å¯ç”¨å®éªŒï¼Œè¯·å…ˆåœ¨'Qlibå·¥ä½œæµ'ä¸­è¿è¡Œå®éªŒ")
            return
        
        st.success(f"âœ… æ‰¾åˆ° {len(all_experiments)} ä¸ªå®éªŒ")
        
        # æ˜¾ç¤ºå®éªŒåˆ—è¡¨
        st.markdown("### ğŸ“œ å¯ç”¨å®éªŒåˆ—è¡¨")
        
        # åˆ›å»ºå®éªŒè¡¨æ ¼
        exp_data = []
        for exp_name, exp_info in all_experiments.items():
            exp_data.append({
                "å®éªŒåç§°": exp_name,
                "è®°å½•æ•°": exp_info.get('n_recorders', 0),
                "åˆ›å»ºæ—¶é—´": exp_info.get('create_time', 'N/A'),
                "çŠ¶æ€": exp_info.get('status', 'unknown')
            })
        
        exp_df = pd.DataFrame(exp_data)
        st.dataframe(exp_df, use_container_width=True)
        
        # å¤šé€‰å®éªŒ
        st.markdown("### âœ… é€‰æ‹©å¯¹æ¯”å®éªŒ")
        
        selected_experiments = st.multiselect(
            "é€‰æ‹©2-10ä¸ªå®éªŒè¿›è¡Œå¯¹æ¯”",
            options=list(all_experiments.keys()),
            default=list(all_experiments.keys())[:min(3, len(all_experiments))],
            help="é€‰æ‹©è¦å¯¹æ¯”çš„å®éªŒï¼ˆå»ºè®®2-5ä¸ªï¼‰"
        )
        
        if len(selected_experiments) < 2:
            st.warning("âš ï¸ è¯·è‡³å°‘é€‰æ‹©2ä¸ªå®éªŒè¿›è¡Œå¯¹æ¯”")
            return
        
        if len(selected_experiments) > 10:
            st.warning("âš ï¸ é€‰æ‹©å®éªŒè¿‡å¤šå¯èƒ½å½±å“æ€§èƒ½ï¼Œå»ºè®®ä¸è¶…è¿‡10ä¸ª")
        
        # ä¿å­˜é€‰æ‹©
        st.session_state['selected_experiments'] = selected_experiments
        
        st.success(f"âœ… å·²é€‰æ‹© {len(selected_experiments)} ä¸ªå®éªŒè¿›è¡Œå¯¹æ¯”")
        
        # åŠ è½½å®éªŒæ•°æ®æŒ‰é’®
        if st.button("ğŸ”„ åŠ è½½å®éªŒæ•°æ®", type="primary", use_container_width=True):
            with st.spinner("æ­£åœ¨åŠ è½½å®éªŒæ•°æ®..."):
                load_experiment_data(selected_experiments)
        
        # æ˜¾ç¤ºå·²åŠ è½½çš„å®éªŒæ•°æ®æ‘˜è¦
        if 'experiment_data' in st.session_state and st.session_state['experiment_data']:
            st.markdown("### ğŸ“Š å·²åŠ è½½æ•°æ®æ‘˜è¦")
            
            for exp_name in selected_experiments:
                if exp_name in st.session_state['experiment_data']:
                    exp_data = st.session_state['experiment_data'][exp_name]
                    
                    with st.expander(f"ğŸ”¬ {exp_name}"):
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            st.metric("è®°å½•æ•°", len(exp_data.get('recorders', {})))
                        
                        with col2:
                            metrics = exp_data.get('metrics', {})
                            st.metric("æŒ‡æ ‡æ•°", len(metrics) if metrics else 0)
                        
                        with col3:
                            params = exp_data.get('params', {})
                            st.metric("å‚æ•°æ•°", len(params) if params else 0)
        
    except Exception as e:
        st.error(f"âŒ åŠ è½½å®éªŒåˆ—è¡¨å¤±è´¥: {e}")
        logger.error(f"åŠ è½½å®éªŒåˆ—è¡¨å¤±è´¥: {e}", exc_info=True)


def render_metrics_comparison():
    """æ¸²æŸ“æŒ‡æ ‡å¯¹æ¯”è¡¨æ ¼"""
    st.subheader("ğŸ“Š æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”")
    
    if 'experiment_data' not in st.session_state or not st.session_state['experiment_data']:
        st.info("è¯·å…ˆåœ¨'å®éªŒé€‰æ‹©'æ ‡ç­¾ä¸­åŠ è½½å®éªŒæ•°æ®")
        return
    
    experiment_data = st.session_state['experiment_data']
    
    # é€‰æ‹©å¯¹æ¯”ç»´åº¦
    comparison_type = st.radio(
        "å¯¹æ¯”ç»´åº¦",
        ["é¢„æµ‹æ€§èƒ½æŒ‡æ ‡", "å›æµ‹æ”¶ç›ŠæŒ‡æ ‡", "é£é™©æŒ‡æ ‡", "å…¨éƒ¨æŒ‡æ ‡"],
        horizontal=True
    )
    
    # æ„å»ºå¯¹æ¯”è¡¨æ ¼
    comparison_df = build_comparison_table(experiment_data, comparison_type)
    
    if comparison_df.empty:
        st.warning("æš‚æ— å¯å¯¹æ¯”çš„æŒ‡æ ‡æ•°æ®")
        return
    
    # æ˜¾ç¤ºå¯¹æ¯”è¡¨æ ¼
    st.markdown("### ğŸ“‹ æŒ‡æ ‡å¯¹æ¯”è¡¨")
    
    # æ·»åŠ é«˜äº®æœ€ä½³å€¼çš„åŠŸèƒ½
    highlight_best = st.checkbox("é«˜äº®æœ€ä½³å€¼", value=True)
    
    if highlight_best:
        # å¯¹æ¯ä¸ªæŒ‡æ ‡è¡Œè¿›è¡Œé«˜äº®ï¼ˆæ•°å€¼è¶Šå¤§è¶Šå¥½æˆ–è¶Šå°è¶Šå¥½ï¼‰
        styled_df = style_comparison_table(comparison_df)
        st.dataframe(styled_df, use_container_width=True)
    else:
        st.dataframe(comparison_df, use_container_width=True)
    
    # ä¸‹è½½å¯¹æ¯”è¡¨æ ¼
    csv = comparison_df.to_csv(index=True).encode('utf-8-sig')
    st.download_button(
        label="ğŸ“¥ ä¸‹è½½å¯¹æ¯”è¡¨æ ¼",
        data=csv,
        file_name=f"experiment_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        mime="text/csv"
    )
    
    # å‚æ•°å·®å¼‚åˆ†æ
    st.markdown("### âš™ï¸ å‚æ•°å·®å¼‚åˆ†æ")
    render_parameter_diff(experiment_data)


def render_visualization_comparison():
    """æ¸²æŸ“å¯è§†åŒ–å¯¹æ¯”åˆ†æ"""
    st.subheader("ğŸ“ˆ å¯è§†åŒ–å¯¹æ¯”åˆ†æ")
    
    if 'experiment_data' not in st.session_state or not st.session_state['experiment_data']:
        st.info("è¯·å…ˆåœ¨'å®éªŒé€‰æ‹©'æ ‡ç­¾ä¸­åŠ è½½å®éªŒæ•°æ®")
        return
    
    experiment_data = st.session_state['experiment_data']
    
    # å›¾è¡¨ç±»å‹é€‰æ‹©
    chart_type = st.selectbox(
        "é€‰æ‹©å›¾è¡¨ç±»å‹",
        [
            "æŒ‡æ ‡é›·è¾¾å›¾",
            "æ”¶ç›Šç‡å¯¹æ¯”",
            "å‡€å€¼æ›²çº¿å¯¹æ¯”",
            "å›æ’¤å¯¹æ¯”",
            "æ”¶ç›Šåˆ†å¸ƒå¯¹æ¯”",
            "IC/ICIRå¯¹æ¯”",
            "å‚æ•°æ•æ„Ÿæ€§åˆ†æ"
        ]
    )
    
    if chart_type == "æŒ‡æ ‡é›·è¾¾å›¾":
        render_radar_chart(experiment_data)
    
    elif chart_type == "æ”¶ç›Šç‡å¯¹æ¯”":
        render_returns_comparison(experiment_data)
    
    elif chart_type == "å‡€å€¼æ›²çº¿å¯¹æ¯”":
        render_equity_curves_comparison(experiment_data)
    
    elif chart_type == "å›æ’¤å¯¹æ¯”":
        render_drawdown_comparison(experiment_data)
    
    elif chart_type == "æ”¶ç›Šåˆ†å¸ƒå¯¹æ¯”":
        render_returns_distribution(experiment_data)
    
    elif chart_type == "IC/ICIRå¯¹æ¯”":
        render_ic_comparison(experiment_data)
    
    elif chart_type == "å‚æ•°æ•æ„Ÿæ€§åˆ†æ":
        render_parameter_sensitivity(experiment_data)


def render_statistical_analysis():
    """æ¸²æŸ“ç»Ÿè®¡åˆ†æ"""
    st.subheader("ğŸ”¬ ç»Ÿè®¡åˆ†æ")
    
    if 'experiment_data' not in st.session_state or not st.session_state['experiment_data']:
        st.info("è¯·å…ˆåœ¨'å®éªŒé€‰æ‹©'æ ‡ç­¾ä¸­åŠ è½½å®éªŒæ•°æ®")
        return
    
    experiment_data = st.session_state['experiment_data']
    
    # åˆ†æç±»å‹é€‰æ‹©
    analysis_type = st.selectbox(
        "é€‰æ‹©åˆ†æç±»å‹",
        [
            "ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ",
            "ç›¸å…³æ€§åˆ†æ",
            "æ’åå’Œè¯„åˆ†",
            "ç¨³å®šæ€§åˆ†æ"
        ]
    )
    
    if analysis_type == "ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ":
        render_significance_test(experiment_data)
    
    elif analysis_type == "ç›¸å…³æ€§åˆ†æ":
        render_correlation_analysis(experiment_data)
    
    elif analysis_type == "æ’åå’Œè¯„åˆ†":
        render_ranking_analysis(experiment_data)
    
    elif analysis_type == "ç¨³å®šæ€§åˆ†æ":
        render_stability_analysis(experiment_data)


# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================

def get_all_experiments() -> Dict[str, Dict[str, Any]]:
    """è·å–æ‰€æœ‰å¯ç”¨å®éªŒ"""
    try:
        experiments = {}
        
        # æ–¹æ³•1: ä»session stateè·å–
        if 'workflow_executions' in st.session_state:
            for execution in st.session_state['workflow_executions']:
                exp_name = execution.get('experiment_name')
                if exp_name:
                    experiments[exp_name] = {
                        'create_time': execution.get('timestamp', 'N/A'),
                        'status': execution.get('status', 'unknown'),
                        'n_recorders': 1
                    }
        
        # æ–¹æ³•2: ä»MLflowç›®å½•æ‰«æ
        try:
            mlruns_dir = Path("mlruns")
            if mlruns_dir.exists():
                for exp_dir in mlruns_dir.iterdir():
                    if exp_dir.is_dir() and exp_dir.name.isdigit():
                        # å°è¯•è¯»å–å®éªŒå…ƒæ•°æ®
                        meta_file = exp_dir / "meta.yaml"
                        if meta_file.exists():
                            try:
                                import yaml
                                with open(meta_file, 'r') as f:
                                    meta = yaml.safe_load(f)
                                    exp_name = meta.get('name', exp_dir.name)
                                    
                                    if exp_name not in experiments:
                                        # ç»Ÿè®¡recorderæ•°é‡
                                        n_recorders = len([d for d in exp_dir.iterdir() 
                                                         if d.is_dir() and len(d.name) == 32])
                                        
                                        experiments[exp_name] = {
                                            'create_time': datetime.fromtimestamp(
                                                exp_dir.stat().st_mtime
                                            ).strftime('%Y-%m-%d %H:%M:%S'),
                                            'status': 'completed',
                                            'n_recorders': n_recorders
                                        }
                            except Exception as e:
                                logger.debug(f"è¯»å–å®éªŒå…ƒæ•°æ®å¤±è´¥: {e}")
        except Exception as e:
            logger.debug(f"æ‰«æMLflowç›®å½•å¤±è´¥: {e}")
        
        return experiments
        
    except Exception as e:
        logger.error(f"è·å–å®éªŒåˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        return {}


def load_experiment_data(experiment_names: List[str]):
    """åŠ è½½å®éªŒæ•°æ®"""
    try:
        experiment_data = {}
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, exp_name in enumerate(experiment_names):
            status_text.text(f"æ­£åœ¨åŠ è½½: {exp_name} ({i+1}/{len(experiment_names)})")
            
            try:
                # ä»MLflowåŠ è½½
                exp = R.get_exp(experiment_name=exp_name, create=False)
                recorders = exp.list_recorders()
                
                if not recorders:
                    logger.warning(f"å®éªŒ {exp_name} æ— è®°å½•")
                    continue
                
                # è·å–æœ€æ–°çš„recorderï¼ˆå‡è®¾ç¬¬ä¸€ä¸ªæ˜¯æœ€æ–°çš„ï¼‰
                recorder = list(recorders.values())[0]
                
                # æå–æŒ‡æ ‡å’Œå‚æ•°
                metrics = {}
                params = {}
                
                try:
                    metrics = recorder.list_metrics()
                except Exception as e:
                    logger.debug(f"æå–æŒ‡æ ‡å¤±è´¥: {e}")
                
                try:
                    params = recorder.list_params()
                except Exception as e:
                    logger.debug(f"æå–å‚æ•°å¤±è´¥: {e}")
                
                experiment_data[exp_name] = {
                    'recorders': recorders,
                    'recorder': recorder,
                    'metrics': metrics,
                    'params': params,
                    'status': recorder.status if hasattr(recorder, 'status') else 'unknown'
                }
                
            except Exception as e:
                logger.error(f"åŠ è½½å®éªŒ {exp_name} å¤±è´¥: {e}")
                st.warning(f"âš ï¸ åŠ è½½å®éªŒ {exp_name} å¤±è´¥: {e}")
            
            progress_bar.progress((i + 1) / len(experiment_names))
        
        progress_bar.empty()
        status_text.empty()
        
        # ä¿å­˜åˆ°session state
        st.session_state['experiment_data'] = experiment_data
        
        st.success(f"âœ… æˆåŠŸåŠ è½½ {len(experiment_data)} ä¸ªå®éªŒçš„æ•°æ®")
        
    except Exception as e:
        st.error(f"âŒ åŠ è½½å®éªŒæ•°æ®å¤±è´¥: {e}")
        logger.error(f"åŠ è½½å®éªŒæ•°æ®å¤±è´¥: {e}", exc_info=True)


def build_comparison_table(experiment_data: Dict, comparison_type: str) -> pd.DataFrame:
    """æ„å»ºå¯¹æ¯”è¡¨æ ¼"""
    try:
        # å®šä¹‰æŒ‡æ ‡åˆ†ç±»
        prediction_metrics = ['IC', 'ICIR', 'Rank IC', 'Rank ICIR']
        backtest_metrics = ['ç´¯è®¡æ”¶ç›Šç‡', 'å¹´åŒ–æ”¶ç›Šç‡', 'å¤æ™®æ¯”ç‡', 'æœ€å¤§å›æ’¤', 'èƒœç‡']
        risk_metrics = ['æ³¢åŠ¨ç‡', 'æœ€å¤§å›æ’¤', 'VaR', 'CVaR', 'ä¸‹è¡Œæ³¢åŠ¨ç‡']
        
        # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡
        all_metrics = set()
        for exp_data in experiment_data.values():
            metrics = exp_data.get('metrics', {})
            all_metrics.update(metrics.keys())
        
        # æ ¹æ®å¯¹æ¯”ç±»å‹ç­›é€‰æŒ‡æ ‡
        if comparison_type == "é¢„æµ‹æ€§èƒ½æŒ‡æ ‡":
            selected_metrics = [m for m in all_metrics if any(pm in m for pm in prediction_metrics)]
        elif comparison_type == "å›æµ‹æ”¶ç›ŠæŒ‡æ ‡":
            selected_metrics = [m for m in all_metrics if any(bm in m for bm in backtest_metrics)]
        elif comparison_type == "é£é™©æŒ‡æ ‡":
            selected_metrics = [m for m in all_metrics if any(rm in m for rm in risk_metrics)]
        else:  # å…¨éƒ¨æŒ‡æ ‡
            selected_metrics = list(all_metrics)
        
        if not selected_metrics:
            return pd.DataFrame()
        
        # æ„å»ºè¡¨æ ¼
        table_data = {}
        for exp_name, exp_data in experiment_data.items():
            metrics = exp_data.get('metrics', {})
            table_data[exp_name] = {metric: metrics.get(metric, np.nan) 
                                   for metric in selected_metrics}
        
        df = pd.DataFrame(table_data)
        
        # è½¬ç½®ï¼Œä½¿å®éªŒåç§°ä¸ºåˆ—
        df = df.T
        
        # æ’åºæŒ‡æ ‡åˆ—
        df = df[sorted(df.columns)]
        
        return df
        
    except Exception as e:
        logger.error(f"æ„å»ºå¯¹æ¯”è¡¨æ ¼å¤±è´¥: {e}", exc_info=True)
        return pd.DataFrame()


def style_comparison_table(df: pd.DataFrame) -> pd.DataFrame:
    """æ ·å¼åŒ–å¯¹æ¯”è¡¨æ ¼ï¼Œé«˜äº®æœ€ä½³å€¼"""
    try:
        # å®šä¹‰æŒ‡æ ‡æ–¹å‘ï¼ˆTrue=è¶Šå¤§è¶Šå¥½ï¼ŒFalse=è¶Šå°è¶Šå¥½ï¼‰
        metric_directions = {
            'IC': True,
            'ICIR': True,
            'Rank IC': True,
            'Rank ICIR': True,
            'ç´¯è®¡æ”¶ç›Šç‡': True,
            'å¹´åŒ–æ”¶ç›Šç‡': True,
            'å¤æ™®æ¯”ç‡': True,
            'èƒœç‡': True,
            'æœ€å¤§å›æ’¤': False,
            'æ³¢åŠ¨ç‡': False,
            'VaR': False,
            'CVaR': False,
            'ä¸‹è¡Œæ³¢åŠ¨ç‡': False
        }
        
        def highlight_best(s):
            """é«˜äº®æœ€ä½³å€¼"""
            if s.name not in df.columns:
                return [''] * len(s)
            
            # åˆ¤æ–­æ–¹å‘
            is_higher_better = True
            for metric_name, direction in metric_directions.items():
                if metric_name in s.name:
                    is_higher_better = direction
                    break
            
            # æ‰¾åˆ°æœ€ä½³å€¼
            if is_higher_better:
                best_idx = s.idxmax()
            else:
                best_idx = s.idxmin()
            
            # åˆ›å»ºæ ·å¼
            return ['background-color: #90EE90' if idx == best_idx else '' 
                   for idx in s.index]
        
        # åº”ç”¨æ ·å¼
        styled = df.style.apply(highlight_best, axis=0)
        
        # æ ¼å¼åŒ–æ•°å€¼
        styled = styled.format("{:.4f}", na_rep="-")
        
        return styled
        
    except Exception as e:
        logger.error(f"æ ·å¼åŒ–è¡¨æ ¼å¤±è´¥: {e}", exc_info=True)
        return df


def render_parameter_diff(experiment_data: Dict):
    """æ¸²æŸ“å‚æ•°å·®å¼‚åˆ†æ"""
    try:
        # æ”¶é›†æ‰€æœ‰å‚æ•°
        all_params = {}
        for exp_name, exp_data in experiment_data.items():
            params = exp_data.get('params', {})
            all_params[exp_name] = params
        
        if not all_params:
            st.info("æš‚æ— å‚æ•°æ•°æ®")
            return
        
        # æ‰¾å‡ºæœ‰å·®å¼‚çš„å‚æ•°
        param_keys = set()
        for params in all_params.values():
            param_keys.update(params.keys())
        
        diff_params = {}
        for key in param_keys:
            values = [all_params[exp].get(key, None) for exp in all_params.keys()]
            # å¦‚æœä¸æ˜¯æ‰€æœ‰å€¼éƒ½ç›¸åŒ
            if len(set(str(v) for v in values)) > 1:
                diff_params[key] = {exp: all_params[exp].get(key, 'N/A') 
                                   for exp in all_params.keys()}
        
        if not diff_params:
            st.info("âœ… æ‰€æœ‰å®éªŒçš„å‚æ•°å®Œå…¨ç›¸åŒ")
            return
        
        st.warning(f"âš ï¸ å‘ç° {len(diff_params)} ä¸ªå‚æ•°å­˜åœ¨å·®å¼‚")
        
        # æ˜¾ç¤ºå·®å¼‚å‚æ•°è¡¨æ ¼
        diff_df = pd.DataFrame(diff_params).T
        st.dataframe(diff_df, use_container_width=True)
        
    except Exception as e:
        logger.error(f"å‚æ•°å·®å¼‚åˆ†æå¤±è´¥: {e}", exc_info=True)
        st.error(f"âŒ å‚æ•°å·®å¼‚åˆ†æå¤±è´¥: {e}")


def render_radar_chart(experiment_data: Dict):
    """æ¸²æŸ“é›·è¾¾å›¾"""
    try:
        st.markdown("### ğŸ¯ å…³é”®æŒ‡æ ‡é›·è¾¾å›¾")
        
        # é€‰æ‹©è¦å±•ç¤ºçš„æŒ‡æ ‡
        default_metrics = ['IC', 'ICIR', 'å¤æ™®æ¯”ç‡', 'å¹´åŒ–æ”¶ç›Šç‡']
        
        all_metrics = set()
        for exp_data in experiment_data.values():
            all_metrics.update(exp_data.get('metrics', {}).keys())
        
        selected_metrics = st.multiselect(
            "é€‰æ‹©æŒ‡æ ‡",
            options=sorted(all_metrics),
            default=[m for m in default_metrics if m in all_metrics]
        )
        
        if not selected_metrics:
            st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæŒ‡æ ‡")
            return
        
        # æ„å»ºé›·è¾¾å›¾æ•°æ®
        fig = go.Figure()
        
        for exp_name, exp_data in experiment_data.items():
            metrics = exp_data.get('metrics', {})
            values = [metrics.get(m, 0) for m in selected_metrics]
            
            # å½’ä¸€åŒ–åˆ°0-1
            max_vals = [max(abs(experiment_data[e].get('metrics', {}).get(m, 0)) 
                           for e in experiment_data.keys()) for m in selected_metrics]
            normalized_values = [v / max_v if max_v != 0 else 0 
                                for v, max_v in zip(values, max_vals)]
            
            fig.add_trace(go.Scatterpolar(
                r=normalized_values + [normalized_values[0]],  # é—­åˆ
                theta=selected_metrics + [selected_metrics[0]],
                fill='toself',
                name=exp_name
            ))
        
        fig.update_layout(
            polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
            showlegend=True,
            title="å…³é”®æŒ‡æ ‡å¯¹æ¯”ï¼ˆå½’ä¸€åŒ–ï¼‰"
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
    except Exception as e:
        logger.error(f"é›·è¾¾å›¾æ¸²æŸ“å¤±è´¥: {e}", exc_info=True)
        st.error(f"âŒ é›·è¾¾å›¾æ¸²æŸ“å¤±è´¥: {e}")


def render_returns_comparison(experiment_data: Dict):
    """æ¸²æŸ“æ”¶ç›Šç‡å¯¹æ¯”æŸ±çŠ¶å›¾"""
    try:
        st.markdown("### ğŸ“Š æ”¶ç›Šç‡å¯¹æ¯”")
        
        # æ”¶é›†æ”¶ç›Šç‡æ•°æ®
        return_metrics = ['ç´¯è®¡æ”¶ç›Šç‡', 'å¹´åŒ–æ”¶ç›Šç‡', 'æœ€å¤§å›æ’¤']
        
        data_dict = {metric: [] for metric in return_metrics}
        exp_names = []
        
        for exp_name, exp_data in experiment_data.items():
            metrics = exp_data.get('metrics', {})
            exp_names.append(exp_name)
            
            for metric in return_metrics:
                value = metrics.get(metric, 0)
                # å¦‚æœæ˜¯ç™¾åˆ†æ¯”æ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºæ•°å€¼
                if isinstance(value, str) and '%' in value:
                    value = float(value.replace('%', '')) / 100
                data_dict[metric].append(value)
        
        # åˆ›å»ºå­å›¾
        fig = make_subplots(
            rows=1, cols=len(return_metrics),
            subplot_titles=return_metrics
        )
        
        colors = px.colors.qualitative.Plotly
        
        for i, metric in enumerate(return_metrics, 1):
            fig.add_trace(
                go.Bar(
                    name=metric,
                    x=exp_names,
                    y=data_dict[metric],
                    marker_color=colors[i-1],
                    showlegend=False
                ),
                row=1, col=i
            )
        
        fig.update_layout(height=400, title_text="æ”¶ç›Šç‡å¯¹æ¯”åˆ†æ")
        st.plotly_chart(fig, use_container_width=True)
        
    except Exception as e:
        logger.error(f"æ”¶ç›Šç‡å¯¹æ¯”æ¸²æŸ“å¤±è´¥: {e}", exc_info=True)
        st.error(f"âŒ æ”¶ç›Šç‡å¯¹æ¯”æ¸²æŸ“å¤±è´¥: {e}")


def render_equity_curves_comparison(experiment_data: Dict):
    """æ¸²æŸ“å‡€å€¼æ›²çº¿å¯¹æ¯”"""
    st.info("ğŸ’¡ å‡€å€¼æ›²çº¿å¯¹æ¯”åŠŸèƒ½éœ€è¦å®Œæ•´çš„å›æµ‹æ•°æ®ï¼Œå½“å‰ä»MLflowè·å–çš„æ•°æ®å¯èƒ½ä¸åŒ…å«æ—¶åºæ•°æ®")
    st.markdown("å¦‚éœ€æŸ¥çœ‹å‡€å€¼æ›²çº¿ï¼Œè¯·å‰å¾€'Qlibå›æµ‹'æ ‡ç­¾è¿è¡Œå®Œæ•´å›æµ‹")


def render_drawdown_comparison(experiment_data: Dict):
    """æ¸²æŸ“å›æ’¤å¯¹æ¯”"""
    st.info("ğŸ’¡ å›æ’¤æ›²çº¿å¯¹æ¯”åŠŸèƒ½éœ€è¦å®Œæ•´çš„å›æµ‹æ•°æ®ï¼Œå½“å‰ä»MLflowè·å–çš„æ•°æ®å¯èƒ½ä¸åŒ…å«æ—¶åºæ•°æ®")
    st.markdown("å¦‚éœ€æŸ¥çœ‹å›æ’¤æ›²çº¿ï¼Œè¯·å‰å¾€'Qlibå›æµ‹'æ ‡ç­¾è¿è¡Œå®Œæ•´å›æµ‹")


def render_returns_distribution(experiment_data: Dict):
    """æ¸²æŸ“æ”¶ç›Šåˆ†å¸ƒå¯¹æ¯”"""
    st.info("ğŸ’¡ æ”¶ç›Šåˆ†å¸ƒå¯¹æ¯”åŠŸèƒ½éœ€è¦å®Œæ•´çš„å›æµ‹æ•°æ®ï¼Œå½“å‰ä»MLflowè·å–çš„æ•°æ®å¯èƒ½ä¸åŒ…å«æ—¶åºæ•°æ®")
    st.markdown("å¦‚éœ€æŸ¥çœ‹æ”¶ç›Šåˆ†å¸ƒï¼Œè¯·å‰å¾€'Qlibå›æµ‹'æ ‡ç­¾è¿è¡Œå®Œæ•´å›æµ‹")


def render_ic_comparison(experiment_data: Dict):
    """æ¸²æŸ“IC/ICIRå¯¹æ¯”"""
    try:
        st.markdown("### ğŸ“ˆ IC/ICIRå¯¹æ¯”")
        
        # æ”¶é›†ICå’ŒICIRæ•°æ®
        ic_data = []
        icir_data = []
        exp_names = []
        
        for exp_name, exp_data in experiment_data.items():
            metrics = exp_data.get('metrics', {})
            exp_names.append(exp_name)
            
            ic_data.append(metrics.get('IC', 0))
            icir_data.append(metrics.get('ICIR', 0))
        
        # åˆ›å»ºå¯¹æ¯”å›¾
        fig = make_subplots(
            rows=1, cols=2,
            subplot_titles=("ICå¯¹æ¯”", "ICIRå¯¹æ¯”")
        )
        
        # ICæŸ±çŠ¶å›¾
        fig.add_trace(
            go.Bar(name='IC', x=exp_names, y=ic_data, marker_color='lightblue'),
            row=1, col=1
        )
        
        # ICIRæŸ±çŠ¶å›¾
        fig.add_trace(
            go.Bar(name='ICIR', x=exp_names, y=icir_data, marker_color='lightgreen'),
            row=1, col=2
        )
        
        fig.update_layout(
            height=400,
            showlegend=False,
            title_text="ICå’ŒICIRå¯¹æ¯”"
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # æ·»åŠ è¯´æ˜
        st.markdown("""
        **æŒ‡æ ‡è¯´æ˜**:
        - **IC (Information Coefficient)**: é¢„æµ‹å€¼ä¸çœŸå®æ”¶ç›Šçš„ç›¸å…³ç³»æ•°ï¼Œè¶Šé«˜è¶Šå¥½
        - **ICIR (IC/IR)**: ICçš„æ ‡å‡†åŒ–ç‰ˆæœ¬ï¼Œè€ƒè™‘äº†ç¨³å®šæ€§
        """)
        
    except Exception as e:
        logger.error(f"IC/ICIRå¯¹æ¯”æ¸²æŸ“å¤±è´¥: {e}", exc_info=True)
        st.error(f"âŒ IC/ICIRå¯¹æ¯”æ¸²æŸ“å¤±è´¥: {e}")


def render_parameter_sensitivity(experiment_data: Dict):
    """æ¸²æŸ“å‚æ•°æ•æ„Ÿæ€§åˆ†æ"""
    st.markdown("### ğŸ” å‚æ•°æ•æ„Ÿæ€§åˆ†æ")
    st.info("ğŸ’¡ å‚æ•°æ•æ„Ÿæ€§åˆ†æéœ€è¦å¤šç»„å®éªŒç³»ç»Ÿåœ°æ”¹å˜æŸä¸ªå‚æ•°ï¼Œå½“å‰åŠŸèƒ½å¾…å¢å¼º")


def render_significance_test(experiment_data: Dict):
    """æ¸²æŸ“ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ"""
    try:
        st.markdown("### ğŸ“Š ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ")
        st.markdown("æ£€éªŒä¸åŒå®éªŒçš„æ€§èƒ½å·®å¼‚æ˜¯å¦å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§")
        
        # é€‰æ‹©è¦æ£€éªŒçš„æŒ‡æ ‡
        all_metrics = set()
        for exp_data in experiment_data.values():
            all_metrics.update(exp_data.get('metrics', {}).keys())
        
        test_metric = st.selectbox(
            "é€‰æ‹©è¦æ£€éªŒçš„æŒ‡æ ‡",
            options=sorted(all_metrics)
        )
        
        if not test_metric:
            return
        
        # æ”¶é›†æ•°æ®
        exp_names = list(experiment_data.keys())
        values = [experiment_data[exp].get('metrics', {}).get(test_metric, 0) 
                 for exp in exp_names]
        
        # æ˜¾ç¤ºæ•°æ®
        st.markdown("#### ğŸ“‹ æ•°æ®æ¦‚è§ˆ")
        summary_df = pd.DataFrame({
            'å®éªŒåç§°': exp_names,
            test_metric: values
        })
        st.dataframe(summary_df, use_container_width=True)
        
        # è¿›è¡Œtæ£€éªŒï¼ˆä¸¤ä¸¤æ¯”è¾ƒï¼‰
        if len(exp_names) >= 2:
            st.markdown("#### ğŸ”¬ ä¸¤ä¸¤tæ£€éªŒç»“æœ")
            
            # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–äº†ï¼Œå®é™…åº”è¯¥ç”¨æ¯ä¸ªå®éªŒçš„å¤šæ¬¡è¿è¡Œæ•°æ®
            # å½“å‰åªæœ‰å•ä¸ªå€¼ï¼Œæ‰€ä»¥tæ£€éªŒä¸é€‚ç”¨ï¼Œæ”¹ç”¨ç®€å•æ¯”è¾ƒ
            st.info("ğŸ’¡ å½“å‰ä»…æœ‰å•æ¬¡è¿è¡Œç»“æœï¼Œæ— æ³•è¿›è¡Œä¸¥æ ¼çš„ç»Ÿè®¡æ£€éªŒ")
            st.markdown("**ç®€å•æ’å**:")
            
            ranked_df = summary_df.sort_values(test_metric, ascending=False)
            ranked_df['æ’å'] = range(1, len(ranked_df) + 1)
            st.dataframe(ranked_df[['æ’å', 'å®éªŒåç§°', test_metric]], use_container_width=True)
            
    except Exception as e:
        logger.error(f"ç»Ÿè®¡æ£€éªŒå¤±è´¥: {e}", exc_info=True)
        st.error(f"âŒ ç»Ÿè®¡æ£€éªŒå¤±è´¥: {e}")


def render_correlation_analysis(experiment_data: Dict):
    """æ¸²æŸ“ç›¸å…³æ€§åˆ†æ"""
    try:
        st.markdown("### ğŸ”— æŒ‡æ ‡ç›¸å…³æ€§åˆ†æ")
        
        # æ„å»ºæŒ‡æ ‡çŸ©é˜µ
        metrics_matrix = {}
        
        for exp_name, exp_data in experiment_data.items():
            metrics = exp_data.get('metrics', {})
            for metric_name, value in metrics.items():
                if metric_name not in metrics_matrix:
                    metrics_matrix[metric_name] = []
                metrics_matrix[metric_name].append(value)
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(metrics_matrix)
        
        # è®¡ç®—ç›¸å…³çŸ©é˜µ
        corr_matrix = df.corr()
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        fig = px.imshow(
            corr_matrix,
            labels=dict(color="ç›¸å…³ç³»æ•°"),
            x=corr_matrix.columns,
            y=corr_matrix.columns,
            color_continuous_scale='RdBu_r',
            aspect="auto"
        )
        
        fig.update_layout(
            title="æŒ‡æ ‡ç›¸å…³æ€§çƒ­åŠ›å›¾",
            height=600
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # æ˜¾ç¤ºé«˜ç›¸å…³æ€§å¯¹
        st.markdown("#### ğŸ” é«˜ç›¸å…³æ€§æŒ‡æ ‡å¯¹ï¼ˆ|r| > 0.7ï¼‰")
        
        high_corr_pairs = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_val = corr_matrix.iloc[i, j]
                if abs(corr_val) > 0.7:
                    high_corr_pairs.append({
                        'æŒ‡æ ‡1': corr_matrix.columns[i],
                        'æŒ‡æ ‡2': corr_matrix.columns[j],
                        'ç›¸å…³ç³»æ•°': f"{corr_val:.3f}"
                    })
        
        if high_corr_pairs:
            st.dataframe(pd.DataFrame(high_corr_pairs), use_container_width=True)
        else:
            st.info("æœªå‘ç°é«˜åº¦ç›¸å…³çš„æŒ‡æ ‡å¯¹")
        
    except Exception as e:
        logger.error(f"ç›¸å…³æ€§åˆ†æå¤±è´¥: {e}", exc_info=True)
        st.error(f"âŒ ç›¸å…³æ€§åˆ†æå¤±è´¥: {e}")


def render_ranking_analysis(experiment_data: Dict):
    """æ¸²æŸ“æ’åå’Œè¯„åˆ†åˆ†æ"""
    try:
        st.markdown("### ğŸ† å®éªŒæ’åå’Œç»¼åˆè¯„åˆ†")
        
        # å®šä¹‰è¯„åˆ†æƒé‡
        st.markdown("#### âš™ï¸ è®¾ç½®è¯„åˆ†æƒé‡")
        
        col1, col2 = st.columns(2)
        
        with col1:
            weight_ic = st.slider("ICæƒé‡", 0.0, 1.0, 0.3, 0.05)
            weight_icir = st.slider("ICIRæƒé‡", 0.0, 1.0, 0.2, 0.05)
            weight_return = st.slider("å¹´åŒ–æ”¶ç›Šç‡æƒé‡", 0.0, 1.0, 0.3, 0.05)
        
        with col2:
            weight_sharpe = st.slider("å¤æ™®æ¯”ç‡æƒé‡", 0.0, 1.0, 0.2, 0.05)
            weight_drawdown = st.slider("æœ€å¤§å›æ’¤æƒé‡ï¼ˆè´Ÿå‘ï¼‰", 0.0, 1.0, -0.1, 0.05)
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        scores = []
        
        for exp_name, exp_data in experiment_data.items():
            metrics = exp_data.get('metrics', {})
            
            score = 0.0
            score += weight_ic * metrics.get('IC', 0)
            score += weight_icir * metrics.get('ICIR', 0)
            score += weight_return * metrics.get('å¹´åŒ–æ”¶ç›Šç‡', 0)
            score += weight_sharpe * metrics.get('å¤æ™®æ¯”ç‡', 0)
            score += weight_drawdown * metrics.get('æœ€å¤§å›æ’¤', 0)
            
            scores.append({
                'å®éªŒåç§°': exp_name,
                'ç»¼åˆå¾—åˆ†': score,
                'IC': metrics.get('IC', 0),
                'ICIR': metrics.get('ICIR', 0),
                'å¹´åŒ–æ”¶ç›Šç‡': metrics.get('å¹´åŒ–æ”¶ç›Šç‡', 0),
                'å¤æ™®æ¯”ç‡': metrics.get('å¤æ™®æ¯”ç‡', 0),
                'æœ€å¤§å›æ’¤': metrics.get('æœ€å¤§å›æ’¤', 0)
            })
        
        # æ’åº
        scores_df = pd.DataFrame(scores).sort_values('ç»¼åˆå¾—åˆ†', ascending=False)
        scores_df['æ’å'] = range(1, len(scores_df) + 1)
        
        # é‡æ–°æ’åˆ—åˆ—é¡ºåº
        scores_df = scores_df[['æ’å', 'å®éªŒåç§°', 'ç»¼åˆå¾—åˆ†', 'IC', 'ICIR', 
                              'å¹´åŒ–æ”¶ç›Šç‡', 'å¤æ™®æ¯”ç‡', 'æœ€å¤§å›æ’¤']]
        
        st.markdown("#### ğŸ“Š æ’åç»“æœ")
        st.dataframe(scores_df, use_container_width=True)
        
        # å¯è§†åŒ–
        fig = go.Figure(data=[
            go.Bar(
                x=scores_df['å®éªŒåç§°'],
                y=scores_df['ç»¼åˆå¾—åˆ†'],
                marker_color=px.colors.sequential.Viridis,
                text=scores_df['ç»¼åˆå¾—åˆ†'].round(3),
                textposition='auto'
            )
        ])
        
        fig.update_layout(
            title="ç»¼åˆå¾—åˆ†æ’å",
            xaxis_title="å®éªŒåç§°",
            yaxis_title="ç»¼åˆå¾—åˆ†",
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # æ¨èæœ€ä½³æ¨¡å‹
        st.markdown("#### ğŸ¥‡ æ¨èæ¨¡å‹")
        best_exp = scores_df.iloc[0]
        st.success(f"""
        **æœ€ä½³å®éªŒ**: {best_exp['å®éªŒåç§°']}  
        **ç»¼åˆå¾—åˆ†**: {best_exp['ç»¼åˆå¾—åˆ†']:.3f}  
        **IC**: {best_exp['IC']:.4f} | **ICIR**: {best_exp['ICIR']:.4f}  
        **å¹´åŒ–æ”¶ç›Šç‡**: {best_exp['å¹´åŒ–æ”¶ç›Šç‡']:.2%} | **å¤æ™®æ¯”ç‡**: {best_exp['å¤æ™®æ¯”ç‡']:.3f}
        """)
        
    except Exception as e:
        logger.error(f"æ’ååˆ†æå¤±è´¥: {e}", exc_info=True)
        st.error(f"âŒ æ’ååˆ†æå¤±è´¥: {e}")


def render_stability_analysis(experiment_data: Dict):
    """æ¸²æŸ“ç¨³å®šæ€§åˆ†æ"""
    st.markdown("### ğŸ“‰ ç¨³å®šæ€§åˆ†æ")
    st.info("ğŸ’¡ ç¨³å®šæ€§åˆ†æéœ€è¦æ¯ä¸ªå®éªŒçš„å¤šæ¬¡è¿è¡Œæ•°æ®æˆ–æ—¶åºæ•°æ®ï¼Œå½“å‰åŠŸèƒ½å¾…å¢å¼º")
    st.markdown("""
    **å»ºè®®**:
    - å¯¹åŒä¸€æ¨¡å‹è¿›è¡Œå¤šæ¬¡è®­ç»ƒï¼ˆä¸åŒéšæœºç§å­ï¼‰
    - ä½¿ç”¨æ»šåŠ¨çª—å£å›æµ‹è¯„ä¼°æ—¶é—´ç¨³å®šæ€§
    - å¯¹æ¯”ä¸åŒå¸‚åœºç¯å¢ƒä¸‹çš„è¡¨ç°
    """)
