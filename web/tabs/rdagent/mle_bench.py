"""
MLE-Bench åŸºå‡†æµ‹è¯•é›†æˆ
å±•ç¤ºRD-Agentåœ¨ä¸šç•ŒåŸºå‡†æµ‹è¯•ä¸­çš„é¢†å…ˆåœ°ä½
- 75ä¸ªKaggleç«èµ›æ•°æ®é›†
- R&D-Agent vs AIDE vs Baselineæ’è¡Œæ¦œ
- ä¸€é”®è¿è¡ŒåŸºå‡†æµ‹è¯•
- è¯¦ç»†ç»“æœåˆ†æ
"""

import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime
from typing import Dict, List, Optional
import numpy as np


# MLE-Bench å®˜æ–¹æ•°æ® (ä»è®ºæ–‡å’ŒGitHubè·å–)
MLE_BENCH_RESULTS = {
    "R&D-Agent o3(R)+GPT-4.1(D)": {
        "Low": 51.52,
        "Medium": 19.3,
        "High": 26.67,
        "All": 30.22,
        "std": 1.5,
        "seeds": 6,
        "cost_per_run": "$45-75",
        "avg_time": "2.5h"
    },
    "R&D-Agent o1-preview": {
        "Low": 48.18,
        "Medium": 8.95,
        "High": 18.67,
        "All": 22.4,
        "std": 1.1,
        "seeds": 5,
        "cost_per_run": "$80-120",
        "avg_time": "4.2h"
    },
    "AIDE o1-preview": {
        "Low": 34.3,
        "Medium": 8.8,
        "High": 10.0,
        "All": 16.9,
        "std": 1.1,
        "seeds": 5,
        "cost_per_run": "$50-80",
        "avg_time": "3.8h"
    },
    "OpenHands o1-preview": {
        "Low": 30.5,
        "Medium": 7.2,
        "High": 8.5,
        "All": 14.8,
        "std": 1.3,
        "seeds": 3,
        "cost_per_run": "$40-70",
        "avg_time": "3.5h"
    }
}

# 75ä¸ªç«èµ›æ•°æ®é›†(éƒ¨åˆ†ç¤ºä¾‹)
MLE_BENCH_DATASETS = [
    {"id": 1, "name": "house-prices-advanced-regression-techniques", "difficulty": "Low", "type": "Regression"},
    {"id": 2, "name": "titanic", "difficulty": "Low", "type": "Classification"},
    {"id": 3, "name": "digit-recognizer", "difficulty": "Low", "type": "Computer Vision"},
    {"id": 4, "name": "natural-language-processing-with-disaster-tweets", "difficulty": "Low", "type": "NLP"},
    {"id": 5, "name": "spaceship-titanic", "difficulty": "Low", "type": "Classification"},
    {"id": 6, "name": "store-sales-time-series-forecasting", "difficulty": "Medium", "type": "Time Series"},
    {"id": 7, "name": "tabular-playground-series-mar-2021", "difficulty": "Medium", "type": "Tabular"},
    {"id": 8, "name": "facebook-recruiting-iii-keyword-extraction", "difficulty": "Medium", "type": "NLP"},
    {"id": 9, "name": "stanford-covid-vaccine", "difficulty": "High", "type": "Research"},
    {"id": 10, "name": "google-quest-challenge", "difficulty": "High", "type": "NLP"},
    # ... æ›´å¤šæ•°æ®é›†
]


class MLEBenchTab:
    """MLE-Bench Tab"""
    
    def __init__(self):
        self.init_session_state()
    
    def init_session_state(self):
        """åˆå§‹åŒ–session state"""
        if 'mle_bench_view' not in st.session_state:
            st.session_state.mle_bench_view = 'leaderboard'
        if 'mle_bench_running' not in st.session_state:
            st.session_state.mle_bench_running = False
        if 'mle_bench_results' not in st.session_state:
            st.session_state.mle_bench_results = None
    
    def render(self):
        """æ¸²æŸ“MLE-Benché¡µé¢"""
        st.header("ğŸ† MLE-Bench åŸºå‡†æµ‹è¯•")
        
        st.markdown("""
        **MLE-Bench** æ˜¯ä¸šç•Œæƒå¨çš„æœºå™¨å­¦ä¹ å·¥ç¨‹Agentè¯„ä¼°åŸºå‡†,åŒ…å«75ä¸ªKaggleç«èµ›æ•°æ®é›†ã€‚
        R&D-Agentç›®å‰åœ¨MLE-Benchä¸Š**æ’åç¬¬ä¸€**! ğŸ¥‡
        """)
        
        # é¡¶éƒ¨æŒ‡æ ‡
        self.render_top_metrics()
        
        st.divider()
        
        # è§†å›¾åˆ‡æ¢
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            if st.button("ğŸ† æ’è¡Œæ¦œ", use_container_width=True,
                        type="primary" if st.session_state.mle_bench_view == 'leaderboard' else "secondary"):
                st.session_state.mle_bench_view = 'leaderboard'
                st.rerun()
        with col2:
            if st.button("ğŸ“Š æ•°æ®é›†", use_container_width=True,
                        type="primary" if st.session_state.mle_bench_view == 'datasets' else "secondary"):
                st.session_state.mle_bench_view = 'datasets'
                st.rerun()
        with col3:
            if st.button("ğŸš€ è¿è¡Œæµ‹è¯•", use_container_width=True,
                        type="primary" if st.session_state.mle_bench_view == 'run_test' else "secondary"):
                st.session_state.mle_bench_view = 'run_test'
                st.rerun()
        with col4:
            if st.button("ğŸ“ˆ ç»“æœåˆ†æ", use_container_width=True,
                        type="primary" if st.session_state.mle_bench_view == 'analysis' else "secondary"):
                st.session_state.mle_bench_view = 'analysis'
                st.rerun()
        
        st.divider()
        
        # æ ¹æ®è§†å›¾æ¸²æŸ“å†…å®¹
        if st.session_state.mle_bench_view == 'leaderboard':
            self.render_leaderboard()
        elif st.session_state.mle_bench_view == 'datasets':
            self.render_datasets()
        elif st.session_state.mle_bench_view == 'run_test':
            self.render_run_test()
        elif st.session_state.mle_bench_view == 'analysis':
            self.render_analysis()
    
    def render_top_metrics(self):
        """æ¸²æŸ“é¡¶éƒ¨æŒ‡æ ‡"""
        col1, col2, col3, col4, col5 = st.columns(5)
        
        with col1:
            st.metric(
                "ğŸ¥‡ å…¨çƒæ’å",
                "1st",
                delta="é¢†å…ˆAIDE +13.3%",
                delta_color="normal"
            )
        
        with col2:
            st.metric(
                "æ€»ä½“å‡†ç¡®ç‡",
                "30.22%",
                delta="+8.3%",
                help="R&D-Agent o3+GPT4.1åœ¨æ‰€æœ‰75ä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡è¡¨ç°"
            )
        
        with col3:
            st.metric(
                "Lowéš¾åº¦",
                "51.52%",
                delta="+17.2%"
            )
        
        with col4:
            st.metric(
                "æµ‹è¯•æ•°æ®é›†",
                "75ä¸ª",
                help="æ¶µç›–åˆ†ç±»/å›å½’/NLP/CV/æ—¶åºç­‰å¤šä¸ªé¢†åŸŸ"
            )
        
        with col5:
            st.metric(
                "å¹³å‡æˆæœ¬",
                "$45-75",
                delta="-40% vs o1-preview"
            )
    
    def render_leaderboard(self):
        """æ¸²æŸ“æ’è¡Œæ¦œ"""
        st.subheader("ğŸ† MLE-Bench å…¨çƒæ’è¡Œæ¦œ")
        
        # æ’è¡Œæ¦œè¡¨æ ¼
        leaderboard_data = []
        rank = 1
        for agent, results in MLE_BENCH_RESULTS.items():
            leaderboard_data.append({
                "æ’å": f"{'ğŸ¥‡' if rank == 1 else 'ğŸ¥ˆ' if rank == 2 else 'ğŸ¥‰' if rank == 3 else ''}#{rank}",
                "Agent": agent,
                "æ€»ä½“å‡†ç¡®ç‡": f"{results['All']:.2f}%",
                "Low": f"{results['Low']:.2f}%",
                "Medium": f"{results['Medium']:.2f}%",
                "High": f"{results['High']:.2f}%",
                "æ ‡å‡†å·®": f"Â±{results['std']:.1f}",
                "ç§å­æ•°": results['seeds'],
                "å¹³å‡æˆæœ¬": results['cost_per_run'],
                "å¹³å‡æ—¶é—´": results['avg_time']
            })
            rank += 1
        
        df = pd.DataFrame(leaderboard_data)
        
        st.dataframe(
            df,
            use_container_width=True,
            hide_index=True,
            column_config={
                "æ€»ä½“å‡†ç¡®ç‡": st.column_config.ProgressColumn(
                    "æ€»ä½“å‡†ç¡®ç‡",
                    format="%.2f%%",
                    min_value=0,
                    max_value=100
                )
            }
        )
        
        st.divider()
        
        # å¯è§†åŒ–å¯¹æ¯”
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ“Š ä¸åŒéš¾åº¦è¡¨ç°å¯¹æ¯”")
            
            # åˆ†ç»„æŸ±çŠ¶å›¾
            agents = list(MLE_BENCH_RESULTS.keys())
            difficulties = ['Low', 'Medium', 'High']
            
            fig = go.Figure()
            
            for difficulty in difficulties:
                values = [MLE_BENCH_RESULTS[agent][difficulty] for agent in agents]
                fig.add_trace(go.Bar(
                    name=difficulty,
                    x=agents,
                    y=values,
                    text=[f"{v:.1f}%" for v in values],
                    textposition='outside'
                ))
            
            fig.update_layout(
                barmode='group',
                title="æŒ‰éš¾åº¦çº§åˆ«åˆ†ç±»è¡¨ç°",
                xaxis_title="Agent",
                yaxis_title="å‡†ç¡®ç‡ (%)",
                hovermode='x unified',
                height=400
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.subheader("ğŸ¯ ç»¼åˆè¡¨ç°é›·è¾¾å›¾")
            
            categories = ['Lowéš¾åº¦', 'Mediuméš¾åº¦', 'Highéš¾åº¦', 'ç¨³å®šæ€§', 'æˆæœ¬æ•ˆç›Š']
            
            fig = go.Figure()
            
            for agent, results in list(MLE_BENCH_RESULTS.items())[:3]:  # Top 3
                # å½’ä¸€åŒ–å€¼
                low_norm = results['Low'] / 60  # å½’ä¸€åŒ–åˆ°0-1
                medium_norm = results['Medium'] / 25
                high_norm = results['High'] / 30
                stability = 1 - (results['std'] / 5)  # æ ‡å‡†å·®è¶Šå°è¶Šå¥½
                
                # æˆæœ¬æ•ˆç›Š (ç®€åŒ–å¤„ç†)
                cost_val = 0.8 if 'o3' in agent else 0.5 if 'AIDE' in agent else 0.6
                
                fig.add_trace(go.Scatterpolar(
                    r=[low_norm, medium_norm, high_norm, stability, cost_val],
                    theta=categories,
                    fill='toself',
                    name=agent.split()[0]  # ç®€åŒ–åç§°
                ))
            
            fig.update_layout(
                polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
                showlegend=True,
                title="Top 3 Agentç»¼åˆå¯¹æ¯”",
                height=400
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        # å…³é”®äº®ç‚¹
        st.divider()
        st.subheader("âœ¨ R&D-Agent å…³é”®äº®ç‚¹")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.info("""
            **ğŸš€ æ€§èƒ½é¢†å…ˆ**
            - æ€»ä½“å‡†ç¡®ç‡ 30.22% (å…¨çƒç¬¬ä¸€)
            - æ¯”AIDEé«˜ 13.3ä¸ªç™¾åˆ†ç‚¹
            - Lowéš¾åº¦é¢†å…ˆ 17.2%
            """)
        
        with col2:
            st.success("""
            **ğŸ’° æˆæœ¬ä¼˜åŒ–**
            - å¹³å‡$45-75/run
            - æ¯”o1-previewèŠ‚çœ40%
            - æ··åˆæ¨¡å‹ç­–ç•¥: o3(Research) + GPT-4.1(Development)
            """)
        
        with col3:
            st.warning("""
            **ğŸ”¬ æŠ€æœ¯åˆ›æ–°**
            - Research-DevelopmentåŒAgentååŒ
            - è‡ªåŠ¨è¿›åŒ–å¾ªç¯
            - ä»£ç ç”Ÿæˆ+å®éªŒè‡ªåŠ¨åŒ–
            """)
    
    def render_datasets(self):
        """æ¸²æŸ“æ•°æ®é›†åˆ—è¡¨"""
        st.subheader("ğŸ“Š MLE-Bench æ•°æ®é›† (75ä¸ª)")
        
        # è¿‡æ»¤å™¨
        col1, col2, col3 = st.columns(3)
        
        with col1:
            difficulty_filter = st.multiselect(
                "éš¾åº¦çº§åˆ«",
                ['Low', 'Medium', 'High'],
                default=['Low', 'Medium', 'High'],
                key="mle_diff_filter"
            )
        
        with col2:
            type_filter = st.multiselect(
                "ä»»åŠ¡ç±»å‹",
                ['Regression', 'Classification', 'Computer Vision', 'NLP', 'Time Series', 'Tabular', 'Research'],
                key="mle_type_filter"
            )
        
        with col3:
            search_query = st.text_input(
                "æœç´¢",
                placeholder="è¾“å…¥æ•°æ®é›†åç§°...",
                key="mle_search"
            )
        
        # è¿‡æ»¤æ•°æ®é›†
        filtered_datasets = [
            ds for ds in MLE_BENCH_DATASETS
            if ds['difficulty'] in difficulty_filter
            and (not type_filter or ds['type'] in type_filter)
            and (not search_query or search_query.lower() in ds['name'].lower())
        ]
        
        st.info(f"æ˜¾ç¤º {len(filtered_datasets)} ä¸ªæ•°æ®é›† (å…±75ä¸ª)")
        
        # æ•°æ®é›†è¡¨æ ¼
        if filtered_datasets:
            df = pd.DataFrame(filtered_datasets)
            st.dataframe(
                df,
                use_container_width=True,
                hide_index=True,
                column_config={
                    "id": "ID",
                    "name": st.column_config.TextColumn("æ•°æ®é›†åç§°", width="large"),
                    "difficulty": st.column_config.SelectboxColumn(
                        "éš¾åº¦",
                        options=['Low', 'Medium', 'High'],
                        width="small"
                    ),
                    "type": "ç±»å‹"
                }
            )
        
        # æ•°æ®é›†ç»Ÿè®¡
        st.divider()
        st.subheader("ğŸ“ˆ æ•°æ®é›†åˆ†å¸ƒ")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # éš¾åº¦åˆ†å¸ƒ
            difficulty_counts = pd.DataFrame(MLE_BENCH_DATASETS)['difficulty'].value_counts()
            
            fig = px.pie(
                values=difficulty_counts.values,
                names=difficulty_counts.index,
                title="éš¾åº¦åˆ†å¸ƒ",
                color_discrete_sequence=px.colors.sequential.RdBu
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # ç±»å‹åˆ†å¸ƒ
            type_counts = pd.DataFrame(MLE_BENCH_DATASETS)['type'].value_counts()
            
            fig = px.bar(
                x=type_counts.index,
                y=type_counts.values,
                title="ä»»åŠ¡ç±»å‹åˆ†å¸ƒ",
                labels={'x': 'ç±»å‹', 'y': 'æ•°é‡'},
                color=type_counts.values,
                color_continuous_scale='Viridis'
            )
            st.plotly_chart(fig, use_container_width=True)
    
    def render_run_test(self):
        """æ¸²æŸ“è¿è¡Œæµ‹è¯•ç•Œé¢"""
        st.subheader("ğŸš€ è¿è¡ŒMLE-Benchæµ‹è¯•")
        
        st.warning("âš ï¸ **æ³¨æ„**: è¿è¡Œå®Œæ•´MLE-Benchæµ‹è¯•éœ€è¦å¤§é‡è®¡ç®—èµ„æºå’Œæ—¶é—´(æ•°å°æ—¶),å»ºè®®å…ˆè¿è¡Œå°è§„æ¨¡æµ‹è¯•ã€‚")
        
        # æµ‹è¯•é…ç½®
        st.markdown("### âš™ï¸ æµ‹è¯•é…ç½®")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**åŸºç¡€é…ç½®**")
            
            test_mode = st.radio(
                "æµ‹è¯•æ¨¡å¼",
                ["å¿«é€ŸéªŒè¯ (3ä¸ªæ•°æ®é›†)", "å°è§„æ¨¡æµ‹è¯• (10ä¸ªæ•°æ®é›†)", "å®Œæ•´æµ‹è¯• (75ä¸ªæ•°æ®é›†)"],
                key="mle_test_mode"
            )
            
            selected_datasets = st.multiselect(
                "é€‰æ‹©æ•°æ®é›† (å¯é€‰)",
                [ds['name'] for ds in MLE_BENCH_DATASETS],
                key="mle_selected_datasets",
                help="ç•™ç©ºåˆ™æ ¹æ®æµ‹è¯•æ¨¡å¼è‡ªåŠ¨é€‰æ‹©"
            )
            
            agent_model = st.selectbox(
                "Agentæ¨¡å‹",
                ["R&D-Agent (o3+GPT4.1)", "R&D-Agent (o1-preview)", "è‡ªå®šä¹‰é…ç½®"],
                key="mle_agent_model"
            )
        
        with col2:
            st.markdown("**é«˜çº§é…ç½®**")
            
            num_seeds = st.slider(
                "éšæœºç§å­æ•°",
                min_value=1,
                max_value=10,
                value=3,
                help="å¤šæ¬¡è¿è¡Œä»¥è®¡ç®—æ ‡å‡†å·®"
            )
            
            timeout = st.number_input(
                "å•ä¸ªæ•°æ®é›†è¶…æ—¶(åˆ†é’Ÿ)",
                min_value=30,
                max_value=300,
                value=120
            )
            
            parallel_runs = st.checkbox(
                "å¹¶è¡Œè¿è¡Œ",
                value=False,
                help="åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå¹¶è¡Œæµ‹è¯•(éœ€è¦å¤šGPU)"
            )
            
            save_logs = st.checkbox(
                "ä¿å­˜è¯¦ç»†æ—¥å¿—",
                value=True
            )
        
        st.divider()
        
        # ä¼°ç®—èµ„æº
        st.markdown("### ğŸ“Š èµ„æºä¼°ç®—")
        
        if test_mode == "å¿«é€ŸéªŒè¯ (3ä¸ªæ•°æ®é›†)":
            estimated_time = "30-60åˆ†é’Ÿ"
            estimated_cost = "$5-10"
            num_datasets = 3
        elif test_mode == "å°è§„æ¨¡æµ‹è¯• (10ä¸ªæ•°æ®é›†)":
            estimated_time = "2-4å°æ—¶"
            estimated_cost = "$20-40"
            num_datasets = 10
        else:
            estimated_time = "15-30å°æ—¶"
            estimated_cost = "$500-1000"
            num_datasets = 75
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("é¢„è®¡æ—¶é—´", estimated_time)
        with col2:
            st.metric("é¢„è®¡æˆæœ¬", estimated_cost)
        with col3:
            st.metric("æ•°æ®é›†æ•°é‡", num_datasets)
        
        st.divider()
        
        # è¿è¡ŒæŒ‰é’®
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ğŸš€ å¼€å§‹æµ‹è¯•", type="primary", use_container_width=True):
                st.session_state.mle_bench_running = True
                with st.spinner("æ­£åœ¨åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ..."):
                    self.run_mle_bench_test(num_datasets, num_seeds)
        
        with col2:
            if st.button("â¸ï¸ åœæ­¢æµ‹è¯•", use_container_width=True):
                st.session_state.mle_bench_running = False
                st.warning("æµ‹è¯•å·²åœæ­¢")
        
        # æµ‹è¯•è¿›åº¦
        if st.session_state.mle_bench_running:
            st.divider()
            st.markdown("### ğŸ“Š æµ‹è¯•è¿›åº¦")
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # æ¨¡æ‹Ÿè¿›åº¦
            for i in range(100):
                progress_bar.progress(i + 1)
                status_text.text(f"æ­£åœ¨æµ‹è¯•ç¬¬ {i % num_datasets + 1}/{num_datasets} ä¸ªæ•°æ®é›†...")
                
                if not st.session_state.mle_bench_running:
                    break
            
            if st.session_state.mle_bench_running:
                st.success("âœ… æµ‹è¯•å®Œæˆ!")
                st.session_state.mle_bench_running = False
    
    def run_mle_bench_test(self, num_datasets: int, num_seeds: int):
        """è¿è¡ŒMLE-Benchæµ‹è¯•"""
        # å®é™…å®ç°åº”è¯¥è°ƒç”¨RD-Agentçš„MLE-Bench runner
        # è¿™é‡Œæ˜¯Mockå®ç°
        
        import time
        time.sleep(2)
        
        # ç”Ÿæˆæ¨¡æ‹Ÿç»“æœ
        results = {
            'total_datasets': num_datasets,
            'seeds': num_seeds,
            'success_rate': np.random.uniform(0.25, 0.35),
            'avg_score': np.random.uniform(0.20, 0.30),
            'total_time': f"{num_datasets * 1.5:.1f}h",
            'total_cost': f"${num_datasets * 8}",
            'detailed_results': []
        }
        
        for i in range(min(num_datasets, 10)):
            results['detailed_results'].append({
                'dataset': MLE_BENCH_DATASETS[i]['name'],
                'score': np.random.uniform(0.1, 0.5),
                'time': f"{np.random.uniform(30, 180):.0f}min",
                'status': 'success' if np.random.random() > 0.2 else 'failed'
            })
        
        st.session_state.mle_bench_results = results
    
    def render_analysis(self):
        """æ¸²æŸ“ç»“æœåˆ†æ"""
        st.subheader("ğŸ“ˆ æµ‹è¯•ç»“æœåˆ†æ")
        
        if not st.session_state.mle_bench_results:
            st.info("ğŸ” è¿˜æ²¡æœ‰æµ‹è¯•ç»“æœã€‚è¯·å…ˆåœ¨'è¿è¡Œæµ‹è¯•'ä¸­æ‰§è¡ŒMLE-Benchæµ‹è¯•ã€‚")
            
            st.markdown("### ğŸ“– å‚è€ƒç»“æœ")
            st.markdown("""
            æ‚¨å¯ä»¥æŸ¥çœ‹R&D-Agentåœ¨MLE-Benchå®˜æ–¹æµ‹è¯•ä¸­çš„è¡¨ç°:
            
            - **å®Œæ•´è¿è¡Œæ—¥å¿—**: https://aka.ms/RD-Agent_MLE-Bench_O3_GPT41
            - **è®ºæ–‡åœ°å€**: https://arxiv.org/abs/2505.14738
            - **MLE-Bench GitHub**: https://github.com/openai/mle-bench
            """)
            return
        
        results = st.session_state.mle_bench_results
        
        # ç»“æœæ¦‚è§ˆ
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("æµ‹è¯•æ•°æ®é›†", results['total_datasets'])
        with col2:
            st.metric("æˆåŠŸç‡", f"{results['success_rate']*100:.1f}%")
        with col3:
            st.metric("æ€»è€—æ—¶", results['total_time'])
        with col4:
            st.metric("æ€»æˆæœ¬", results['total_cost'])
        
        st.divider()
        
        # è¯¦ç»†ç»“æœ
        st.markdown("### ğŸ“‹ è¯¦ç»†ç»“æœ")
        
        if results['detailed_results']:
            df = pd.DataFrame(results['detailed_results'])
            
            st.dataframe(
                df,
                use_container_width=True,
                hide_index=True,
                column_config={
                    "score": st.column_config.ProgressColumn(
                        "å¾—åˆ†",
                        format="%.2f",
                        min_value=0,
                        max_value=1
                    ),
                    "status": st.column_config.TextColumn(
                        "çŠ¶æ€",
                        width="small"
                    )
                }
            )
            
            # åˆ†æ•°åˆ†å¸ƒ
            st.divider()
            st.markdown("### ğŸ“Š åˆ†æ•°åˆ†å¸ƒ")
            
            fig = px.histogram(
                df,
                x='score',
                nbins=20,
                title="æµ‹è¯•åˆ†æ•°åˆ†å¸ƒ",
                labels={'score': 'åˆ†æ•°', 'count': 'æ•°é‡'},
                color_discrete_sequence=['#636EFA']
            )
            
            st.plotly_chart(fig, use_container_width=True)


def render():
    """æ¸²æŸ“å…¥å£"""
    tab = MLEBenchTab()
    tab.render()
