"""
RD-Agent åŸç”Ÿæ—¥å¿—å¯è§†åŒ–
- è‡ªåŠ¨å‘ç° trace.json
- æ”¯æŒ FileStorage pkl æ—¥å¿—
- æ—¶é—´è½´å¯è§†åŒ–
- æ­¥éª¤è¯¦æƒ…æŸ¥çœ‹
"""

import streamlit as st
import json
import pickle
from pathlib import Path
from datetime import datetime, timezone
import plotly.graph_objects as go
from typing import List, Dict, Any, Optional
import os

# å°è¯•å¯¼å…¥ä¸Šæ¸¸ RD-Agent FileStorage
try:
    from rdagent.log.storage import FileStorage
    from rdagent.log.base import Message
    HAS_RDAGENT = True
except ImportError:
    HAS_RDAGENT = False
    FileStorage = None
    Message = None


# å¯èƒ½çš„trace.jsonæ–‡ä»¶ä½ç½®
POSSIBLE_TRACE_FILES = [
    Path.cwd() / 'workspace' / 'trace.json',
    Path.home() / '.rdagent' / 'trace.json',
]
_env_path = os.getenv('RDAGENT_PATH')
if _env_path:
    POSSIBLE_TRACE_FILES.append(Path(_env_path) / 'workspace' / 'trace.json')

# å¯èƒ½çš„FileStorageæ—¥å¿—ç›®å½•
POSSIBLE_LOG_DIRS = [
    Path.cwd() / 'workspace' / 'log',
    Path.cwd() / 'log',
    Path.home() / '.rdagent' / 'log',
]
if _env_path:
    POSSIBLE_LOG_DIRS.append(Path(_env_path) / 'workspace' / 'log')
    POSSIBLE_LOG_DIRS.append(Path(_env_path) / 'log')


def _load_traces_from_json(trace_path: Optional[Path]) -> List[Dict[str, Any]]:
    """ä» trace.json è¯»å–æ—¥å¿—"""
    candidates = [trace_path] if trace_path else POSSIBLE_TRACE_FILES
    for p in candidates:
        try:
            if not p:
                continue
            if p.exists():
                text = p.read_text(encoding='utf-8')
                data = json.loads(text)
                if isinstance(data, list):
                    return data
                elif isinstance(data, dict) and 'traces' in data:
                    return data['traces']
                else:
                    return [data]
        except Exception:
            continue
    return []


def _load_traces_from_filestorage_upstream(log_dir: Path) -> List[Dict[str, Any]]:
    """ä½¿ç”¨ä¸Šæ¸¸ FileStorage ç›´æ¥è¯»å– pkl æ—¥å¿—ï¼ˆä¼˜å…ˆçº§1ï¼‰"""
    if not HAS_RDAGENT or not log_dir or not log_dir.exists():
        return []
    
    traces = []
    try:
        storage = FileStorage(log_dir)
        messages = list(storage.iter_msg())
        
        # è½¬æ¢ Message å¯¹è±¡ä¸º trace æ ¼å¼
        for msg in messages:
            traces.append({
                'id': msg.timestamp.strftime('%Y%m%d_%H%M%S_%f'),
                'type': msg.tag.split('.')[-1] if msg.tag else 'Unknown',
                'stage': msg.tag or 'Unknown',
                'status': 'completed',
                'timestamp': msg.timestamp,
                'duration': 0,
                'description': str(msg.content)[:200] if msg.content else '',
                'metadata': {
                    'tag': msg.tag,
                    'pid_trace': msg.pid_trace,
                    'level': msg.level,
                },
                'result': {'content': msg.content},
                'content': msg.content
            })
        
        st.success(f"âœ… ä½¿ç”¨ä¸Šæ¸¸ FileStorage è¯»å–åˆ° {len(traces)} æ¡æ—¥å¿—")
        return traces
        
    except Exception as e:
        st.warning(f"âš ï¸ FileStorage è¯»å–å¤±è´¥ï¼Œå›é€€åˆ°æœ¬åœ°æ‰«æ: {e}")
        return []


def _load_traces_from_filestorage(log_dir: Path, tag_filter: Optional[str] = None) -> List[Dict[str, Any]]:
    """æœ¬åœ°æ‰«æ pkl æ–‡ä»¶ï¼ˆä¼˜å…ˆçº§2ï¼Œå…œåº•æ–¹æ¡ˆï¼‰"""
    traces = []
    if not log_dir or not log_dir.exists():
        return traces
    
    try:
        # æœç´¢æ‰€æœ‰pklæ–‡ä»¶
        pattern = f"**/{tag_filter.replace('.','/')}/**/*.pkl" if tag_filter else "**/*.pkl"
        pkl_files = list(log_dir.glob(pattern))
        
        for file in pkl_files:
            if file.name == "debug_llm.pkl":
                continue
            try:
                # è§£ætagï¼ˆä»ç›¸å¯¹è·¯å¾„ï¼‰
                rel_path = file.relative_to(log_dir)
                pkl_log_tag = ".".join(rel_path.as_posix().replace("/", ".").split(".")[:-3])
                pid = file.parent.name
                
                # è§£ææ—¶é—´æˆ³
                timestamp = datetime.strptime(file.stem, "%Y-%m-%d_%H-%M-%S-%f").replace(tzinfo=timezone.utc)
                
                # åŠ è½½pklå†…å®¹
                with file.open("rb") as f:
                    content = pickle.load(f)
                
                # è½¬æ¢ä¸ºtraceæ ¼å¼
                traces.append({
                    'id': file.stem,
                    'type': pkl_log_tag.split('.')[-1] if pkl_log_tag else 'Unknown',
                    'stage': pkl_log_tag,
                    'status': 'completed',
                    'timestamp': timestamp.isoformat(),
                    'duration': 0,
                    'description': str(content)[:200] if content else '',
                    'metadata': {
                        'pid_trace': pid,
                        'file': str(file),
                        'tag': pkl_log_tag
                    },
                    'result': {'content': content},
                    'content': content
                })
            except Exception as e:
                st.warning(f"è·³è¿‡æ–‡ä»¶ {file.name}: {e}")
                continue
        
        # æŒ‰æ—¶é—´æ’åº
        traces.sort(key=lambda x: x['timestamp'])
        
    except Exception as e:
        st.error(f"è¯»å–FileStorageæ—¥å¿—å¤±è´¥: {e}")
    
    return traces


def _normalize(traces: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    norm = []
    for i, t in enumerate(traces):
        try:
            ts = t.get('timestamp') or t.get('created_at') or datetime.now().isoformat()
            try:
                # tolerant parse
                ts_dt = datetime.fromisoformat(ts.replace('Z','').split('+')[0])
            except Exception:
                ts_dt = datetime.now()
            norm.append({
                'id': t.get('id', t.get('trace_id', f'trace_{i}')),
                'stage': t.get('type', t.get('stage', 'Unknown')),
                'status': t.get('status', 'completed'),
                'timestamp': ts_dt,
                'duration': float(t.get('duration', 0) or 0),
                'description': t.get('description', t.get('task', '')),
                'metadata': t.get('metadata', t.get('details', {})),
                'result': t.get('result', {})
            })
        except Exception:
            pass
    # sort by time
    norm.sort(key=lambda x: x['timestamp'])
    return norm


def _render_token_statistics(items: List[Dict[str, Any]]):
    """æ¸²æŸ“ Token æˆæœ¬ç»Ÿè®¡ä¿¡æ¯"""
    st.subheader('ğŸ’° Token æˆæœ¬ç»Ÿè®¡')
    
    total_prompt_tokens = 0
    total_completion_tokens = 0
    total_tokens = 0
    token_details_by_stage = {}
    
    # éå†æ‰€æœ‰ items æå– token ä¿¡æ¯
    for item in items:
        metadata = item.get('metadata', {})
        stage = item.get('stage', 'Unknown')
        
        # æå– token æ•°æ®ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
        tokens_data = None
        
        # æ ¼å¼1: metadata['tokens'] æˆ– metadata['token_usage']
        if 'tokens' in metadata:
            tokens_data = metadata['tokens']
        elif 'token_usage' in metadata:
            tokens_data = metadata['token_usage']
        
        # æ ¼å¼2: ç›´æ¥åœ¨ metadata ä¸­
        if not tokens_data and any(k in metadata for k in ['prompt_tokens', 'completion_tokens', 'total_tokens']):
            tokens_data = metadata
        
        if tokens_data:
            prompt = tokens_data.get('prompt_tokens', 0)
            completion = tokens_data.get('completion_tokens', 0)
            total = tokens_data.get('total_tokens', prompt + completion)
            
            total_prompt_tokens += prompt
            total_completion_tokens += completion
            total_tokens += total
            
            # æŒ‰é˜¶æ®µç»Ÿè®¡
            if stage not in token_details_by_stage:
                token_details_by_stage[stage] = {'prompt': 0, 'completion': 0, 'total': 0, 'count': 0}
            token_details_by_stage[stage]['prompt'] += prompt
            token_details_by_stage[stage]['completion'] += completion
            token_details_by_stage[stage]['total'] += total
            token_details_by_stage[stage]['count'] += 1
    
    # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
    if total_tokens > 0:
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric('ğŸ’¬ Prompt Tokens', f'{total_prompt_tokens:,}')
        with col2:
            st.metric('âœ… Completion Tokens', f'{total_completion_tokens:,}')
        with col3:
            st.metric('ğŸ“Š Total Tokens', f'{total_tokens:,}')
        with col4:
            # ä¼°ç®—æˆæœ¬ï¼ˆä»¥ GPT-4 ä»·æ ¼ä¸ºä¾‹ï¼š$0.03/1K prompt, $0.06/1K completionï¼‰
            estimated_cost = (total_prompt_tokens / 1000 * 0.03) + (total_completion_tokens / 1000 * 0.06)
            st.metric('ğŸ’µ ä¼°ç®—æˆæœ¬ (USD)', f'${estimated_cost:.4f}')
        
        st.caption('ğŸ’¡ æˆæœ¬ä¼°ç®—åŸºäº GPT-4 å®šä»·ï¼Œå®é™…æˆæœ¬å–å†³äºä½¿ç”¨çš„å…·ä½“æ¨¡å‹')
        
        # æŒ‰é˜¶æ®µåˆ†è§£
        if token_details_by_stage:
            with st.expander('ğŸ“ˆ æŒ‰é˜¶æ®µåˆ†è§£'):
                import pandas as pd
                df_data = []
                for stage, data in token_details_by_stage.items():
                    stage_cost = (data['prompt'] / 1000 * 0.03) + (data['completion'] / 1000 * 0.06)
                    df_data.append({
                        'é˜¶æ®µ': stage,
                        'è°ƒç”¨æ¬¡æ•°': data['count'],
                        'Prompt Tokens': f"{data['prompt']:,}",
                        'Completion Tokens': f"{data['completion']:,}",
                        'Total Tokens': f"{data['total']:,}",
                        'ä¼°ç®—æˆæœ¬ (USD)': f"${stage_cost:.4f}"
                    })
                df = pd.DataFrame(df_data)
                st.dataframe(df, use_container_width=True, hide_index=True)
    else:
        st.info('ğŸš¨ å½“å‰æ—¥å¿—ä¸­æœªæ‰¾åˆ° Token ç»Ÿè®¡ä¿¡æ¯')
        st.caption('â„¹ï¸ Token ç»Ÿè®¡ä»…åœ¨ä½¿ç”¨ FileStorage è®°å½•çš„æ—¥å¿—ä¸­å¯ç”¨')


def _render_timeline(items: List[Dict[str, Any]]):
    if not items:
        st.info('æœªæ‰¾åˆ°traceè®°å½•')
        return
    stages = list({it['stage'] for it in items})
    fig = go.Figure()
    color_map = {
        'Research': 'blue',
        'Development': 'orange',
        'Experiment': 'green',
        'Evaluation': 'purple',
        'Unknown': 'gray'
    }
    for s in stages:
        xs = [it['timestamp'] for it in items if it['stage'] == s]
        ys = [s] * len(xs)
        texts = [it['description'] or it['status'] for it in items if it['stage'] == s]
        fig.add_trace(go.Scatter(x=xs, y=ys, mode='markers', name=s, marker=dict(size=10, color=color_map.get(s, 'gray')), text=texts))
    fig.update_layout(title='RD-Agent æ‰§è¡Œæ—¶é—´è½´', xaxis_title='æ—¶é—´', yaxis_title='é˜¶æ®µ', height=400)
    st.plotly_chart(fig, use_container_width=True)


def _render_detail(items: List[Dict[str, Any]]):
    ids = [it['id'] for it in items]
    sel = st.selectbox('é€‰æ‹©æ­¥éª¤ID', ids)
    it = next((x for x in items if x['id'] == sel), None)
    if not it:
        return
    st.subheader('æ­¥éª¤è¯¦æƒ…')
    c1, c2, c3 = st.columns(3)
    with c1:
        st.metric('é˜¶æ®µ', it['stage'])
    with c2:
        st.metric('çŠ¶æ€', it['status'])
    with c3:
        st.metric('è€—æ—¶(s)', f"{it['duration']}")
    st.text_area('æè¿°', it.get('description',''), height=80)
    with st.expander('å…ƒæ•°æ®'):
        st.json(it.get('metadata', {}))
    with st.expander('ç»“æœ'):
        st.json(it.get('result', {}))


def render():
    st.title('ğŸ§¾ RD-Agent åŸç”Ÿæ—¥å¿—å¯è§†åŒ–')
    st.caption('æ”¯æŒ trace.json å’Œ FileStorage pkl æ—¥å¿—')

    # é€‰æ‹©æ—¥å¿—æºç±»å‹
    log_source = st.radio("æ—¥å¿—æºç±»å‹", ['trace.json', 'FileStorage (ç›®å½•)'], horizontal=True)
    
    traces = []
    
    if log_source == 'trace.json':
        # trace.jsonæ¨¡å¼
        default = ''
        for p in POSSIBLE_TRACE_FILES:
            if p.exists():
                default = str(p)
                break
        user_path = st.text_input('trace.json è·¯å¾„(å¯é€‰)', value=default)
        trace_path = Path(user_path) if user_path else None
        traces = _load_traces_from_json(trace_path)
    else:
        # FileStorageç›®å½•æ¨¡å¼ - ä¸‰çº§ä¼˜å…ˆçº§
        default_dir = ''
        for p in POSSIBLE_LOG_DIRS:
            if p.exists():
                default_dir = str(p)
                break
        log_dir_path = st.text_input('FileStorage æ—¥å¿—ç›®å½•', value=default_dir, help='è¾“å…¥åŒ…å« pkl æ–‡ä»¶çš„ç›®å½•è·¯å¾„')
        tag_filter = st.text_input('æ ‡ç­¾è¿‡æ»¤(å¯é€‰)', value='', help='ä¾‹å¦‚: loop.step')
        
        if log_dir_path:
            log_dir = Path(log_dir_path)
            
            # ä¼˜å…ˆçº§1: ä¸Šæ¸¸ FileStorage
            traces = _load_traces_from_filestorage_upstream(log_dir)
            
            # ä¼˜å…ˆçº§2: æœ¬åœ° pkl æ‰«æ
            if not traces:
                traces = _load_traces_from_filestorage(log_dir, tag_filter if tag_filter.strip() else None)
            
            # ä¼˜å…ˆçº§3: trace.json å…œåº•
            if not traces:
                trace_json = log_dir / 'trace.json'
                if trace_json.exists():
                    st.info("ğŸ”„ å›é€€åˆ° trace.json æ¨¡å¼")
                    traces = _load_traces_from_json(trace_json)

    # åŠ è½½å¹¶è§„èŒƒåŒ–æ•°æ®
    items = _normalize(traces)
    
    # Token æˆæœ¬ç»Ÿè®¡ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
    if items:
        _render_token_statistics(items)
        st.divider()
    
    # è¿‡æ»¤å™¨
    col1, col2 = st.columns(2)
    with col1:
        stage_filter = st.multiselect('é˜¶æ®µè¿‡æ»¤', ['Research','Development','Experiment','Evaluation'])
    with col2:
        status_filter = st.multiselect('çŠ¶æ€è¿‡æ»¤', ['success','failed','running','completed'])

    # åº”ç”¨è¿‡æ»¤
    if stage_filter:
        items = [x for x in items if x['stage'] in stage_filter]
    if status_filter:
        items = [x for x in items if (x['status'] or '').lower() in {s.lower() for s in status_filter}]
    
    st.info(f"å…±æ‰¾åˆ° {len(items)} æ¡æ—¥å¿—è®°å½•")

    _render_timeline(items)
    st.divider()
    _render_detail(items)


if __name__ == '__main__':
    render()
