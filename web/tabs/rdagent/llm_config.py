"""
LLMé…ç½®ç®¡ç†ç•Œé¢

åŠŸèƒ½:
1. æ¨¡å‹æä¾›å•†é€‰æ‹© (OpenAI/Claude/æœ¬åœ°æ¨¡å‹/å…¶ä»–)
2. API Keyå®‰å…¨ç®¡ç†
3. æ¨¡å‹å‚æ•°é…ç½®
4. é…ç½®ä¿å­˜å’ŒåŠ è½½
5. è¿æ¥æµ‹è¯•
6. ä½¿ç”¨ç»Ÿè®¡
"""

import streamlit as st
import json
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import os


class LLMConfigManager:
    """LLMé…ç½®ç®¡ç†å™¨"""
    
    CONFIG_FILE = Path("config/llm_config.json")
    
    def __init__(self):
        self.init_session_state()
        self.ensure_config_dir()
    
    def init_session_state(self):
        """åˆå§‹åŒ–sessionçŠ¶æ€"""
        if 'llm_config' not in st.session_state:
            st.session_state.llm_config = self.load_config()
        if 'llm_test_result' not in st.session_state:
            st.session_state.llm_test_result = None
    
    def ensure_config_dir(self):
        """ç¡®ä¿é…ç½®ç›®å½•å­˜åœ¨"""
        self.CONFIG_FILE.parent.mkdir(parents=True, exist_ok=True)
    
    def load_config(self) -> Dict:
        """åŠ è½½é…ç½®"""
        if self.CONFIG_FILE.exists():
            try:
                with open(self.CONFIG_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                st.error(f"é…ç½®åŠ è½½å¤±è´¥: {e}")
        
        # é»˜è®¤é…ç½®
        return {
            'provider': 'OpenAI',
            'openai': {
                'api_key': '',
                'base_url': 'https://api.openai.com/v1',
                'model': 'gpt-4',
                'temperature': 0.7,
                'max_tokens': 4096,
                'top_p': 0.9
            },
            'claude': {
                'api_key': '',
                'model': 'claude-3-5-sonnet-20241022',
                'temperature': 0.7,
                'max_tokens': 8192
            },
            'local': {
                'base_url': 'http://localhost:11434',
                'model': 'llama2',
                'temperature': 0.7,
                'max_tokens': 2048
            },
            'azure': {
                'api_key': '',
                'endpoint': '',
                'deployment_name': '',
                'api_version': '2024-02-15-preview',
                'temperature': 0.7,
                'max_tokens': 4096
            },
            'usage_stats': {
                'total_requests': 0,
                'total_tokens': 0,
                'last_used': None
            }
        }
    
    def save_config(self, config: Dict):
        """ä¿å­˜é…ç½®"""
        try:
            with open(self.CONFIG_FILE, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            return True
        except Exception as e:
            st.error(f"é…ç½®ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def mask_api_key(self, api_key: str) -> str:
        """é®ç›–API Key"""
        if not api_key or len(api_key) < 8:
            return api_key
        return f"{api_key[:4]}{'*' * (len(api_key) - 8)}{api_key[-4:]}"
    
    def test_connection(self, provider: str, config: Dict) -> Dict:
        """æµ‹è¯•è¿æ¥"""
        try:
            if provider == 'OpenAI':
                return self._test_openai(config)
            elif provider == 'Claude':
                return self._test_claude(config)
            elif provider == 'Local':
                return self._test_local(config)
            elif provider == 'Azure':
                return self._test_azure(config)
            else:
                return {'success': False, 'message': 'ä¸æ”¯æŒçš„æä¾›å•†'}
        except Exception as e:
            return {'success': False, 'message': f'è¿æ¥å¤±è´¥: {str(e)}'}
    
    def _test_openai(self, config: Dict) -> Dict:
        """æµ‹è¯•OpenAIè¿æ¥"""
        try:
            import openai
            client = openai.OpenAI(
                api_key=config.get('api_key'),
                base_url=config.get('base_url')
            )
            
            # ç®€å•çš„æµ‹è¯•è¯·æ±‚
            response = client.chat.completions.create(
                model=config.get('model', 'gpt-3.5-turbo'),
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=10
            )
            
            return {
                'success': True,
                'message': 'è¿æ¥æˆåŠŸ!',
                'model': response.model,
                'tokens_used': response.usage.total_tokens
            }
        except Exception as e:
            return {'success': False, 'message': f'OpenAIè¿æ¥å¤±è´¥: {str(e)}'}
    
    def _test_claude(self, config: Dict) -> Dict:
        """æµ‹è¯•Claudeè¿æ¥"""
        try:
            import anthropic
            client = anthropic.Anthropic(api_key=config.get('api_key'))
            
            message = client.messages.create(
                model=config.get('model', 'claude-3-5-sonnet-20241022'),
                max_tokens=10,
                messages=[{"role": "user", "content": "Hello"}]
            )
            
            return {
                'success': True,
                'message': 'è¿æ¥æˆåŠŸ!',
                'model': message.model,
                'tokens_used': message.usage.input_tokens + message.usage.output_tokens
            }
        except Exception as e:
            return {'success': False, 'message': f'Claudeè¿æ¥å¤±è´¥: {str(e)}'}
    
    def _test_local(self, config: Dict) -> Dict:
        """æµ‹è¯•æœ¬åœ°æ¨¡å‹è¿æ¥"""
        try:
            import requests
            base_url = config.get('base_url', 'http://localhost:11434')
            
            # æµ‹è¯•Ollama API
            response = requests.get(f"{base_url}/api/tags", timeout=5)
            
            if response.status_code == 200:
                models = response.json().get('models', [])
                return {
                    'success': True,
                    'message': f'è¿æ¥æˆåŠŸ! å‘ç° {len(models)} ä¸ªæ¨¡å‹',
                    'available_models': [m.get('name') for m in models]
                }
            else:
                return {'success': False, 'message': f'è¿æ¥å¤±è´¥: HTTP {response.status_code}'}
        except Exception as e:
            return {'success': False, 'message': f'æœ¬åœ°æ¨¡å‹è¿æ¥å¤±è´¥: {str(e)}'}
    
    def _test_azure(self, config: Dict) -> Dict:
        """æµ‹è¯•Azure OpenAIè¿æ¥"""
        try:
            import openai
            client = openai.AzureOpenAI(
                api_key=config.get('api_key'),
                azure_endpoint=config.get('endpoint'),
                api_version=config.get('api_version')
            )
            
            response = client.chat.completions.create(
                model=config.get('deployment_name'),
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=10
            )
            
            return {
                'success': True,
                'message': 'è¿æ¥æˆåŠŸ!',
                'deployment': config.get('deployment_name'),
                'tokens_used': response.usage.total_tokens
            }
        except Exception as e:
            return {'success': False, 'message': f'Azureè¿æ¥å¤±è´¥: {str(e)}'}
    
    def render_provider_selection(self):
        """æ¸²æŸ“æä¾›å•†é€‰æ‹©"""
        st.subheader("ğŸ¤– æ¨¡å‹æä¾›å•†")
        
        providers = ['OpenAI', 'Claude', 'Azure', 'Local', 'å…¶ä»–']
        
        current_provider = st.session_state.llm_config.get('provider', 'OpenAI')
        
        selected = st.selectbox(
            "é€‰æ‹©LLMæä¾›å•†",
            providers,
            index=providers.index(current_provider) if current_provider in providers else 0,
            help="é€‰æ‹©è¦ä½¿ç”¨çš„å¤§è¯­è¨€æ¨¡å‹æä¾›å•†"
        )
        
        st.session_state.llm_config['provider'] = selected
        
        # æä¾›å•†æè¿°
        descriptions = {
            'OpenAI': 'ğŸ”µ OpenAIå®˜æ–¹API (GPT-3.5/GPT-4)',
            'Claude': 'ğŸŸ£ Anthropic Claude (Claude 3/3.5)',
            'Azure': 'ğŸ”· Azure OpenAI Service',
            'Local': 'ğŸŸ¢ æœ¬åœ°éƒ¨ç½²æ¨¡å‹ (Ollama/LM Studio)',
            'å…¶ä»–': 'âšª å…¶ä»–å…¼å®¹OpenAI APIçš„æœåŠ¡'
        }
        
        st.info(descriptions.get(selected, ''))
    
    def render_openai_config(self):
        """æ¸²æŸ“OpenAIé…ç½®"""
        st.subheader("ğŸ”µ OpenAI é…ç½®")
        
        config = st.session_state.llm_config.get('openai', {})
        
        col1, col2 = st.columns([3, 1])
        
        with col1:
            api_key = st.text_input(
                "API Key",
                value=config.get('api_key', ''),
                type="password",
                help="ä» https://platform.openai.com/api-keys è·å–"
            )
            config['api_key'] = api_key
        
        with col2:
            if api_key:
                st.text("å·²è®¾ç½®")
                st.caption(self.mask_api_key(api_key))
        
        base_url = st.text_input(
            "Base URL",
            value=config.get('base_url', 'https://api.openai.com/v1'),
            help="APIåŸºç¡€URL,ä½¿ç”¨ä»£ç†æ—¶å¯ä¿®æ”¹"
        )
        config['base_url'] = base_url
        
        col1, col2 = st.columns(2)
        
        with col1:
            model = st.selectbox(
                "æ¨¡å‹",
                ['gpt-4', 'gpt-4-turbo', 'gpt-3.5-turbo', 'gpt-4o', 'gpt-4o-mini'],
                index=['gpt-4', 'gpt-4-turbo', 'gpt-3.5-turbo', 'gpt-4o', 'gpt-4o-mini'].index(config.get('model', 'gpt-4'))
            )
            config['model'] = model
        
        with col2:
            temperature = st.slider(
                "Temperature",
                0.0, 2.0,
                float(config.get('temperature', 0.7)),
                0.1,
                help="æ§åˆ¶è¾“å‡ºéšæœºæ€§,è¶Šé«˜è¶Šéšæœº"
            )
            config['temperature'] = temperature
        
        col1, col2 = st.columns(2)
        
        with col1:
            max_tokens = st.number_input(
                "Max Tokens",
                100, 128000,
                int(config.get('max_tokens', 4096)),
                100,
                help="æœ€å¤§ç”Ÿæˆtokenæ•°"
            )
            config['max_tokens'] = max_tokens
        
        with col2:
            top_p = st.slider(
                "Top P",
                0.0, 1.0,
                float(config.get('top_p', 0.9)),
                0.05,
                help="æ ¸é‡‡æ ·å‚æ•°"
            )
            config['top_p'] = top_p
        
        st.session_state.llm_config['openai'] = config
    
    def render_claude_config(self):
        """æ¸²æŸ“Claudeé…ç½®"""
        st.subheader("ğŸŸ£ Claude é…ç½®")
        
        config = st.session_state.llm_config.get('claude', {})
        
        col1, col2 = st.columns([3, 1])
        
        with col1:
            api_key = st.text_input(
                "API Key",
                value=config.get('api_key', ''),
                type="password",
                help="ä» https://console.anthropic.com è·å–"
            )
            config['api_key'] = api_key
        
        with col2:
            if api_key:
                st.text("å·²è®¾ç½®")
                st.caption(self.mask_api_key(api_key))
        
        col1, col2 = st.columns(2)
        
        with col1:
            model = st.selectbox(
                "æ¨¡å‹",
                ['claude-3-5-sonnet-20241022', 'claude-3-opus-20240229', 'claude-3-sonnet-20240229', 'claude-3-haiku-20240307'],
                index=0
            )
            config['model'] = model
        
        with col2:
            temperature = st.slider(
                "Temperature",
                0.0, 1.0,
                float(config.get('temperature', 0.7)),
                0.1
            )
            config['temperature'] = temperature
        
        max_tokens = st.number_input(
            "Max Tokens",
            100, 200000,
            int(config.get('max_tokens', 8192)),
            100
        )
        config['max_tokens'] = max_tokens
        
        st.session_state.llm_config['claude'] = config
    
    def render_azure_config(self):
        """æ¸²æŸ“Azureé…ç½®"""
        st.subheader("ğŸ”· Azure OpenAI é…ç½®")
        
        config = st.session_state.llm_config.get('azure', {})
        
        api_key = st.text_input(
            "API Key",
            value=config.get('api_key', ''),
            type="password"
        )
        config['api_key'] = api_key
        
        endpoint = st.text_input(
            "Endpoint",
            value=config.get('endpoint', ''),
            placeholder="https://your-resource.openai.azure.com/",
            help="Azureèµ„æºçš„ç«¯ç‚¹URL"
        )
        config['endpoint'] = endpoint
        
        col1, col2 = st.columns(2)
        
        with col1:
            deployment_name = st.text_input(
                "Deployment Name",
                value=config.get('deployment_name', ''),
                help="éƒ¨ç½²çš„æ¨¡å‹åç§°"
            )
            config['deployment_name'] = deployment_name
        
        with col2:
            api_version = st.selectbox(
                "API Version",
                ['2024-02-15-preview', '2023-12-01-preview', '2023-05-15'],
                index=0
            )
            config['api_version'] = api_version
        
        col1, col2 = st.columns(2)
        
        with col1:
            temperature = st.slider(
                "Temperature",
                0.0, 2.0,
                float(config.get('temperature', 0.7)),
                0.1
            )
            config['temperature'] = temperature
        
        with col2:
            max_tokens = st.number_input(
                "Max Tokens",
                100, 128000,
                int(config.get('max_tokens', 4096)),
                100
            )
            config['max_tokens'] = max_tokens
        
        st.session_state.llm_config['azure'] = config
    
    def render_local_config(self):
        """æ¸²æŸ“æœ¬åœ°æ¨¡å‹é…ç½®"""
        st.subheader("ğŸŸ¢ æœ¬åœ°æ¨¡å‹é…ç½®")
        
        config = st.session_state.llm_config.get('local', {})
        
        st.info("ğŸ’¡ æ”¯æŒ Ollama, LM Studio ç­‰æœ¬åœ°æ¨¡å‹æœåŠ¡")
        
        base_url = st.text_input(
            "Base URL",
            value=config.get('base_url', 'http://localhost:11434'),
            help="æœ¬åœ°æ¨¡å‹æœåŠ¡åœ°å€"
        )
        config['base_url'] = base_url
        
        col1, col2 = st.columns(2)
        
        with col1:
            model = st.text_input(
                "æ¨¡å‹åç§°",
                value=config.get('model', 'llama2'),
                help="ä¾‹å¦‚: llama2, mistral, codellama"
            )
            config['model'] = model
        
        with col2:
            temperature = st.slider(
                "Temperature",
                0.0, 2.0,
                float(config.get('temperature', 0.7)),
                0.1
            )
            config['temperature'] = temperature
        
        max_tokens = st.number_input(
            "Max Tokens",
            100, 32000,
            int(config.get('max_tokens', 2048)),
            100
        )
        config['max_tokens'] = max_tokens
        
        # Ollamaå®‰è£…æŒ‡å—
        with st.expander("ğŸ“– Ollama å®‰è£…æŒ‡å—"):
            st.markdown("""
            ### å®‰è£… Ollama
            
            **Windows:**
            ```bash
            # ä¸‹è½½å®‰è£…åŒ…
            https://ollama.ai/download/windows
            ```
            
            **Linux/Mac:**
            ```bash
            curl -fsSL https://ollama.ai/install.sh | sh
            ```
            
            ### ä¸‹è½½æ¨¡å‹
            ```bash
            # Llama 2
            ollama pull llama2
            
            # Mistral
            ollama pull mistral
            
            # Code Llama
            ollama pull codellama
            ```
            
            ### å¯åŠ¨æœåŠ¡
            ```bash
            ollama serve
            ```
            """)
        
        st.session_state.llm_config['local'] = config
    
    def render_test_connection(self):
        """æ¸²æŸ“è¿æ¥æµ‹è¯•"""
        st.subheader("ğŸ”Œ è¿æ¥æµ‹è¯•")
        
        provider = st.session_state.llm_config.get('provider')
        
        if st.button("ğŸ§ª æµ‹è¯•è¿æ¥", type="primary"):
            with st.spinner("æµ‹è¯•ä¸­..."):
                config_key = provider.lower()
                config = st.session_state.llm_config.get(config_key, {})
                
                result = self.test_connection(provider, config)
                st.session_state.llm_test_result = result
        
        if st.session_state.llm_test_result:
            result = st.session_state.llm_test_result
            
            if result['success']:
                st.success(f"âœ… {result['message']}")
                
                if 'model' in result:
                    st.info(f"ğŸ“‹ ä½¿ç”¨æ¨¡å‹: {result['model']}")
                if 'tokens_used' in result:
                    st.info(f"ğŸ« æµ‹è¯•æ¶ˆè€—token: {result['tokens_used']}")
                if 'available_models' in result:
                    st.info(f"ğŸ“¦ å¯ç”¨æ¨¡å‹: {', '.join(result['available_models'][:5])}")
            else:
                st.error(f"âŒ {result['message']}")
    
    def render_usage_stats(self):
        """æ¸²æŸ“ä½¿ç”¨ç»Ÿè®¡"""
        st.subheader("ğŸ“Š ä½¿ç”¨ç»Ÿè®¡")
        
        stats = st.session_state.llm_config.get('usage_stats', {})
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("æ€»è¯·æ±‚æ•°", f"{stats.get('total_requests', 0):,}")
        
        with col2:
            st.metric("æ€»Tokenæ•°", f"{stats.get('total_tokens', 0):,}")
        
        with col3:
            last_used = stats.get('last_used')
            if last_used:
                st.metric("æœ€åä½¿ç”¨", last_used)
            else:
                st.metric("æœ€åä½¿ç”¨", "æœªä½¿ç”¨")
        
        st.info("ğŸ’¡ ä½¿ç”¨ç»Ÿè®¡å°†åœ¨å®é™…è°ƒç”¨LLMæ—¶è‡ªåŠ¨æ›´æ–°")
    
    def render_save_load(self):
        """æ¸²æŸ“ä¿å­˜/åŠ è½½"""
        st.subheader("ğŸ’¾ é…ç½®ç®¡ç†")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸ’¾ ä¿å­˜é…ç½®", type="primary", use_container_width=True):
                if self.save_config(st.session_state.llm_config):
                    st.success("âœ… é…ç½®å·²ä¿å­˜!")
                    st.balloons()
        
        with col2:
            if st.button("ğŸ”„ é‡æ–°åŠ è½½", use_container_width=True):
                st.session_state.llm_config = self.load_config()
                st.success("âœ… é…ç½®å·²é‡æ–°åŠ è½½!")
                st.rerun()
        
        with col3:
            if st.button("ğŸ—‘ï¸ é‡ç½®ä¸ºé»˜è®¤", use_container_width=True):
                if st.button("ç¡®è®¤é‡ç½®?", key="confirm_reset"):
                    st.session_state.llm_config = self.load_config()
                    st.success("âœ… å·²é‡ç½®ä¸ºé»˜è®¤é…ç½®!")
                    st.rerun()
        
        st.caption(f"ğŸ“ é…ç½®æ–‡ä»¶è·¯å¾„: `{self.CONFIG_FILE.absolute()}`")
    
    def render(self):
        """ä¸»æ¸²æŸ“å‡½æ•°"""
        st.title("âš™ï¸ LLMé…ç½®ç®¡ç†")
        
        st.markdown("""
        é…ç½®å¤§è¯­è¨€æ¨¡å‹è¿æ¥å‚æ•°,æ”¯æŒå¤šç§æä¾›å•†ã€‚
        æ‰€æœ‰é…ç½®å°†å®‰å…¨ä¿å­˜åœ¨æœ¬åœ°ã€‚
        """)
        
        st.divider()
        
        # æä¾›å•†é€‰æ‹©
        self.render_provider_selection()
        
        st.divider()
        
        # æ ¹æ®é€‰æ‹©çš„æä¾›å•†æ˜¾ç¤ºé…ç½®
        provider = st.session_state.llm_config.get('provider')
        
        if provider == 'OpenAI':
            self.render_openai_config()
        elif provider == 'Claude':
            self.render_claude_config()
        elif provider == 'Azure':
            self.render_azure_config()
        elif provider == 'Local':
            self.render_local_config()
        else:
            st.info("è¯¥æä¾›å•†é…ç½®ç•Œé¢å¼€å‘ä¸­...")
        
        st.divider()
        
        # è¿æ¥æµ‹è¯•
        self.render_test_connection()
        
        st.divider()
        
        # ä½¿ç”¨ç»Ÿè®¡
        self.render_usage_stats()
        
        st.divider()
        
        # ä¿å­˜/åŠ è½½
        self.render_save_load()


def main():
    """ä¸»å‡½æ•°"""
    manager = LLMConfigManager()
    manager.render()


if __name__ == "__main__":
    main()
