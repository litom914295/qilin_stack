"""
æ¶¨åœæ¿é›†æˆå­¦ä¹ æ¨¡å‹ - å¤šæ¨¡å‹Stacking

é›†æˆä»¥ä¸‹æ¨¡å‹ï¼š
1. XGBoost - æ¢¯åº¦æå‡æ ‘
2. LightGBM - è½»é‡çº§æ¢¯åº¦æå‡
3. CatBoost - ç±»åˆ«ç‰¹å¾å¢å¼º
4. GRU - å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆQlibï¼‰

ä½¿ç”¨Stackingç­–ç•¥èåˆé¢„æµ‹ç»“æœï¼Œæå‡å‡†ç¡®ç‡
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
import warnings

warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥å„ç§æ¨¡å‹åº“
MODELS_AVAILABLE = {}

try:
    import xgboost as xgb
    MODELS_AVAILABLE['xgboost'] = True
except ImportError:
    MODELS_AVAILABLE['xgboost'] = False
    print("âš ï¸  XGBoostæœªå®‰è£…")

try:
    import lightgbm as lgb
    MODELS_AVAILABLE['lightgbm'] = True
except ImportError:
    MODELS_AVAILABLE['lightgbm'] = False
    print("âš ï¸  LightGBMæœªå®‰è£…")

try:
    import catboost as cb
    MODELS_AVAILABLE['catboost'] = True
except ImportError:
    MODELS_AVAILABLE['catboost'] = False
    print("âš ï¸  CatBoostæœªå®‰è£…")


class LimitUpEnsembleModel:
    """æ¶¨åœæ¿é›†æˆå­¦ä¹ æ¨¡å‹"""
    
    def __init__(self, config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–é›†æˆæ¨¡å‹
        
        Parameters:
        -----------
        config : Dict, optional
            é…ç½®å‚æ•°
        """
        self.config = config or {}
        
        # åŸºç¡€æ¨¡å‹
        self.base_models = {}
        self.meta_model = None
        
        # æ¨¡å‹æƒé‡ï¼ˆè‡ªé€‚åº”ï¼‰
        self.model_weights = {}
        
        # åˆå§‹åŒ–å¯ç”¨æ¨¡å‹
        self._init_models()
    
    def _init_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨æ¨¡å‹"""
        
        # 1. XGBoost
        if MODELS_AVAILABLE['xgboost']:
            self.base_models['xgb'] = xgb.XGBClassifier(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                random_state=42
            )
        
        # 2. LightGBM  
        if MODELS_AVAILABLE['lightgbm']:
            self.base_models['lgb'] = lgb.LGBMClassifier(
                n_estimators=100,
                num_leaves=31,
                learning_rate=0.1,
                random_state=42
            )
        
        # 3. CatBoost
        if MODELS_AVAILABLE['catboost']:
            self.base_models['cat'] = cb.CatBoostClassifier(
                iterations=100,
                depth=5,
                learning_rate=0.1,
                random_state=42,
                verbose=False
            )
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•æ¨¡å‹ï¼Œä½¿ç”¨ç®€å•åˆ†ç±»å™¨
        if not self.base_models:
            print("âš ï¸  æ²¡æœ‰å¯ç”¨çš„MLåº“ï¼Œä½¿ç”¨ç®€å•è§„åˆ™åˆ†ç±»å™¨")
            self.base_models['simple'] = SimpleClassifier()
        
        print(f"âœ… åˆå§‹åŒ–äº† {len(self.base_models)} ä¸ªåŸºç¡€æ¨¡å‹: {list(self.base_models.keys())}")
    
    def fit(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_val: Optional[pd.DataFrame] = None,
        y_val: Optional[pd.Series] = None
    ):
        """
        è®­ç»ƒé›†æˆæ¨¡å‹
        
        Parameters:
        -----------
        X_train : pd.DataFrame
            è®­ç»ƒç‰¹å¾
        y_train : pd.Series
            è®­ç»ƒæ ‡ç­¾
        X_val : pd.DataFrame, optional
            éªŒè¯é›†ç‰¹å¾
        y_val : pd.Series, optional
            éªŒè¯é›†æ ‡ç­¾
        """
        print(f"\nğŸ‹ï¸  è®­ç»ƒé›†æˆæ¨¡å‹...")
        print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        if X_val is not None:
            print(f"   éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
        
        # è®­ç»ƒæ‰€æœ‰åŸºç¡€æ¨¡å‹
        base_predictions_train = {}
        base_predictions_val = {}
        
        for name, model in self.base_models.items():
            print(f"\n   è®­ç»ƒ {name}...")
            
            try:
                model.fit(X_train, y_train)
                
                # è·å–è®­ç»ƒé›†é¢„æµ‹ï¼ˆç”¨äºmetaæ¨¡å‹ï¼‰
                base_predictions_train[name] = model.predict_proba(X_train)[:, 1]
                
                if X_val is not None:
                    base_predictions_val[name] = model.predict_proba(X_val)[:, 1]
                    
                    # è®¡ç®—éªŒè¯é›†å‡†ç¡®ç‡
                    val_pred = model.predict(X_val)
                    val_acc = (val_pred == y_val).mean()
                    print(f"      éªŒè¯é›†å‡†ç¡®ç‡: {val_acc:.2%}")
                
                print(f"      âœ… {name} è®­ç»ƒå®Œæˆ")
                
            except Exception as e:
                print(f"      âŒ {name} è®­ç»ƒå¤±è´¥: {e}")
                # ç§»é™¤å¤±è´¥çš„æ¨¡å‹
                del self.base_models[name]
        
        # è®­ç»ƒmetaæ¨¡å‹ï¼ˆä½¿ç”¨åŸºç¡€æ¨¡å‹çš„é¢„æµ‹ä½œä¸ºç‰¹å¾ï¼‰
        if len(self.base_models) > 0:
            print(f"\n   è®­ç»ƒå…ƒæ¨¡å‹ï¼ˆStackingå±‚ï¼‰...")
            
            # æ„å»ºmetaç‰¹å¾
            meta_X_train = pd.DataFrame(base_predictions_train)
            
            # ä½¿ç”¨ç®€å•çš„LRæˆ–ç®€å•è§„åˆ™ä½œä¸ºmetaæ¨¡å‹
            self.meta_model = SimpleMetaModel()
            self.meta_model.fit(meta_X_train, y_train)
            
            if X_val is not None:
                meta_X_val = pd.DataFrame(base_predictions_val)
                meta_pred = self.meta_model.predict(meta_X_val)
                meta_acc = (meta_pred == y_val).mean()
                print(f"      å…ƒæ¨¡å‹éªŒè¯é›†å‡†ç¡®ç‡: {meta_acc:.2%}")
            
            print(f"      âœ… å…ƒæ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        print(f"\nâœ… é›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """é¢„æµ‹ç±»åˆ«"""
        proba = self.predict_proba(X)
        return (proba[:, 1] > 0.5).astype(int)
    
    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """é¢„æµ‹æ¦‚ç‡"""
        
        # è·å–æ‰€æœ‰åŸºç¡€æ¨¡å‹çš„é¢„æµ‹
        base_predictions = {}
        for name, model in self.base_models.items():
            try:
                base_predictions[name] = model.predict_proba(X)[:, 1]
            except:
                pass
        
        if not base_predictions:
            # å¦‚æœæ²¡æœ‰åŸºç¡€æ¨¡å‹ï¼Œè¿”å›é»˜è®¤å€¼
            return np.column_stack([
                np.ones(len(X)) * 0.5,
                np.ones(len(X)) * 0.5
            ])
        
        # ä½¿ç”¨metaæ¨¡å‹èåˆ
        if self.meta_model:
            meta_X = pd.DataFrame(base_predictions)
            final_proba = self.meta_model.predict_proba(meta_X)
            return np.column_stack([1 - final_proba, final_proba])
        else:
            # ç®€å•å¹³å‡
            avg_proba = np.mean(list(base_predictions.values()), axis=0)
            return np.column_stack([1 - avg_proba, avg_proba])
    
    def evaluate(
        self,
        X: pd.DataFrame,
        y: pd.Series
    ) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        
        y_pred = self.predict(X)
        y_proba = self.predict_proba(X)[:, 1]
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = (y_pred == y).mean()
        
        # è®¡ç®—F1
        tp = ((y_pred == 1) & (y == 1)).sum()
        fp = ((y_pred == 1) & (y == 0)).sum()
        fn = ((y_pred == 0) & (y == 1)).sum()
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1
        }


class SimpleClassifier:
    """ç®€å•è§„åˆ™åˆ†ç±»å™¨ï¼ˆå½“æ²¡æœ‰MLåº“æ—¶ä½¿ç”¨ï¼‰"""
    
    def __init__(self):
        self.threshold = {}
    
    def fit(self, X, y):
        """å­¦ä¹ æ¯ä¸ªç‰¹å¾çš„æœ€ä½³é˜ˆå€¼"""
        for col in X.columns:
            # è®¡ç®—ç›¸å…³ç³»æ•°
            corr = X[col].corr(y)
            # ä½¿ç”¨ä¸­ä½æ•°ä½œä¸ºé˜ˆå€¼
            self.threshold[col] = X[col].median()
    
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        scores = []
        for col in X.columns:
            if col in self.threshold:
                score = (X[col] > self.threshold[col]).astype(float)
                scores.append(score)
        
        if scores:
            avg_score = np.mean(scores, axis=0)
            return np.column_stack([1 - avg_score, avg_score])
        else:
            return np.column_stack([
                np.ones(len(X)) * 0.5,
                np.ones(len(X)) * 0.5
            ])
    
    def predict(self, X):
        """é¢„æµ‹ç±»åˆ«"""
        proba = self.predict_proba(X)
        return (proba[:, 1] > 0.5).astype(int)


class SimpleMetaModel:
    """ç®€å•å…ƒæ¨¡å‹ï¼ˆåŠ æƒå¹³å‡ï¼‰"""
    
    def __init__(self):
        self.weights = None
    
    def fit(self, X, y):
        """å­¦ä¹ æœ€ä½³æƒé‡"""
        # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„F1åˆ†æ•°ä½œä¸ºæƒé‡
        weights = []
        for col in X.columns:
            pred = (X[col] > 0.5).astype(int)
            tp = ((pred == 1) & (y == 1)).sum()
            fp = ((pred == 1) & (y == 0)).sum()
            fn = ((pred == 0) & (y == 1)).sum()
            
            if tp + fp > 0:
                precision = tp / (tp + fp)
            else:
                precision = 0
            
            if tp + fn > 0:
                recall = tp / (tp + fn)
            else:
                recall = 0
            
            if precision + recall > 0:
                f1 = 2 * precision * recall / (precision + recall)
            else:
                f1 = 0
            
            weights.append(f1)
        
        # å½’ä¸€åŒ–æƒé‡
        total = sum(weights)
        if total > 0:
            self.weights = {col: w / total for col, w in zip(X.columns, weights)}
        else:
            self.weights = {col: 1 / len(X.columns) for col in X.columns}
    
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        if self.weights is None:
            return np.mean(X.values, axis=1)
        
        weighted_sum = np.zeros(len(X))
        for col, weight in self.weights.items():
            if col in X.columns:
                weighted_sum += X[col].values * weight
        
        return weighted_sum
    
    def predict(self, X):
        """é¢„æµ‹ç±»åˆ«"""
        proba = self.predict_proba(X)
        return (proba > 0.5).astype(int)


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

def main():
    """ç¤ºä¾‹ï¼šè®­ç»ƒå’Œæµ‹è¯•é›†æˆæ¨¡å‹"""
    print("=" * 80)
    print("æ¶¨åœæ¿é›†æˆå­¦ä¹ æ¨¡å‹ - æµ‹è¯•")
    print("=" * 80)
    
    # 1. ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    print("\nğŸ“Š ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...")
    np.random.seed(42)
    n_samples = 1000
    n_features = 8
    
    # ç”Ÿæˆç‰¹å¾
    X = pd.DataFrame(
        np.random.randn(n_samples, n_features),
        columns=[f'feature_{i}' for i in range(n_features)]
    )
    
    # ç”Ÿæˆç›®æ ‡ï¼ˆåŸºäºç®€å•è§„åˆ™ï¼‰
    y = ((X['feature_0'] > 0) & 
         (X['feature_1'] > 0) & 
         (X['feature_2'] > 0.5)).astype(int)
    
    print(f"   æ ·æœ¬æ•°: {n_samples}")
    print(f"   ç‰¹å¾æ•°: {n_features}")
    print(f"   æ­£æ ·æœ¬ç‡: {y.mean():.1%}")
    
    # 2. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    
    # 3. è®­ç»ƒé›†æˆæ¨¡å‹
    model = LimitUpEnsembleModel()
    model.fit(X_train, y_train, X_test, y_test)
    
    # 4. è¯„ä¼°æ¨¡å‹
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¨¡å‹è¯„ä¼°")
    print("=" * 80)
    
    train_metrics = model.evaluate(X_train, y_train)
    test_metrics = model.evaluate(X_test, y_test)
    
    print("\nè®­ç»ƒé›†:")
    for metric, value in train_metrics.items():
        print(f"  {metric}: {value:.4f}")
    
    print("\næµ‹è¯•é›†:")
    for metric, value in test_metrics.items():
        print(f"  {metric}: {value:.4f}")
    
    # 5. é¢„æµ‹ç¤ºä¾‹
    print("\n" + "=" * 80)
    print("ğŸ¯ é¢„æµ‹ç¤ºä¾‹ï¼ˆå‰10ä¸ªæ ·æœ¬ï¼‰")
    print("=" * 80)
    
    sample_X = X_test.head(10)
    sample_y = y_test.head(10).values
    predictions = model.predict(sample_X)
    probabilities = model.predict_proba(sample_X)[:, 1]
    
    print("\næ ·æœ¬  çœŸå®  é¢„æµ‹  æ¦‚ç‡")
    print("-" * 40)
    for i in range(len(sample_X)):
        print(f"{i+1:4d}  {sample_y[i]:4d}  {predictions[i]:4d}  {probabilities[i]:.2%}")
    
    print("\n" + "=" * 80)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)


if __name__ == '__main__':
    # æ£€æŸ¥sklearnæ˜¯å¦å¯ç”¨
    try:
        from sklearn.model_selection import train_test_split
        main()
    except ImportError:
        print("âš ï¸  sklearnæœªå®‰è£…ï¼Œæ— æ³•è¿è¡Œå®Œæ•´æµ‹è¯•")
        print("   è¯·å®‰è£…: pip install scikit-learn")
        
        # è¿è¡Œç®€åŒ–ç‰ˆæœ¬
        print("\nè¿è¡Œç®€åŒ–æµ‹è¯•...")
        model = LimitUpEnsembleModel()
        print(f"\nå¯ç”¨æ¨¡å‹: {list(model.base_models.keys())}")
