"""
æ¶¨åœæ¿é›†æˆå­¦ä¹ æ¨¡å‹ - å¤šæ¨¡å‹Stacking

é›†æˆä»¥ä¸‹æ¨¡å‹ï¼š
1. XGBoost - æ¢¯åº¦æå‡æ ‘
2. LightGBM - è½»é‡çº§æ¢¯åº¦æå‡
3. CatBoost - ç±»åˆ«ç‰¹å¾å¢å¼º
4. GRU - å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆQlibï¼‰

ä½¿ç”¨Stackingç­–ç•¥èåˆé¢„æµ‹ç»“æœï¼Œæå‡å‡†ç¡®ç‡
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
import warnings
import logging
import os
import pickle

logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥å„ç§æ¨¡å‹åº“
MODELS_AVAILABLE = {}

try:
    import xgboost as xgb
    MODELS_AVAILABLE['xgboost'] = True
except ImportError:
    MODELS_AVAILABLE['xgboost'] = False
    logger.warning("XGBoost æœªå®‰è£…ï¼Œç›¸å…³åŸºå­¦ä¹ å™¨å°†è¢«è·³è¿‡")

try:
    import lightgbm as lgb
    MODELS_AVAILABLE['lightgbm'] = True
except ImportError:
    MODELS_AVAILABLE['lightgbm'] = False
    logger.warning("LightGBM æœªå®‰è£…ï¼Œç›¸å…³åŸºå­¦ä¹ å™¨å°†è¢«è·³è¿‡")

try:
    import catboost as cb
    MODELS_AVAILABLE['catboost'] = True
except ImportError:
    MODELS_AVAILABLE['catboost'] = False
    logger.warning("CatBoost æœªå®‰è£…ï¼Œç›¸å…³åŸºå­¦ä¹ å™¨å°†è¢«è·³è¿‡")


class LimitUpEnsembleModel:
    """æ¶¨åœæ¿é›†æˆå­¦ä¹ æ¨¡å‹"""
    
    def __init__(self, config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–é›†æˆæ¨¡å‹
        
        Parameters:
        -----------
        config : Dict, optional
            é…ç½®å‚æ•°
        """
        self.config = config or {}
        
        # åŸºç¡€æ¨¡å‹
        self.base_models = {}
        self.meta_model = None
        
        # æ¨¡å‹æƒé‡ï¼ˆè‡ªé€‚åº”ï¼‰
        self.model_weights = {}
        
        # åˆå§‹åŒ–å¯ç”¨æ¨¡å‹
        self._init_models()
    
    def _init_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨æ¨¡å‹"""
        
        # 1. XGBoost
        if MODELS_AVAILABLE['xgboost']:
            self.base_models['xgb'] = xgb.XGBClassifier(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                random_state=42
            )
        
        # 2. LightGBM  
        if MODELS_AVAILABLE['lightgbm']:
            self.base_models['lgb'] = lgb.LGBMClassifier(
                n_estimators=100,
                num_leaves=31,
                learning_rate=0.1,
                random_state=42
            )
        
        # 3. CatBoost
        if MODELS_AVAILABLE['catboost']:
            self.base_models['cat'] = cb.CatBoostClassifier(
                iterations=100,
                depth=5,
                learning_rate=0.1,
                random_state=42,
                verbose=False
            )
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•æ¨¡å‹ï¼Œä½¿ç”¨ç®€å•åˆ†ç±»å™¨
        if not self.base_models:
            logger.warning("æ²¡æœ‰å¯ç”¨çš„ ML åº“ï¼Œä½¿ç”¨ç®€å•è§„åˆ™åˆ†ç±»å™¨(SimpleClassifier)")
            self.base_models['simple'] = SimpleClassifier()
        
        logger.info(f"åˆå§‹åŒ– {len(self.base_models)} ä¸ªåŸºç¡€æ¨¡å‹: {list(self.base_models.keys())}")
    
    def fit(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_val: Optional[pd.DataFrame] = None,
        y_val: Optional[pd.Series] = None
    ):
        """
        è®­ç»ƒé›†æˆæ¨¡å‹
        
        Parameters:
        -----------
        X_train : pd.DataFrame
            è®­ç»ƒç‰¹å¾
        y_train : pd.Series
            è®­ç»ƒæ ‡ç­¾
        X_val : pd.DataFrame, optional
            éªŒè¯é›†ç‰¹å¾
        y_val : pd.Series, optional
            éªŒè¯é›†æ ‡ç­¾
        """
        logger.info("è®­ç»ƒé›†æˆæ¨¡å‹...")
        logger.info(f"è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        if X_val is not None:
            logger.info(f"éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
        
        # è®­ç»ƒæ‰€æœ‰åŸºç¡€æ¨¡å‹
        base_predictions_train = {}
        base_predictions_val = {}
        
        for name, model in self.base_models.items():
            logger.info(f"å¼€å§‹è®­ç»ƒåŸºæ¨¡å‹ {name} ...")
            try:
                model.fit(X_train, y_train)
                base_predictions_train[name] = model.predict_proba(X_train)[:, 1]
                if X_val is not None:
                    base_predictions_val[name] = model.predict_proba(X_val)[:, 1]
                    val_pred = model.predict(X_val)
                    val_acc = (val_pred == y_val).mean()
                    logger.info(f"{name} éªŒè¯é›†å‡†ç¡®ç‡: {val_acc:.2%}")
                logger.info(f"{name} è®­ç»ƒå®Œæˆ")
            except Exception as e:
                logger.exception(f"åŸºæ¨¡å‹ {name} è®­ç»ƒå¤±è´¥: {e}")
                del self.base_models[name]
        
        # è®­ç»ƒmetaæ¨¡å‹ï¼ˆä½¿ç”¨åŸºç¡€æ¨¡å‹çš„é¢„æµ‹ä½œä¸ºç‰¹å¾ï¼‰
        if len(self.base_models) > 0:
            logger.info("è®­ç»ƒå…ƒæ¨¡å‹ï¼ˆStacking å±‚ï¼‰...")
            meta_X_train = pd.DataFrame(base_predictions_train)
            self.meta_model = SimpleMetaModel()
            self.meta_model.fit(meta_X_train, y_train)
            if X_val is not None:
                meta_X_val = pd.DataFrame(base_predictions_val)
                meta_pred = self.meta_model.predict(meta_X_val)
                meta_acc = (meta_pred == y_val).mean()
                logger.info(f"å…ƒæ¨¡å‹éªŒè¯é›†å‡†ç¡®ç‡: {meta_acc:.2%}")
            logger.info("å…ƒæ¨¡å‹è®­ç»ƒå®Œæˆ")
        logger.info("é›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """é¢„æµ‹ç±»åˆ«"""
        proba = self.predict_proba(X)
        return (proba[:, 1] > 0.5).astype(int)
    
    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """é¢„æµ‹æ¦‚ç‡"""
        
        # è·å–æ‰€æœ‰åŸºç¡€æ¨¡å‹çš„é¢„æµ‹
        base_predictions = {}
        for name, model in self.base_models.items():
            try:
                base_predictions[name] = model.predict_proba(X)[:, 1]
            except:
                pass
        
        if not base_predictions:
            # å¦‚æœæ²¡æœ‰åŸºç¡€æ¨¡å‹ï¼Œè¿”å›é»˜è®¤å€¼
            return np.column_stack([
                np.ones(len(X)) * 0.5,
                np.ones(len(X)) * 0.5
            ])
        
        # ä½¿ç”¨metaæ¨¡å‹èåˆ
        if self.meta_model:
            meta_X = pd.DataFrame(base_predictions)
            final_proba = self.meta_model.predict_proba(meta_X)
            return np.column_stack([1 - final_proba, final_proba])
        else:
            # ç®€å•å¹³å‡
            avg_proba = np.mean(list(base_predictions.values()), axis=0)
            return np.column_stack([1 - avg_proba, avg_proba])
    
    def evaluate(
        self,
        X: pd.DataFrame,
        y: pd.Series
    ) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        
        y_pred = self.predict(X)
        y_proba = self.predict_proba(X)[:, 1]
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = (y_pred == y).mean()
        
        # è®¡ç®—F1
        tp = ((y_pred == 1) & (y == 1)).sum()
        fp = ((y_pred == 1) & (y == 0)).sum()
        fn = ((y_pred == 0) & (y == 1)).sum()
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1
        }

    def save(self, file_path: str) -> None:
        """ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶ã€‚

        ä¼˜å…ˆä½¿ç”¨ joblibï¼ˆå¦‚å¯ç”¨ï¼‰ï¼Œå¦åˆ™å›é€€åˆ° pickleã€‚
        ä»…ä¿å­˜å¿…è¦çŠ¶æ€ï¼šconfigã€base_modelsã€meta_modelã€‚
        """
        try:
            os.makedirs(os.path.dirname(file_path), exist_ok=True) if os.path.dirname(file_path) else None
        except Exception:
            pass

        state = {
            'version': 1,
            'config': self.config,
            'base_models': self.base_models,
            'meta_model': self.meta_model,
        }

        # å°è¯•ä½¿ç”¨ joblib
        try:
            import joblib  # type: ignore
            joblib.dump(state, file_path)
            logger.info(f"æ¨¡å‹å·²ä¿å­˜: {file_path} (joblib)")
            return
        except Exception as e:
            logger.debug(f"joblib ä¿å­˜å¤±è´¥ï¼Œæ”¹ç”¨ pickle: {e}")

        # å›é€€åˆ° pickle
        try:
            with open(file_path, 'wb') as f:
                pickle.dump(state, f)
            logger.info(f"æ¨¡å‹å·²ä¿å­˜: {file_path} (pickle)")
        except Exception as e:
            logger.exception(f"æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
            raise

    @classmethod
    def load(cls, file_path: str) -> "LimitUpEnsembleModel":
        """ä»æ–‡ä»¶åŠ è½½æ¨¡å‹å¹¶è¿”å›å®ä¾‹ã€‚"""
        state = None

        # å°è¯•ä½¿ç”¨ joblib åŠ è½½
        try:
            import joblib  # type: ignore
            state = joblib.load(file_path)
            logger.info(f"æ¨¡å‹å·²åŠ è½½: {file_path} (joblib)")
        except Exception as e:
            logger.debug(f"joblib åŠ è½½å¤±è´¥ï¼Œæ”¹ç”¨ pickle: {e}")
            try:
                with open(file_path, 'rb') as f:
                    state = pickle.load(f)
                logger.info(f"æ¨¡å‹å·²åŠ è½½: {file_path} (pickle)")
            except Exception as e2:
                logger.exception(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e2}")
                raise

        # è¿˜åŸå®ä¾‹
        config = state.get('config', {}) if isinstance(state, dict) else {}
        model = cls(config=config)
        if isinstance(state, dict):
            model.base_models = state.get('base_models', {})
            model.meta_model = state.get('meta_model')
        return model


class SimpleClassifier:
    """ç®€å•è§„åˆ™åˆ†ç±»å™¨ï¼ˆå½“æ²¡æœ‰MLåº“æ—¶ä½¿ç”¨ï¼‰"""
    
    def __init__(self):
        self.threshold = {}
    
    def fit(self, X, y):
        """å­¦ä¹ æ¯ä¸ªç‰¹å¾çš„æœ€ä½³é˜ˆå€¼"""
        for col in X.columns:
            # è®¡ç®—ç›¸å…³ç³»æ•°
            corr = X[col].corr(y)
            # ä½¿ç”¨ä¸­ä½æ•°ä½œä¸ºé˜ˆå€¼
            self.threshold[col] = X[col].median()
    
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        scores = []
        for col in X.columns:
            if col in self.threshold:
                score = (X[col] > self.threshold[col]).astype(float)
                scores.append(score)
        
        if scores:
            avg_score = np.mean(scores, axis=0)
            return np.column_stack([1 - avg_score, avg_score])
        else:
            return np.column_stack([
                np.ones(len(X)) * 0.5,
                np.ones(len(X)) * 0.5
            ])
    
    def predict(self, X):
        """é¢„æµ‹ç±»åˆ«"""
        proba = self.predict_proba(X)
        return (proba[:, 1] > 0.5).astype(int)


class SimpleMetaModel:
    """ç®€å•å…ƒæ¨¡å‹ï¼ˆåŠ æƒå¹³å‡ï¼‰"""
    
    def __init__(self):
        self.weights = None
    
    def fit(self, X, y):
        """å­¦ä¹ æœ€ä½³æƒé‡"""
        # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„F1åˆ†æ•°ä½œä¸ºæƒé‡
        weights = []
        for col in X.columns:
            pred = (X[col] > 0.5).astype(int)
            tp = ((pred == 1) & (y == 1)).sum()
            fp = ((pred == 1) & (y == 0)).sum()
            fn = ((pred == 0) & (y == 1)).sum()
            
            if tp + fp > 0:
                precision = tp / (tp + fp)
            else:
                precision = 0
            
            if tp + fn > 0:
                recall = tp / (tp + fn)
            else:
                recall = 0
            
            if precision + recall > 0:
                f1 = 2 * precision * recall / (precision + recall)
            else:
                f1 = 0
            
            weights.append(f1)
        
        # å½’ä¸€åŒ–æƒé‡
        total = sum(weights)
        if total > 0:
            self.weights = {col: w / total for col, w in zip(X.columns, weights)}
        else:
            self.weights = {col: 1 / len(X.columns) for col in X.columns}
    
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        if self.weights is None:
            return np.mean(X.values, axis=1)
        
        weighted_sum = np.zeros(len(X))
        for col, weight in self.weights.items():
            if col in X.columns:
                weighted_sum += X[col].values * weight
        
        return weighted_sum
    
    def predict(self, X):
        """é¢„æµ‹ç±»åˆ«"""
        proba = self.predict_proba(X)
        return (proba > 0.5).astype(int)


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

def main():
    """ç¤ºä¾‹ï¼šè®­ç»ƒå’Œæµ‹è¯•é›†æˆæ¨¡å‹"""
    logger.info("=" * 80)
    logger.info("æ¶¨åœæ¿é›†æˆå­¦ä¹ æ¨¡å‹ - æµ‹è¯•")
    logger.info("=" * 80)
    
    # 1. ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    logger.info("\nğŸ“Š ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...")
    np.random.seed(42)
    n_samples = 1000
    n_features = 8
    
    # ç”Ÿæˆç‰¹å¾
    X = pd.DataFrame(
        np.random.randn(n_samples, n_features),
        columns=[f'feature_{i}' for i in range(n_features)]
    )
    
    # ç”Ÿæˆç›®æ ‡ï¼ˆåŸºäºç®€å•è§„åˆ™ï¼‰
    y = ((X['feature_0'] > 0) & 
         (X['feature_1'] > 0) & 
         (X['feature_2'] > 0.5)).astype(int)
    
    logger.info(f"   æ ·æœ¬æ•°: {n_samples}")
    logger.info(f"   ç‰¹å¾æ•°: {n_features}")
    logger.info(f"   æ­£æ ·æœ¬ç‡: {y.mean():.1%}")
    
    # 2. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    logger.info(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    logger.info(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    
    # 3. è®­ç»ƒé›†æˆæ¨¡å‹
    model = LimitUpEnsembleModel()
    model.fit(X_train, y_train, X_test, y_test)
    
    # 4. è¯„ä¼°æ¨¡å‹
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“Š æ¨¡å‹è¯„ä¼°")
    logger.info("=" * 80)
    
    train_metrics = model.evaluate(X_train, y_train)
    test_metrics = model.evaluate(X_test, y_test)
    
    logger.info("\nè®­ç»ƒé›†:")
    for metric, value in train_metrics.items():
        logger.info(f"  {metric}: {value:.4f}")
    
    logger.info("\næµ‹è¯•é›†:")
    for metric, value in test_metrics.items():
        logger.info(f"  {metric}: {value:.4f}")
    
    # 5. é¢„æµ‹ç¤ºä¾‹
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ¯ é¢„æµ‹ç¤ºä¾‹ï¼ˆå‰10ä¸ªæ ·æœ¬ï¼‰")
    logger.info("=" * 80)
    
    sample_X = X_test.head(10)
    sample_y = y_test.head(10).values
    predictions = model.predict(sample_X)
    probabilities = model.predict_proba(sample_X)[:, 1]
    
    logger.info("\næ ·æœ¬  çœŸå®  é¢„æµ‹  æ¦‚ç‡")
    logger.info("-" * 40)
    for i in range(len(sample_X)):
        logger.info(f"{i+1:4d}  {sample_y[i]:4d}  {predictions[i]:4d}  {probabilities[i]:.2%}")
    
    logger.info("\n" + "=" * 80)
    logger.info("âœ… æµ‹è¯•å®Œæˆï¼")
    logger.info("=" * 80)


if __name__ == '__main__':
    from app.core.logging_setup import setup_logging
    setup_logging()
    # æ£€æŸ¥sklearnæ˜¯å¦å¯ç”¨
    try:
        from sklearn.model_selection import train_test_split
        main()
    except ImportError:
        logger.warning("âš ï¸  sklearnæœªå®‰è£…ï¼Œæ— æ³•è¿è¡Œå®Œæ•´æµ‹è¯•")
        logger.info("   è¯·å®‰è£…: pip install scikit-learn")
        
        # è¿è¡Œç®€åŒ–ç‰ˆæœ¬
        logger.info("\nè¿è¡Œç®€åŒ–æµ‹è¯•...")
        model = LimitUpEnsembleModel()
        logger.info(f"\nå¯ç”¨æ¨¡å‹: {list(model.base_models.keys())}")
