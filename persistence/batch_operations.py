"""
æ‰¹é‡æ•°æ®åº“æ“ä½œä¼˜åŒ–å™¨ (Batch Database Operations)
æä¾›é«˜æ€§èƒ½çš„æ‰¹é‡æ’å…¥ã€æ›´æ–°å’ŒæŸ¥è¯¢åŠŸèƒ½
"""

import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from contextlib import contextmanager
import pandas as pd

try:
    from sqlalchemy import create_engine, text
    from sqlalchemy.orm import sessionmaker, Session
    from sqlalchemy.pool import QueuePool
    SQLALCHEMY_AVAILABLE = True
except ImportError:
    SQLALCHEMY_AVAILABLE = False
    print("âš ï¸  SQLAlchemyæœªå®‰è£…ï¼Œæ‰¹é‡æ“ä½œåŠŸèƒ½å°†å—é™ã€‚å®‰è£…æ–¹æ³•: pip install sqlalchemy")

logger = logging.getLogger(__name__)


class BatchDatabaseOperations:
    """
    æ‰¹é‡æ•°æ®åº“æ“ä½œç±»
    
    ç‰¹æ€§:
    1. æ‰¹é‡æ’å…¥ä¼˜åŒ–ï¼ˆä½¿ç”¨bulk_insert_mappingsï¼‰
    2. æ‰¹é‡æ›´æ–°ä¼˜åŒ–
    3. äº‹åŠ¡ç®¡ç†
    4. è¿æ¥æ± ç®¡ç†
    5. è‡ªåŠ¨æ‰¹æ¬¡åˆ†å‰²
    """
    
    def __init__(
        self,
        database_url: str,
        batch_size: int = 1000,
        pool_size: int = 5,
        max_overflow: int = 10
    ):
        """
        åˆå§‹åŒ–æ‰¹é‡æ“ä½œå™¨
        
        Args:
            database_url: æ•°æ®åº“è¿æ¥URL
            batch_size: æ¯æ‰¹æ¬¡æ“ä½œçš„è®°å½•æ•°
            pool_size: è¿æ¥æ± å¤§å°
            max_overflow: è¿æ¥æ± æœ€å¤§æº¢å‡ºæ•°
        """
        if not SQLALCHEMY_AVAILABLE:
            raise ImportError("éœ€è¦å®‰è£…SQLAlchemy: pip install sqlalchemy")
        
        self.database_url = database_url
        self.batch_size = batch_size
        
        # åˆ›å»ºå¼•æ“ï¼ˆä½¿ç”¨è¿æ¥æ± ï¼‰
        self.engine = create_engine(
            database_url,
            poolclass=QueuePool,
            pool_size=pool_size,
            max_overflow=max_overflow,
            pool_pre_ping=True,  # è‡ªåŠ¨pingæ£€æŸ¥è¿æ¥æœ‰æ•ˆæ€§
            echo=False
        )
        
        # åˆ›å»ºSessionå·¥å‚
        self.SessionLocal = sessionmaker(
            autocommit=False,
            autoflush=False,
            bind=self.engine
        )
        
        logger.info(f"âœ… æ‰¹é‡æ•°æ®åº“æ“ä½œå™¨å·²åˆå§‹åŒ–: batch_size={batch_size}")
    
    @contextmanager
    def get_session(self) -> Session:
        """è·å–æ•°æ®åº“ä¼šè¯ï¼ˆä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰"""
        session = self.SessionLocal()
        try:
            yield session
            session.commit()
        except Exception as e:
            session.rollback()
            logger.error(f"æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
            raise
        finally:
            session.close()
    
    def bulk_insert(
        self,
        table_name: str,
        data: List[Dict[str, Any]],
        conflict_action: str = 'ignore'
    ) -> int:
        """
        æ‰¹é‡æ’å…¥æ•°æ®
        
        Args:
            table_name: è¡¨å
            data: æ•°æ®åˆ—è¡¨ï¼Œæ¯é¡¹æ˜¯ä¸€ä¸ªå­—å…¸
            conflict_action: å†²çªå¤„ç†æ–¹å¼ ('ignore', 'update')
            
        Returns:
            æ’å…¥çš„è®°å½•æ•°
        """
        if not data:
            return 0
        
        total_inserted = 0
        
        try:
            # åˆ†æ‰¹å¤„ç†
            batches = self._split_into_batches(data)
            
            with self.get_session() as session:
                for batch in batches:
                    try:
                        if conflict_action == 'ignore':
                            # ä½¿ç”¨INSERT IGNORE (MySQL) æˆ– ON CONFLICT DO NOTHING (PostgreSQL)
                            self._bulk_insert_ignore(session, table_name, batch)
                        elif conflict_action == 'update':
                            # ä½¿ç”¨ ON DUPLICATE KEY UPDATE (MySQL) æˆ– ON CONFLICT DO UPDATE (PostgreSQL)
                            self._bulk_insert_update(session, table_name, batch)
                        else:
                            # æ™®é€šæ’å…¥
                            self._bulk_insert_normal(session, table_name, batch)
                        
                        total_inserted += len(batch)
                        
                    except Exception as e:
                        logger.error(f"æ‰¹æ¬¡æ’å…¥å¤±è´¥: {e}")
                        raise
            
            logger.info(f"âœ… æ‰¹é‡æ’å…¥å®Œæˆ: {total_inserted} æ¡è®°å½•")
            return total_inserted
            
        except Exception as e:
            logger.error(f"æ‰¹é‡æ’å…¥å¤±è´¥: {e}")
            raise
    
    def _bulk_insert_normal(
        self,
        session: Session,
        table_name: str,
        data: List[Dict[str, Any]]
    ):
        """æ™®é€šæ‰¹é‡æ’å…¥"""
        # æ„å»ºINSERTè¯­å¥
        if not data:
            return
        
        columns = list(data[0].keys())
        placeholders = ', '.join([f':{col}' for col in columns])
        columns_str = ', '.join(columns)
        
        sql = f"INSERT INTO {table_name} ({columns_str}) VALUES ({placeholders})"
        
        session.execute(text(sql), data)
    
    def _bulk_insert_ignore(
        self,
        session: Session,
        table_name: str,
        data: List[Dict[str, Any]]
    ):
        """æ‰¹é‡æ’å…¥ï¼ˆå¿½ç•¥å†²çªï¼‰"""
        if not data:
            return
        
        columns = list(data[0].keys())
        placeholders = ', '.join([f':{col}' for col in columns])
        columns_str = ', '.join(columns)
        
        # æ ¹æ®æ•°æ®åº“ç±»å‹ä½¿ç”¨ä¸åŒçš„è¯­æ³•
        if 'postgresql' in self.database_url:
            sql = f"""
                INSERT INTO {table_name} ({columns_str})
                VALUES ({placeholders})
                ON CONFLICT DO NOTHING
            """
        elif 'mysql' in self.database_url:
            sql = f"""
                INSERT IGNORE INTO {table_name} ({columns_str})
                VALUES ({placeholders})
            """
        else:
            # SQLite
            sql = f"""
                INSERT OR IGNORE INTO {table_name} ({columns_str})
                VALUES ({placeholders})
            """
        
        session.execute(text(sql), data)
    
    def _bulk_insert_update(
        self,
        session: Session,
        table_name: str,
        data: List[Dict[str, Any]]
    ):
        """æ‰¹é‡æ’å…¥æˆ–æ›´æ–°"""
        if not data:
            return
        
        columns = list(data[0].keys())
        placeholders = ', '.join([f':{col}' for col in columns])
        columns_str = ', '.join(columns)
        
        # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯ä¸»é”®
        pk_column = columns[0]
        update_columns = columns[1:]
        update_str = ', '.join([f"{col} = excluded.{col}" for col in update_columns])
        
        if 'postgresql' in self.database_url:
            sql = f"""
                INSERT INTO {table_name} ({columns_str})
                VALUES ({placeholders})
                ON CONFLICT ({pk_column}) DO UPDATE SET {update_str}
            """
        elif 'mysql' in self.database_url:
            update_str = ', '.join([f"{col} = VALUES({col})" for col in update_columns])
            sql = f"""
                INSERT INTO {table_name} ({columns_str})
                VALUES ({placeholders})
                ON DUPLICATE KEY UPDATE {update_str}
            """
        else:
            # SQLite
            sql = f"""
                INSERT INTO {table_name} ({columns_str})
                VALUES ({placeholders})
                ON CONFLICT({pk_column}) DO UPDATE SET {update_str}
            """
        
        session.execute(text(sql), data)
    
    def bulk_update(
        self,
        table_name: str,
        data: List[Dict[str, Any]],
        key_column: str = 'id'
    ) -> int:
        """
        æ‰¹é‡æ›´æ–°æ•°æ®
        
        Args:
            table_name: è¡¨å
            data: æ•°æ®åˆ—è¡¨
            key_column: ä¸»é”®åˆ—å
            
        Returns:
            æ›´æ–°çš„è®°å½•æ•°
        """
        if not data:
            return 0
        
        total_updated = 0
        
        try:
            batches = self._split_into_batches(data)
            
            with self.get_session() as session:
                for batch in batches:
                    # æ„å»ºæ‰¹é‡UPDATEè¯­å¥
                    for item in batch:
                        if key_column not in item:
                            continue
                        
                        key_value = item[key_column]
                        update_fields = {k: v for k, v in item.items() if k != key_column}
                        
                        if not update_fields:
                            continue
                        
                        set_clause = ', '.join([f"{k} = :{k}" for k in update_fields.keys()])
                        sql = f"UPDATE {table_name} SET {set_clause} WHERE {key_column} = :key_value"
                        
                        params = {**update_fields, 'key_value': key_value}
                        session.execute(text(sql), params)
                        total_updated += 1
            
            logger.info(f"âœ… æ‰¹é‡æ›´æ–°å®Œæˆ: {total_updated} æ¡è®°å½•")
            return total_updated
            
        except Exception as e:
            logger.error(f"æ‰¹é‡æ›´æ–°å¤±è´¥: {e}")
            raise
    
    def bulk_delete(
        self,
        table_name: str,
        ids: List[Any],
        key_column: str = 'id'
    ) -> int:
        """
        æ‰¹é‡åˆ é™¤æ•°æ®
        
        Args:
            table_name: è¡¨å
            ids: IDåˆ—è¡¨
            key_column: ä¸»é”®åˆ—å
            
        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        if not ids:
            return 0
        
        try:
            with self.get_session() as session:
                # ä½¿ç”¨INå­å¥æ‰¹é‡åˆ é™¤
                sql = f"DELETE FROM {table_name} WHERE {key_column} IN :ids"
                result = session.execute(text(sql), {'ids': tuple(ids)})
                deleted_count = result.rowcount
            
            logger.info(f"âœ… æ‰¹é‡åˆ é™¤å®Œæˆ: {deleted_count} æ¡è®°å½•")
            return deleted_count
            
        except Exception as e:
            logger.error(f"æ‰¹é‡åˆ é™¤å¤±è´¥: {e}")
            raise
    
    def bulk_query(
        self,
        sql: str,
        params: Optional[Dict[str, Any]] = None,
        return_df: bool = False
    ) -> List[Dict] | pd.DataFrame:
        """
        æ‰¹é‡æŸ¥è¯¢
        
        Args:
            sql: SQLæŸ¥è¯¢è¯­å¥
            params: æŸ¥è¯¢å‚æ•°
            return_df: æ˜¯å¦è¿”å›DataFrame
            
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        try:
            with self.get_session() as session:
                result = session.execute(text(sql), params or {})
                
                if return_df:
                    # è¿”å›DataFrame
                    df = pd.DataFrame(result.fetchall(), columns=result.keys())
                    return df
                else:
                    # è¿”å›å­—å…¸åˆ—è¡¨
                    rows = []
                    for row in result:
                        rows.append(dict(row._mapping))
                    return rows
                    
        except Exception as e:
            logger.error(f"æ‰¹é‡æŸ¥è¯¢å¤±è´¥: {e}")
            raise
    
    def bulk_upsert_from_dataframe(
        self,
        df: pd.DataFrame,
        table_name: str,
        if_exists: str = 'append'
    ) -> int:
        """
        ä»DataFrameæ‰¹é‡æ’å…¥/æ›´æ–°æ•°æ®
        
        Args:
            df: DataFrame
            table_name: è¡¨å
            if_exists: 'append', 'replace', 'fail'
            
        Returns:
            æ“ä½œçš„è®°å½•æ•°
        """
        if df.empty:
            return 0
        
        try:
            # ä½¿ç”¨pandasçš„to_sqlæ–¹æ³•ï¼ˆé«˜åº¦ä¼˜åŒ–ï¼‰
            df.to_sql(
                name=table_name,
                con=self.engine,
                if_exists=if_exists,
                index=False,
                method='multi',  # ä½¿ç”¨å¤šå€¼INSERT
                chunksize=self.batch_size
            )
            
            row_count = len(df)
            logger.info(f"âœ… DataFrameæ‰¹é‡æ“ä½œå®Œæˆ: {row_count} æ¡è®°å½•")
            return row_count
            
        except Exception as e:
            logger.error(f"DataFrameæ‰¹é‡æ“ä½œå¤±è´¥: {e}")
            raise
    
    def _split_into_batches(
        self,
        data: List[Any]
    ) -> List[List[Any]]:
        """å°†æ•°æ®åˆ†å‰²æˆæ‰¹æ¬¡"""
        batches = []
        for i in range(0, len(data), self.batch_size):
            batches.append(data[i:i + self.batch_size])
        return batches
    
    def execute_transaction(
        self,
        operations: List[Tuple[str, List[Dict[str, Any]]]]
    ) -> bool:
        """
        æ‰§è¡Œäº‹åŠ¡ï¼ˆå¤šä¸ªæ“ä½œåŸå­æ€§ï¼‰
        
        Args:
            operations: æ“ä½œåˆ—è¡¨ [(table_name, data), ...]
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            with self.get_session() as session:
                for table_name, data in operations:
                    self._bulk_insert_normal(session, table_name, data)
                
                # äº‹åŠ¡ä¼šåœ¨ä¸Šä¸‹æ–‡é€€å‡ºæ—¶è‡ªåŠ¨æäº¤
            
            logger.info(f"âœ… äº‹åŠ¡æ‰§è¡ŒæˆåŠŸ: {len(operations)} ä¸ªæ“ä½œ")
            return True
            
        except Exception as e:
            logger.error(f"äº‹åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            return False
    
    def get_table_stats(self, table_name: str) -> Dict[str, Any]:
        """è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯"""
        try:
            sql = f"SELECT COUNT(*) as count FROM {table_name}"
            result = self.bulk_query(sql)
            
            return {
                'table_name': table_name,
                'row_count': result[0]['count'] if result else 0,
                'timestamp': datetime.now()
            }
            
        except Exception as e:
            logger.error(f"è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {'table_name': table_name, 'error': str(e)}
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        self.engine.dispose()
        logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    
    # åˆ›å»ºæ‰¹é‡æ“ä½œå™¨ï¼ˆä½¿ç”¨SQLiteä½œä¸ºç¤ºä¾‹ï¼‰
    db_ops = BatchDatabaseOperations(
        database_url="sqlite:///./test_batch.db",
        batch_size=500
    )
    
    print("\nğŸ“Š æµ‹è¯•æ‰¹é‡æ•°æ®åº“æ“ä½œ\n")
    
    # åˆ›å»ºæµ‹è¯•è¡¨
    with db_ops.get_session() as session:
        session.execute(text("""
            CREATE TABLE IF NOT EXISTS test_stocks (
                symbol VARCHAR(20) PRIMARY KEY,
                name VARCHAR(100),
                price FLOAT,
                volume INTEGER,
                updated_at TIMESTAMP
            )
        """))
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_data = [
        {
            'symbol': f'00000{i}.SZ',
            'name': f'Stock {i}',
            'price': 10.0 + i * 0.1,
            'volume': 1000 * i,
            'updated_at': datetime.now()
        }
        for i in range(1, 1001)
    ]
    
    print(f"å‡†å¤‡æ’å…¥ {len(test_data)} æ¡è®°å½•...")
    
    # æµ‹è¯•æ‰¹é‡æ’å…¥
    import time
    start = time.time()
    inserted = db_ops.bulk_insert('test_stocks', test_data, conflict_action='ignore')
    elapsed = time.time() - start
    
    print(f"âœ… æ’å…¥å®Œæˆ: {inserted} æ¡è®°å½•ï¼Œè€—æ—¶: {elapsed:.2f}ç§’")
    print(f"   æ€§èƒ½: {inserted/elapsed:.0f} æ¡/ç§’")
    
    # æŸ¥è¯¢ç»Ÿè®¡
    stats = db_ops.get_table_stats('test_stocks')
    print(f"\nğŸ“ˆ è¡¨ç»Ÿè®¡: {stats}")
    
    # æ¸…ç†
    print("\nğŸ§¹ æ¸…ç†æµ‹è¯•æ•°æ®...")
    db_ops.close()
    import os
    if os.path.exists('test_batch.db'):
        os.remove('test_batch.db')
    print("âœ… å®Œæˆ")
