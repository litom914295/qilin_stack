"""å°†P2-1ç¼ è®ºAlphaå› å­å†™å…¥Qlibå­˜å‚¨

è¿™ä¸ªè„šæœ¬è´Ÿè´£è®¡ç®—å¹¶æŒä¹…åŒ–ä¸‰ä¸ªP2-1 Alphaæ´¾ç”Ÿå› å­åˆ°Qlibæ•°æ®ä»“åº“ï¼Œ
ä»¥ä¾¿ICåˆ†æå’Œå›æµ‹Tabå¯ä»¥æ— ç¼åŠ è½½è¿™äº›å› å­ï¼Œæ— éœ€æ‰‹åŠ¨æ³¨å…¥ã€‚

ä¸‰ä¸ªAlphaå› å­ï¼š
1. alpha_zs_movement = zs_movement_direction Ã— zs_movement_confidence
2. alpha_zs_upgrade = zs_upgrade_flag Ã— zs_upgrade_strength  
3. alpha_confluence = tanh(confluence_score)

ä¾èµ–ï¼š
- qlib_enhanced/chanlun/chanlun_alpha.py: Alphaè®¡ç®—é€»è¾‘
- features/chanlun/chanpy_features.py: åŸºç¡€ç¼ è®ºç‰¹å¾ç”Ÿæˆï¼ˆåŒ…å«ä¸­æ¢ç§»åŠ¨/å…±æŒ¯å­—æ®µï¼‰

ä½œè€…: Warp AI Assistant
æ—¥æœŸ: 2025-01
é¡¹ç›®: éº’éºŸé‡åŒ–ç³»ç»Ÿ - P2 Alphaé›†æˆ
"""
import sys
from pathlib import Path
import logging
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    import qlib
    from qlib.data import D
    from qlib.config import C
except ImportError:
    qlib = None
    D = None
    C = None
    print("âš ï¸  Qlibæœªå®‰è£…æˆ–ä¸å¯ç”¨")

from qlib_enhanced.chanlun.chanlun_alpha import ChanLunAlphaFactors
from features.chanlun.chanpy_features import ChanPyFeatureGenerator

logger = logging.getLogger(__name__)


class ChanLunAlphaWriter:
    """ç¼ è®ºAlphaå› å­å†™å…¥å™¨
    
    åŠŸèƒ½ï¼š
    1. é€è‚¡ç¥¨ç”Ÿæˆå®Œæ•´çš„ç¼ è®ºåŸºç¡€ç‰¹å¾ï¼ˆåŒ…å«ä¸­æ¢ç§»åŠ¨/å…±æŒ¯ï¼‰
    2. è®¡ç®—P2-1ä¸‰ä¸ªAlphaæ´¾ç”Ÿå› å­
    3. å†™å…¥Qlibå­˜å‚¨ï¼Œä»¥ä¾¿åç»­åˆ†æå’Œå›æµ‹
    """
    
    ALPHA_FIELDS = [
        'alpha_zs_movement',   # ä¸­æ¢ç§»åŠ¨å¼ºåº¦
        'alpha_zs_upgrade',    # ä¸­æ¢å‡çº§å¼ºåº¦
        'alpha_confluence',    # å¤šå‘¨æœŸå…±æŒ¯å¼ºåº¦
    ]
    
    def __init__(self, 
                 provider_uri: str = None,
                 region: str = 'cn'):
        """åˆå§‹åŒ–
        
        Args:
            provider_uri: Qlibæ•°æ®è·¯å¾„ (None=ä½¿ç”¨é»˜è®¤)
            region: åŒºåŸŸä»£ç 
        """
        self.provider_uri = provider_uri
        self.region = region
        
        # åˆå§‹åŒ–Qlib
        if qlib is not None:
            try:
                qlib.init(provider_uri=provider_uri, region=region)
                logger.info(f"âœ… Qlibåˆå§‹åŒ–æˆåŠŸ: {C.get_data_path()}")
            except Exception as e:
                logger.warning(f"Qlibåˆå§‹åŒ–è­¦å‘Š: {e}")
        else:
            raise RuntimeError("Qlibä¸å¯ç”¨ï¼Œæ— æ³•å†™å…¥æ•°æ®")
        
        # åˆå§‹åŒ–ç‰¹å¾ç”Ÿæˆå™¨
        self.chanpy_gen = ChanPyFeatureGenerator(
            seg_algo='chan',
            bi_algo='normal',
            zs_combine=True
        )
        
        print("="*70)
        print("ğŸš€ ç¼ è®ºAlphaå› å­å†™å…¥å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ç›®æ ‡å› å­: {', '.join(self.ALPHA_FIELDS)}")
        print("="*70)
    
    def generate_alpha_for_stock(self, 
                                  code: str, 
                                  start: str, 
                                  end: str) -> pd.DataFrame:
        """ä¸ºå•ä¸ªè‚¡ç¥¨ç”ŸæˆAlphaå› å­
        
        Args:
            code: è‚¡ç¥¨ä»£ç  (å¦‚ 'SH600000')
            start: å¼€å§‹æ—¥æœŸ
            end: ç»“æŸæ—¥æœŸ
            
        Returns:
            DataFrame with columns: [datetime, alpha_zs_movement, alpha_zs_upgrade, alpha_confluence]
        """
        try:
            # 1. ä»QlibåŠ è½½OHLCV
            ohlcv_df = D.features(
                instruments=[code],
                fields=['$open', '$close', '$high', '$low', '$volume'],
                start_time=start,
                end_time=end,
                freq='day'
            )
            
            if ohlcv_df is None or len(ohlcv_df) == 0:
                logger.warning(f"{code}: æ— OHLCVæ•°æ®")
                return pd.DataFrame()
            
            # é‡ç½®ç´¢å¼•å¹¶å‡†å¤‡è¾“å…¥æ ¼å¼
            if isinstance(ohlcv_df.index, pd.MultiIndex):
                ohlcv_df = ohlcv_df.reset_index(level=0, drop=True)  # ç§»é™¤instrumentå±‚
            ohlcv_df = ohlcv_df.reset_index()  # datetimeå˜ä¸ºåˆ—
            ohlcv_df.rename(columns={'index': 'datetime'}, inplace=True)
            
            input_df = pd.DataFrame({
                'datetime': ohlcv_df['datetime'],
                'open': ohlcv_df['$open'],
                'close': ohlcv_df['$close'],
                'high': ohlcv_df['$high'],
                'low': ohlcv_df['$low'],
                'volume': ohlcv_df['$volume'],
            })
            
            # 2. ç”Ÿæˆå®Œæ•´ç¼ è®ºç‰¹å¾ï¼ˆåŒ…å«ä¸­æ¢ç§»åŠ¨/å…±æŒ¯å­—æ®µï¼‰
            full_features_df = self.chanpy_gen.generate_features(input_df, code=code)
            
            # 3. è®¡ç®—Alphaæ´¾ç”Ÿå› å­
            alpha_df = ChanLunAlphaFactors.generate_alpha_factors(full_features_df, code=code)
            
            # 4. æå–ä¸‰ä¸ªç›®æ ‡Alpha
            result = alpha_df[['datetime'] + self.ALPHA_FIELDS].copy()
            result['instrument'] = code
            
            return result
            
        except Exception as e:
            logger.error(f"{code}: Alphaç”Ÿæˆå¤±è´¥ - {e}", exc_info=True)
            return pd.DataFrame()
    
    def write_alphas_to_store(self, 
                               instruments: str = 'csi300',
                               start: str = '2020-01-01',
                               end: str = '2023-12-31',
                               output_path: str = None):
        """æ‰¹é‡è®¡ç®—å¹¶å†™å…¥Alphaå› å­åˆ°Qlibå­˜å‚¨
        
        Args:
            instruments: è‚¡ç¥¨æ±  (å¦‚ 'csi300', 'csi500')
            start: å¼€å§‹æ—¥æœŸ
            end: ç»“æŸæ—¥æœŸ
            output_path: å¯é€‰çš„CSVè¾“å‡ºè·¯å¾„ï¼ˆç”¨äºè°ƒè¯•/éªŒè¯ï¼‰
        
        Returns:
            ç”Ÿæˆçš„Alpha DataFrame
        """
        print(f"\nğŸ”„ å¼€å§‹æ‰¹é‡ç”ŸæˆAlphaå› å­...")
        print(f"   è‚¡ç¥¨æ± : {instruments}")
        print(f"   æ—¶é—´èŒƒå›´: {start} ~ {end}")
        
        # 1. è·å–è‚¡ç¥¨åˆ—è¡¨
        try:
            inst_list_df = D.instruments(market=instruments)
            if isinstance(inst_list_df, pd.DataFrame):
                inst_codes = inst_list_df.index.tolist()
            else:
                inst_codes = inst_list_df
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return None
        
        print(f"   è‚¡ç¥¨æ•°é‡: {len(inst_codes)}")
        
        # 2. é€è‚¡ç¥¨ç”Ÿæˆ
        alpha_list = []
        success_count = 0
        fail_count = 0
        
        for i, code in enumerate(inst_codes, 1):
            print(f"\r   è¿›åº¦: {i}/{len(inst_codes)} - {code}", end='', flush=True)
            
            alpha_df = self.generate_alpha_for_stock(code, start, end)
            
            if not alpha_df.empty:
                alpha_list.append(alpha_df)
                success_count += 1
            else:
                fail_count += 1
        
        print()  # æ¢è¡Œ
        
        if not alpha_list:
            print("âŒ æœªç”Ÿæˆä»»ä½•Alphaæ•°æ®")
            return None
        
        # 3. åˆå¹¶æ‰€æœ‰è‚¡ç¥¨çš„Alpha
        all_alphas = pd.concat(alpha_list, ignore_index=True)
        all_alphas = all_alphas.set_index(['instrument', 'datetime']).sort_index()
        
        print(f"\nâœ… Alphaå› å­ç”Ÿæˆå®Œæˆ")
        print(f"   æˆåŠŸ: {success_count}, å¤±è´¥: {fail_count}")
        print(f"   æ•°æ®å½¢çŠ¶: {all_alphas.shape}")
        print(f"   æ—¥æœŸèŒƒå›´: {all_alphas.index.get_level_values('datetime').min()} ~ {all_alphas.index.get_level_values('datetime').max()}")
        
        # 4. å¯é€‰ï¼šä¿å­˜ä¸ºCSVï¼ˆè°ƒè¯•ç”¨ï¼‰
        if output_path:
            output_file = Path(output_path)
            output_file.parent.mkdir(parents=True, exist_ok=True)
            all_alphas.to_csv(output_file)
            print(f"   ğŸ’¾ å·²ä¿å­˜CSV: {output_file}")
        
        # 5. å†™å…¥Qlibå­˜å‚¨
        self._write_to_qlib_store(all_alphas, instruments, start, end)
        
        return all_alphas
    
    def _write_to_qlib_store(self, 
                             alpha_df: pd.DataFrame,
                             instruments: str,
                             start: str,
                             end: str):
        """å°†Alphaæ•°æ®å†™å…¥Qlib feature store
        
        æ³¨æ„ï¼šQlibçš„feature storeæ˜¯é€šè¿‡D.features()è¯»å–çš„åº•å±‚å­˜å‚¨
        é»˜è®¤æƒ…å†µä¸‹ï¼Œè‡ªå®šä¹‰å› å­éœ€è¦é€šè¿‡ä»¥ä¸‹æ–¹å¼æŒä¹…åŒ–ï¼š
        1. ä½¿ç”¨dump_binå·¥å…·å°†DataFrameè½¬æ¢ä¸ºQlibäºŒè¿›åˆ¶æ ¼å¼
        2. æˆ–è€…é€šè¿‡æ‰©å±•Providerå®ç°è‡ªå®šä¹‰æ•°æ®æº
        
        è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨ä¸­é—´æ–¹æ¡ˆï¼šå°†Alphaä¿å­˜åˆ°é¡¹ç›®ç›®å½•ä¸‹çš„pickleç¼“å­˜ï¼Œ
        ä¾›åç»­load_factor_from_qlibæ›¿æ¢é€»è¾‘ä½¿ç”¨
        """
        print(f"\nğŸ“¦ å‡†å¤‡å†™å…¥Qlibå­˜å‚¨...")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        store_dir = project_root / 'data' / 'qlib_alpha_cache'
        store_dir.mkdir(parents=True, exist_ok=True)
        
        # é€ä¸ªAlphaå› å­ä¿å­˜ä¸ºå•ç‹¬çš„pickleæ–‡ä»¶
        for alpha_name in self.ALPHA_FIELDS:
            if alpha_name not in alpha_df.columns:
                logger.warning(f"âš ï¸  Alphaå› å­ {alpha_name} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                continue
            
            # æå–å•ä¸ªAlphaçš„Series
            alpha_series = alpha_df[alpha_name]
            
            # æ–‡ä»¶å‘½å: {alpha_name}_{instruments}_{start}_{end}.pkl
            filename = f"{alpha_name}_{instruments}_{start}_{end}.pkl"
            filepath = store_dir / filename
            
            # ä¿å­˜
            alpha_series.to_pickle(filepath)
            print(f"   âœ… {alpha_name} -> {filepath.name}")
        
        # ä¿å­˜å…ƒä¿¡æ¯
        meta = {
            'instruments': instruments,
            'start': start,
            'end': end,
            'alpha_fields': self.ALPHA_FIELDS,
            'generated_at': datetime.now().isoformat(),
            'shape': alpha_df.shape,
        }
        meta_file = store_dir / f"_meta_{instruments}_{start}_{end}.json"
        import json
        with open(meta_file, 'w') as f:
            json.dump(meta, f, indent=2)
        print(f"   â„¹ï¸  å…ƒä¿¡æ¯: {meta_file.name}")
        
        print(f"\nâœ… Alphaå› å­å·²å†™å…¥æœ¬åœ°ç¼“å­˜: {store_dir}")
        print(f"   åç»­ICåˆ†æå’Œå›æµ‹å¯é€šè¿‡load_factor_from_qlib_cache()åŠ è½½")
    
    def verify_alphas(self, 
                      instruments: str = 'csi300',
                      start: str = '2020-01-01',
                      end: str = '2023-12-31') -> Dict:
        """éªŒè¯å†™å…¥çš„Alphaå› å­
        
        Args:
            instruments: è‚¡ç¥¨æ± 
            start: å¼€å§‹æ—¥æœŸ
            end: ç»“æŸæ—¥æœŸ
            
        Returns:
            éªŒè¯ç»Ÿè®¡å­—å…¸
        """
        print(f"\nğŸ” éªŒè¯Alphaå› å­...")
        
        store_dir = project_root / 'data' / 'qlib_alpha_cache'
        
        stats = {}
        
        for alpha_name in self.ALPHA_FIELDS:
            filename = f"{alpha_name}_{instruments}_{start}_{end}.pkl"
            filepath = store_dir / filename
            
            if not filepath.exists():
                stats[alpha_name] = {'status': 'âŒ æœªæ‰¾åˆ°'}
                continue
            
            try:
                alpha_series = pd.read_pickle(filepath)
                
                stats[alpha_name] = {
                    'status': 'âœ… æ­£å¸¸',
                    'shape': alpha_series.shape,
                    'null_ratio': f"{alpha_series.isna().sum() / len(alpha_series) * 100:.2f}%",
                    'mean': f"{alpha_series.mean():.4f}",
                    'std': f"{alpha_series.std():.4f}",
                    'min': f"{alpha_series.min():.4f}",
                    'max': f"{alpha_series.max():.4f}",
                }
            except Exception as e:
                stats[alpha_name] = {'status': f'âŒ åŠ è½½å¤±è´¥: {e}'}
        
        # æ‰“å°éªŒè¯ç»“æœ
        print("\nğŸ“Š éªŒè¯ç»“æœ:")
        for alpha_name, stat in stats.items():
            print(f"\n   {alpha_name}:")
            for k, v in stat.items():
                print(f"      {k}: {v}")
        
        return stats


def load_factor_from_qlib_cache(
    alpha_name: str,
    instruments: str = 'csi300',
    start: str = '2020-01-01',
    end: str = '2023-12-31',
) -> pd.DataFrame:
    """ä»Qlib Alphaç¼“å­˜åŠ è½½å› å­
    
    è¿™æ˜¯ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œä¾›ICåˆ†æå’Œå›æµ‹Tabä½¿ç”¨ï¼Œ
    ç”¨äºæ— ç¼åŠ è½½å·²æŒä¹…åŒ–çš„Alphaå› å­ã€‚
    
    Args:
        alpha_name: Alphaå› å­åç§° (å¦‚ 'alpha_confluence')
        instruments: è‚¡ç¥¨æ± 
        start: å¼€å§‹æ—¥æœŸ
        end: ç»“æŸæ—¥æœŸ
        
    Returns:
        DataFrame with MultiIndex[instrument, datetime] and single column
    """
    store_dir = Path(__file__).parent.parent / 'data' / 'qlib_alpha_cache'
    filename = f"{alpha_name}_{instruments}_{start}_{end}.pkl"
    filepath = store_dir / filename
    
    if not filepath.exists():
        raise FileNotFoundError(f"Alphaç¼“å­˜æœªæ‰¾åˆ°: {filepath}")
    
    alpha_series = pd.read_pickle(filepath)
    
    # è½¬æ¢ä¸ºDataFrameæ ¼å¼ï¼ˆä¸load_factor_from_qlibå…¼å®¹ï¼‰
    df = alpha_series.to_frame(name='factor')
    df['label'] = 0  # placeholder
    
    return df


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å°†P2-1ç¼ è®ºAlphaå› å­å†™å…¥Qlibå­˜å‚¨')
    parser.add_argument('--instruments', type=str, default='csi300',
                        help='è‚¡ç¥¨æ±  (é»˜è®¤: csi300)')
    parser.add_argument('--start', type=str, default='2020-01-01',
                        help='å¼€å§‹æ—¥æœŸ (é»˜è®¤: 2020-01-01)')
    parser.add_argument('--end', type=str, default='2023-12-31',
                        help='ç»“æŸæ—¥æœŸ (é»˜è®¤: 2023-12-31)')
    parser.add_argument('--provider-uri', type=str, default=None,
                        help='Qlibæ•°æ®è·¯å¾„ (é»˜è®¤: None=ä½¿ç”¨Qlibé»˜è®¤)')
    parser.add_argument('--output-csv', type=str, default=None,
                        help='å¯é€‰çš„CSVè¾“å‡ºè·¯å¾„ï¼ˆç”¨äºè°ƒè¯•ï¼‰')
    parser.add_argument('--verify', action='store_true',
                        help='ä»…éªŒè¯å·²æœ‰çš„Alphaæ•°æ®ï¼Œä¸ç”Ÿæˆ')
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler(project_root / 'logs' / 'chanlun_alpha_write.log'),
            logging.StreamHandler()
        ]
    )
    
    # åˆ›å»ºå†™å…¥å™¨
    writer = ChanLunAlphaWriter(
        provider_uri=args.provider_uri,
        region='cn'
    )
    
    # éªŒè¯æ¨¡å¼
    if args.verify:
        writer.verify_alphas(
            instruments=args.instruments,
            start=args.start,
            end=args.end
        )
        return
    
    # ç”Ÿæˆå¹¶å†™å…¥
    start_time = datetime.now()
    
    alpha_df = writer.write_alphas_to_store(
        instruments=args.instruments,
        start=args.start,
        end=args.end,
        output_path=args.output_csv
    )
    
    elapsed = (datetime.now() - start_time).total_seconds()
    
    if alpha_df is not None:
        print(f"\nâ±ï¸  æ€»è€—æ—¶: {elapsed:.1f}ç§’")
        
        # éªŒè¯
        writer.verify_alphas(
            instruments=args.instruments,
            start=args.start,
            end=args.end
        )
        
        print(f"\nâœ… P2-1 Alphaå› å­å†™å…¥å®Œæˆ!")
        print(f"   åç»­åœ¨ICåˆ†æ/å›æµ‹Tabä¸­å¯ç›´æ¥ä½¿ç”¨:")
        print(f"      - $alpha_zs_movement")
        print(f"      - $alpha_zs_upgrade")
        print(f"      - $alpha_confluence")
    else:
        print(f"\nâŒ Alphaå› å­å†™å…¥å¤±è´¥")


if __name__ == '__main__':
    main()
