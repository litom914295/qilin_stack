"""
åŸºå‡†æ¨¡å‹è®­ç»ƒè„šæœ¬

æ ¹æ® docs/IMPROVEMENT_ROADMAP.md é˜¶æ®µä¸€ä»»åŠ¡1.4
ç›®æ ‡ï¼šä½¿ç”¨å•ä¸€LightGBMå’Œ50æ ¸å¿ƒç‰¹å¾è®­ç»ƒåŸºå‡†æ¨¡å‹

æ¨¡å‹é…ç½®ï¼ˆä¿å®ˆè®¾ç½®ï¼‰ï¼š
- ç®—æ³•: LightGBMï¼ˆå•ä¸€æ¨¡å‹ï¼Œæ— é›†æˆï¼‰
- max_depth: 5
- num_leaves: 31
- learning_rate: 0.05
- n_estimators: 100
- æ•°æ®åˆ’åˆ†: 60%è®­ç»ƒ / 20%éªŒè¯ / 20%æµ‹è¯•ï¼ˆä¸¥æ ¼æ—¶é—´åˆ‡åˆ†ï¼‰

éªŒæ”¶æ ‡å‡†ï¼š
- æ ·æœ¬å¤–AUC > 0.68
- AUCæ ‡å‡†å·® < 0.05
- ç”ŸæˆSHAPç‰¹å¾è§£é‡Š

ä½œè€…ï¼šQilin Quant Team
åˆ›å»ºï¼š2025-10-30
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import sys
from datetime import datetime
import pickle
import json
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# æœºå™¨å­¦ä¹ åº“
try:
    import lightgbm as lgb
    from sklearn.model_selection import TimeSeriesSplit
    from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score
    import shap
except ImportError as e:
    print(f"âš ï¸ ç¼ºå°‘ä¾èµ–åº“: {e}")
    print("è¯·è¿è¡Œ: pip install lightgbm scikit-learn shap")
    sys.exit(1)


class BaselineModelTrainer:
    """åŸºå‡†æ¨¡å‹è®­ç»ƒå™¨"""
    
    # ä¿å®ˆçš„è¶…å‚æ•°é…ç½®
    DEFAULT_PARAMS = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'max_depth': 5,
        'num_leaves': 31,
        'learning_rate': 0.05,
        'n_estimators': 100,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'min_child_samples': 20,
        'reg_alpha': 0.1,
        'reg_lambda': 0.1,
        'random_state': 42,
        'verbose': -1
    }
    
    def __init__(self, 
                 model_params: Dict = None,
                 train_ratio: float = 0.6,
                 valid_ratio: float = 0.2,
                 test_ratio: float = 0.2):
        """
        åˆå§‹åŒ–åŸºå‡†æ¨¡å‹è®­ç»ƒå™¨
        
        Args:
            model_params: æ¨¡å‹è¶…å‚æ•°
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            valid_ratio: éªŒè¯é›†æ¯”ä¾‹
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        """
        self.params = model_params or self.DEFAULT_PARAMS.copy()
        self.train_ratio = train_ratio
        self.valid_ratio = valid_ratio
        self.test_ratio = test_ratio
        
        # è®­ç»ƒç»“æœ
        self.model = None
        self.feature_names = []
        self.metrics = {}
        self.training_history = {}
        
        print(f"ğŸ¯ åŸºå‡†æ¨¡å‹è®­ç»ƒå™¨åˆå§‹åŒ–")
        print(f"   æ¨¡å‹: LightGBM (å•ä¸€æ¨¡å‹)")
        print(f"   æ•°æ®åˆ’åˆ†: {train_ratio:.0%} / {valid_ratio:.0%} / {test_ratio:.0%}")
        print("=" * 70)
    
    def load_data(self, data_path: str = None) -> Tuple[pd.DataFrame, pd.Series]:
        """
        åŠ è½½è®­ç»ƒæ•°æ®
        
        Returns:
            X: ç‰¹å¾æ•°æ®
            y: æ ‡ç­¾æ•°æ®
        """
        print("\nğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®...")
        
        if data_path is None:
            # é»˜è®¤è·¯å¾„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®å®é™…é¡¹ç›®è°ƒæ•´
            data_path = project_root / 'data' / 'train_data.csv'
        
        if not Path(data_path).exists():
            print(f"   âš ï¸ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            print(f"   ğŸ’¡ ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º...")
            return self._generate_mock_data()
        
        try:
            df = pd.read_csv(data_path)
            
            # å‡è®¾æœ€åä¸€åˆ—æ˜¯æ ‡ç­¾
            X = df.iloc[:, :-1]
            y = df.iloc[:, -1]
            
            print(f"   âœ… åŠ è½½æˆåŠŸ")
            print(f"   æ ·æœ¬æ•°: {len(X)}")
            print(f"   ç‰¹å¾æ•°: {X.shape[1]}")
            print(f"   æ­£æ ·æœ¬æ¯”ä¾‹: {y.mean():.2%}")
            
            return X, y
        
        except Exception as e:
            print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
            print(f"   ğŸ’¡ ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º...")
            return self._generate_mock_data()
    
    def _generate_mock_data(self, n_samples: int = 10000, n_features: int = 50) -> Tuple[pd.DataFrame, pd.Series]:
        """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºæ¼”ç¤º"""
        print(f"   ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®: {n_samples}æ ·æœ¬ x {n_features}ç‰¹å¾...")
        
        # ç”Ÿæˆæ¨¡æ‹Ÿç‰¹å¾
        np.random.seed(42)
        X = pd.DataFrame(
            np.random.randn(n_samples, n_features),
            columns=[f'feature_{i}' for i in range(n_features)]
        )
        
        # ç”Ÿæˆæ¨¡æ‹Ÿæ ‡ç­¾ï¼ˆä¸€è¿›äºŒæˆåŠŸç‡çº¦25%ï¼‰
        # ä½¿ç”¨éƒ¨åˆ†ç‰¹å¾ç”Ÿæˆæ ‡ç­¾ï¼Œæ¨¡æ‹ŸçœŸå®æƒ…å†µ
        signal = (
            X['feature_0'] * 0.3 +
            X['feature_1'] * 0.2 +
            X['feature_2'] * 0.15 +
            np.random.randn(n_samples) * 0.5
        )
        y = pd.Series((signal > 0.5).astype(int), name='label')
        
        print(f"   âœ… æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå®Œæˆ")
        print(f"   æ­£æ ·æœ¬æ¯”ä¾‹: {y.mean():.2%}")
        
        return X, y
    
    def split_data_by_time(self, 
                          X: pd.DataFrame, 
                          y: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, 
                                                  pd.Series, pd.Series, pd.Series]:
        """æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†æ•°æ®é›†"""
        print("\nğŸ”ª åˆ’åˆ†æ•°æ®é›†ï¼ˆä¸¥æ ¼æ—¶é—´åˆ‡åˆ†ï¼‰...")
        
        n_samples = len(X)
        n_train = int(n_samples * self.train_ratio)
        n_valid = int(n_samples * self.valid_ratio)
        
        # æ—¶é—´åˆ‡åˆ†
        X_train = X.iloc[:n_train]
        y_train = y.iloc[:n_train]
        
        X_valid = X.iloc[n_train:n_train+n_valid]
        y_valid = y.iloc[n_train:n_train+n_valid]
        
        X_test = X.iloc[n_train+n_valid:]
        y_test = y.iloc[n_train+n_valid:]
        
        print(f"   è®­ç»ƒé›†: {len(X_train)} ({len(X_train)/n_samples:.0%})")
        print(f"   éªŒè¯é›†: {len(X_valid)} ({len(X_valid)/n_samples:.0%})")
        print(f"   æµ‹è¯•é›†: {len(X_test)} ({len(X_test)/n_samples:.0%})")
        
        # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
        print(f"\n   æ ‡ç­¾åˆ†å¸ƒ:")
        print(f"   è®­ç»ƒé›†æ­£æ ·æœ¬: {y_train.mean():.2%}")
        print(f"   éªŒè¯é›†æ­£æ ·æœ¬: {y_valid.mean():.2%}")
        print(f"   æµ‹è¯•é›†æ­£æ ·æœ¬: {y_test.mean():.2%}")
        
        return X_train, X_valid, X_test, y_train, y_valid, y_test
    
    def train(self, 
             X_train: pd.DataFrame, 
             y_train: pd.Series,
             X_valid: pd.DataFrame,
             y_valid: pd.Series) -> lgb.LGBMClassifier:
        """è®­ç»ƒåŸºå‡†æ¨¡å‹"""
        print("\nğŸš€ å¼€å§‹è®­ç»ƒåŸºå‡†æ¨¡å‹...")
        print(f"   è¶…å‚æ•°:")
        for key, value in self.params.items():
            if key != 'verbose':
                print(f"      {key}: {value}")
        
        # åˆ›å»ºæ¨¡å‹
        model = lgb.LGBMClassifier(**self.params)
        
        # è®­ç»ƒ
        start_time = datetime.now()
        model.fit(
            X_train, y_train,
            eval_set=[(X_valid, y_valid)],
            eval_metric='auc',
            callbacks=[
                lgb.early_stopping(stopping_rounds=50, verbose=False),
                lgb.log_evaluation(period=0)  # ä¸æ‰“å°è®­ç»ƒæ—¥å¿—
            ]
        )
        train_time = (datetime.now() - start_time).total_seconds()
        
        self.model = model
        self.feature_names = list(X_train.columns)
        
        print(f"   âœ… è®­ç»ƒå®Œæˆ")
        print(f"   è®­ç»ƒæ—¶é—´: {train_time:.2f}ç§’")
        print(f"   æœ€ä½³è¿­ä»£: {model.best_iteration_}")
        
        return model
    
    def evaluate(self,
                X_test: pd.DataFrame,
                y_test: pd.Series,
                X_train: pd.DataFrame = None,
                y_train: pd.Series = None,
                X_valid: pd.DataFrame = None,
                y_valid: pd.Series = None) -> Dict:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("\nğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        metrics = {}
        
        # 1. è®­ç»ƒé›†æ€§èƒ½
        if X_train is not None and y_train is not None:
            y_train_pred = self.model.predict_proba(X_train)[:, 1]
            metrics['train_auc'] = roc_auc_score(y_train, y_train_pred)
            print(f"   è®­ç»ƒé›† AUC: {metrics['train_auc']:.4f}")
        
        # 2. éªŒè¯é›†æ€§èƒ½
        if X_valid is not None and y_valid is not None:
            y_valid_pred = self.model.predict_proba(X_valid)[:, 1]
            metrics['valid_auc'] = roc_auc_score(y_valid, y_valid_pred)
            print(f"   éªŒè¯é›† AUC: {metrics['valid_auc']:.4f}")
        
        # 3. æµ‹è¯•é›†æ€§èƒ½ï¼ˆæœ€é‡è¦ï¼‰
        y_test_pred = self.model.predict_proba(X_test)[:, 1]
        metrics['test_auc'] = roc_auc_score(y_test, y_test_pred)
        
        # è®¡ç®—P@20ï¼ˆTop 20çš„ç²¾ç¡®ç‡ï¼‰
        top_20_idx = np.argsort(y_test_pred)[-20:]
        metrics['test_p@20'] = y_test.iloc[top_20_idx].mean()
        
        # è®¡ç®—å…¶ä»–æŒ‡æ ‡
        y_test_pred_binary = (y_test_pred > 0.5).astype(int)
        metrics['test_precision'] = precision_score(y_test, y_test_pred_binary)
        metrics['test_recall'] = recall_score(y_test, y_test_pred_binary)
        metrics['test_f1'] = f1_score(y_test, y_test_pred_binary)
        
        print(f"\n   ğŸ¯ æµ‹è¯•é›†æ€§èƒ½ï¼ˆæ ·æœ¬å¤–ï¼‰:")
        print(f"      AUC: {metrics['test_auc']:.4f}")
        print(f"      P@20: {metrics['test_p@20']:.4f}")
        print(f"      Precision: {metrics['test_precision']:.4f}")
        print(f"      Recall: {metrics['test_recall']:.4f}")
        print(f"      F1: {metrics['test_f1']:.4f}")
        
        # 4. éªŒæ”¶æ ‡å‡†æ£€æŸ¥
        print(f"\n   ğŸ“‹ éªŒæ”¶æ ‡å‡†æ£€æŸ¥:")
        auc_pass = metrics['test_auc'] > 0.68
        print(f"      AUC > 0.68: {'âœ… é€šè¿‡' if auc_pass else 'âŒ æœªé€šè¿‡'} ({metrics['test_auc']:.4f})")
        
        self.metrics = metrics
        return metrics
    
    def analyze_feature_importance(self, top_n: int = 20) -> pd.DataFrame:
        """åˆ†æç‰¹å¾é‡è¦æ€§"""
        print(f"\nğŸ“ˆ åˆ†æç‰¹å¾é‡è¦æ€§ï¼ˆTop {top_n}ï¼‰...")
        
        # è·å–ç‰¹å¾é‡è¦æ€§
        importance = self.model.feature_importances_
        feature_importance = pd.DataFrame({
            'feature': self.feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)
        
        print(f"\n   Top {top_n} é‡è¦ç‰¹å¾:")
        for i, row in feature_importance.head(top_n).iterrows():
            print(f"      {i+1}. {row['feature']}: {row['importance']:.0f}")
        
        return feature_importance
    
    def generate_shap_explanation(self, 
                                 X_test: pd.DataFrame,
                                 max_samples: int = 100) -> Optional[shap.Explainer]:
        """ç”ŸæˆSHAPè§£é‡Š"""
        print(f"\nğŸ” ç”ŸæˆSHAPç‰¹å¾è§£é‡Šï¼ˆé‡‡æ ·{max_samples}ä¸ªæ ·æœ¬ï¼‰...")
        
        try:
            # é‡‡æ ·ï¼ˆSHAPè®¡ç®—è¾ƒæ…¢ï¼‰
            X_sample = X_test.sample(n=min(max_samples, len(X_test)), random_state=42)
            
            # åˆ›å»ºSHAPè§£é‡Šå™¨
            explainer = shap.TreeExplainer(self.model)
            shap_values = explainer.shap_values(X_sample)
            
            # å¦‚æœæ˜¯äºŒåˆ†ç±»ï¼Œå–æ­£ç±»çš„shapå€¼
            if isinstance(shap_values, list):
                shap_values = shap_values[1]
            
            # è®¡ç®—å¹³å‡ç»å¯¹SHAPå€¼
            mean_abs_shap = np.abs(shap_values).mean(axis=0)
            shap_importance = pd.DataFrame({
                'feature': self.feature_names,
                'mean_abs_shap': mean_abs_shap
            }).sort_values('mean_abs_shap', ascending=False)
            
            print(f"   âœ… SHAPè§£é‡Šç”Ÿæˆå®Œæˆ")
            print(f"\n   Top 10 SHAPé‡è¦ç‰¹å¾:")
            for i, row in shap_importance.head(10).iterrows():
                print(f"      {i+1}. {row['feature']}: {row['mean_abs_shap']:.4f}")
            
            return explainer
        
        except Exception as e:
            print(f"   âš ï¸ SHAPè§£é‡Šç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def save_model(self, output_path: str = None) -> str:
        """ä¿å­˜æ¨¡å‹"""
        print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
        
        if output_path is None:
            output_path = project_root / 'models' / 'baseline_lgbm_v1.pkl'
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹å’Œå…ƒæ•°æ®
        model_data = {
            'model': self.model,
            'feature_names': self.feature_names,
            'params': self.params,
            'metrics': self.metrics,
            'train_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        with open(output_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"   âœ… æ¨¡å‹å·²ä¿å­˜: {output_path}")
        
        return str(output_path)
    
    def generate_report(self, 
                       feature_importance: pd.DataFrame,
                       output_path: str = None) -> str:
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        print("\nğŸ“„ ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š...")
        
        if output_path is None:
            output_path = project_root / 'reports' / 'baseline_performance.md'
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        report = []
        report.append("# åŸºå‡†æ¨¡å‹æ€§èƒ½æŠ¥å‘Š\n\n")
        report.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        report.append(f"**ä»»åŠ¡æ¥æº**: docs/IMPROVEMENT_ROADMAP.md - é˜¶æ®µä¸€ä»»åŠ¡1.4\n")
        report.append(f"**æ¨¡å‹ç±»å‹**: LightGBM (å•ä¸€æ¨¡å‹ï¼Œæ— é›†æˆ)\n")
        report.append("\n---\n\n")
        
        # 1. æ¨¡å‹é…ç½®
        report.append("## 1. æ¨¡å‹é…ç½®\n\n")
        report.append("### è¶…å‚æ•°ï¼ˆä¿å®ˆè®¾ç½®ï¼‰\n\n")
        report.append("| å‚æ•° | å€¼ |\n")
        report.append("|------|----|\n")
        for key, value in self.params.items():
            if key != 'verbose':
                report.append(f"| {key} | {value} |\n")
        report.append("\n")
        
        # 2. æ•°æ®é›†ä¿¡æ¯
        report.append("## 2. æ•°æ®é›†åˆ’åˆ†\n\n")
        report.append(f"- **è®­ç»ƒé›†**: {self.train_ratio:.0%}\n")
        report.append(f"- **éªŒè¯é›†**: {self.valid_ratio:.0%}\n")
        report.append(f"- **æµ‹è¯•é›†**: {self.test_ratio:.0%}\n")
        report.append(f"- **åˆ’åˆ†æ–¹å¼**: ä¸¥æ ¼æ—¶é—´åˆ‡åˆ†ï¼ˆé¿å…æœªæ¥ä¿¡æ¯æ³„éœ²ï¼‰\n\n")
        
        # 3. æ€§èƒ½æŒ‡æ ‡
        report.append("## 3. æ€§èƒ½æŒ‡æ ‡\n\n")
        report.append("### æ ·æœ¬å†…æ€§èƒ½\n\n")
        report.append("| æ•°æ®é›† | AUC |\n")
        report.append("|--------|-----|\n")
        if 'train_auc' in self.metrics:
            report.append(f"| è®­ç»ƒé›† | {self.metrics['train_auc']:.4f} |\n")
        if 'valid_auc' in self.metrics:
            report.append(f"| éªŒè¯é›† | {self.metrics['valid_auc']:.4f} |\n")
        report.append("\n")
        
        report.append("### æ ·æœ¬å¤–æ€§èƒ½ï¼ˆæµ‹è¯•é›†ï¼‰â­\n\n")
        report.append("| æŒ‡æ ‡ | å€¼ |\n")
        report.append("|------|----|\n")
        report.append(f"| AUC | {self.metrics.get('test_auc', 0):.4f} |\n")
        report.append(f"| P@20 | {self.metrics.get('test_p@20', 0):.4f} |\n")
        report.append(f"| Precision | {self.metrics.get('test_precision', 0):.4f} |\n")
        report.append(f"| Recall | {self.metrics.get('test_recall', 0):.4f} |\n")
        report.append(f"| F1 Score | {self.metrics.get('test_f1', 0):.4f} |\n")
        report.append("\n")
        
        # 4. éªŒæ”¶æ ‡å‡†
        report.append("## 4. éªŒæ”¶æ ‡å‡†\n\n")
        test_auc = self.metrics.get('test_auc', 0)
        report.append("| æ ‡å‡† | ç›®æ ‡ | å®é™… | ç»“æœ |\n")
        report.append("|------|------|------|------|\n")
        report.append(f"| æ ·æœ¬å¤–AUC | > 0.68 | {test_auc:.4f} | {'âœ… é€šè¿‡' if test_auc > 0.68 else 'âŒ æœªé€šè¿‡'} |\n")
        report.append("\n")
        
        # 5. ç‰¹å¾é‡è¦æ€§
        report.append("## 5. ç‰¹å¾é‡è¦æ€§åˆ†æ\n\n")
        report.append("### Top 20 é‡è¦ç‰¹å¾\n\n")
        report.append("| æ’å | ç‰¹å¾åç§° | é‡è¦æ€§ |\n")
        report.append("|------|----------|--------|\n")
        for i, row in feature_importance.head(20).iterrows():
            report.append(f"| {i+1} | {row['feature']} | {row['importance']:.0f} |\n")
        report.append("\n")
        
        # 6. å…³é”®å‘ç°
        report.append("## 6. å…³é”®å‘ç°ä¸å»ºè®®\n\n")
        
        report.append("### ğŸ” å…³é”®å‘ç°\n\n")
        if test_auc > 0.75:
            report.append("1. âœ… **æ¨¡å‹æ€§èƒ½ä¼˜ç§€**: æ ·æœ¬å¤–AUC > 0.75\n")
        elif test_auc > 0.68:
            report.append("1. âœ… **æ¨¡å‹æ€§èƒ½è¾¾æ ‡**: æ ·æœ¬å¤–AUC > 0.68\n")
        else:
            report.append("1. âš ï¸ **æ¨¡å‹æ€§èƒ½å¾…æå‡**: æ ·æœ¬å¤–AUC < 0.68\n")
        
        if 'train_auc' in self.metrics and 'test_auc' in self.metrics:
            gap = self.metrics['train_auc'] - self.metrics['test_auc']
            if gap < 0.05:
                report.append("2. âœ… **æ¨¡å‹æ³›åŒ–è‰¯å¥½**: è®­ç»ƒ/æµ‹è¯•AUCå·®è· < 0.05\n")
            elif gap < 0.10:
                report.append("2. âš ï¸ **è½»å¾®è¿‡æ‹Ÿåˆ**: è®­ç»ƒ/æµ‹è¯•AUCå·®è· 0.05-0.10\n")
            else:
                report.append("2. âŒ **æ˜æ˜¾è¿‡æ‹Ÿåˆ**: è®­ç»ƒ/æµ‹è¯•AUCå·®è· > 0.10\n")
        
        report.append("\n### ğŸ’¡ ä¸‹ä¸€æ­¥è¡ŒåŠ¨\n\n")
        report.append("æ ¹æ® `docs/IMPROVEMENT_ROADMAP.md`:\n\n")
        report.append("1. âœ… **å®Œæˆ**: åŸºå‡†æ¨¡å‹è®­ç»ƒï¼ˆå½“å‰ä»»åŠ¡ï¼‰\n")
        report.append("2. â­ï¸ **ç¬¬äºŒå‘¨**: å› å­è¡°å‡ç›‘æ§ç³»ç»Ÿ (`monitoring/factor_decay_monitor.py`)\n")
        report.append("3. ğŸ“Œ **æŒç»­**: Walk-ForwardéªŒè¯æ¡†æ¶ï¼Œè¯„ä¼°æ¨¡å‹ç¨³å®šæ€§\n\n")
        
        report.append("### âš ï¸ é‡è¦æé†’\n\n")
        report.append("- åŸºå‡†æ¨¡å‹å·²ä¿å­˜åˆ° `models/baseline_lgbm_v1.pkl`\n")
        report.append("- æœ¬æ¨¡å‹ä½¿ç”¨æ ¸å¿ƒç‰¹å¾é›†v1.0ï¼ˆ50ä¸ªå¯é ç‰¹å¾ï¼‰\n")
        report.append("- å»ºè®®ä¸ä½¿ç”¨å…¨ç‰¹å¾é›†çš„å¤æ‚æ¨¡å‹è¿›è¡Œå¯¹æ¯”\n")
        report.append("- å¦‚æœAUCæœªè¾¾æ ‡ï¼Œè¯·æ£€æŸ¥ç‰¹å¾è´¨é‡å’Œæ ‡ç­¾å®šä¹‰\n\n")
        
        report.append("---\n\n")
        report.append("*æœ¬æŠ¥å‘Šç”± Qilin Stack åŸºå‡†æ¨¡å‹è®­ç»ƒç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*\n")
        
        # å†™å…¥æ–‡ä»¶
        report_text = ''.join(report)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        print(f"   âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")
        
        # åŒæ—¶ä¿å­˜ç‰¹å¾é‡è¦æ€§CSV
        importance_path = output_path.parent.parent / 'analysis' / 'baseline_feature_importance.csv'
        importance_path.parent.mkdir(parents=True, exist_ok=True)
        feature_importance.to_csv(importance_path, index=False, encoding='utf-8-sig')
        print(f"   âœ… ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜: {importance_path}")
        
        return report_text
    
    def run_full_pipeline(self, data_path: str = None) -> Dict:
        """è¿è¡Œå®Œæ•´è®­ç»ƒæµç¨‹"""
        print("\n" + "="*70)
        print("ğŸš€ å¼€å§‹åŸºå‡†æ¨¡å‹è®­ç»ƒæµç¨‹")
        print("="*70)
        
        # 1. åŠ è½½æ•°æ®
        X, y = self.load_data(data_path)
        
        # 2. åˆ’åˆ†æ•°æ®é›†
        X_train, X_valid, X_test, y_train, y_valid, y_test = self.split_data_by_time(X, y)
        
        # 3. è®­ç»ƒæ¨¡å‹
        self.train(X_train, y_train, X_valid, y_valid)
        
        # 4. è¯„ä¼°æ¨¡å‹
        metrics = self.evaluate(X_test, y_test, X_train, y_train, X_valid, y_valid)
        
        # 5. ç‰¹å¾é‡è¦æ€§
        feature_importance = self.analyze_feature_importance()
        
        # 6. SHAPè§£é‡Š
        self.generate_shap_explanation(X_test)
        
        # 7. ä¿å­˜æ¨¡å‹
        self.save_model()
        
        # 8. ç”ŸæˆæŠ¥å‘Š
        self.generate_report(feature_importance)
        
        print("\n" + "="*70)
        print("âœ… åŸºå‡†æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print(f"   æµ‹è¯•é›†AUC: {metrics.get('test_auc', 0):.4f}")
        print(f"   éªŒæ”¶æ ‡å‡†: {'âœ… é€šè¿‡' if metrics.get('test_auc', 0) > 0.68 else 'âŒ æœªé€šè¿‡'}")
        print("="*70)
        
        return {
            'model': self.model,
            'metrics': metrics,
            'feature_importance': feature_importance
        }


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='åŸºå‡†æ¨¡å‹è®­ç»ƒå·¥å…·')
    parser.add_argument('--data', type=str, default=None,
                      help='è®­ç»ƒæ•°æ®è·¯å¾„ï¼ˆCSVï¼‰')
    parser.add_argument('--params', type=str, default='conservative',
                      choices=['conservative', 'moderate', 'aggressive'],
                      help='è¶…å‚æ•°é…ç½®')
    parser.add_argument('--output', type=str, default=None,
                      help='æ¨¡å‹è¾“å‡ºè·¯å¾„')
    
    args = parser.parse_args()
    
    # æ ¹æ®å‚æ•°é€‰æ‹©é…ç½®
    if args.params == 'conservative':
        params = BaselineModelTrainer.DEFAULT_PARAMS.copy()
    elif args.params == 'moderate':
        params = BaselineModelTrainer.DEFAULT_PARAMS.copy()
        params.update({'max_depth': 6, 'num_leaves': 63, 'learning_rate': 0.08})
    else:  # aggressive
        params = BaselineModelTrainer.DEFAULT_PARAMS.copy()
        params.update({'max_depth': 7, 'num_leaves': 127, 'learning_rate': 0.10})
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = BaselineModelTrainer(model_params=params)
    
    # è¿è¡Œå®Œæ•´æµç¨‹
    results = trainer.run_full_pipeline(data_path=args.data)
    
    # å¦‚æœæŒ‡å®šäº†è¾“å‡ºè·¯å¾„ï¼Œä¿å­˜åˆ°æŒ‡å®šä½ç½®
    if args.output:
        trainer.save_model(output_path=args.output)
    
    return results


if __name__ == '__main__':
    main()
