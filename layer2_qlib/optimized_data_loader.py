"""
ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨ (Optimized Data Loader)
ä½¿ç”¨Redisç¼“å­˜æå‡æ•°æ®åŠ è½½æ€§èƒ½
"""

import os
import hashlib
import pickle
import logging
from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta
from functools import wraps

import pandas as pd
import numpy as np

try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    print("âš ï¸  Redisæœªå®‰è£…ï¼Œç¼“å­˜åŠŸèƒ½å°†è¢«ç¦ç”¨ã€‚å®‰è£…æ–¹æ³•: pip install redis")

logger = logging.getLogger(__name__)


class CacheConfig:
    """ç¼“å­˜é…ç½®"""
    def __init__(self):
        self.enabled = os.getenv('REDIS_ENABLED', 'True').lower() == 'true' and REDIS_AVAILABLE
        self.host = os.getenv('REDIS_HOST', 'localhost')
        self.port = int(os.getenv('REDIS_PORT', '6379'))
        self.db = int(os.getenv('REDIS_DB', '0'))
        self.password = os.getenv('REDIS_PASSWORD')
        self.default_ttl = int(os.getenv('CACHE_TTL', '3600'))  # é»˜è®¤1å°æ—¶
        self.prefix = 'qilin:data:'


class OptimizedDataLoader:
    """
    ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨
    
    ç‰¹æ€§:
    1. Redisç¼“å­˜æœºåˆ¶
    2. æ‰¹é‡åŠ è½½ä¼˜åŒ–
    3. å†…å­˜ç¼“å­˜ï¼ˆLRUï¼‰
    4. æ‡’åŠ è½½
    5. å¼‚æ­¥åŠ è½½æ”¯æŒ
    """
    
    def __init__(self, cache_config: Optional[CacheConfig] = None):
        self.cache_config = cache_config or CacheConfig()
        self.redis_client = None
        self._memory_cache = {}  # ç®€å•çš„å†…å­˜ç¼“å­˜
        self._memory_cache_size = 100
        
        if self.cache_config.enabled:
            self._init_redis()
    
    def _init_redis(self):
        """åˆå§‹åŒ–Redisè¿æ¥"""
        try:
            self.redis_client = redis.Redis(
                host=self.cache_config.host,
                port=self.cache_config.port,
                db=self.cache_config.db,
                password=self.cache_config.password,
                decode_responses=False,  # æˆ‘ä»¬ä¼šå­˜å‚¨äºŒè¿›åˆ¶æ•°æ®
                socket_connect_timeout=5,
                socket_timeout=5
            )
            # æµ‹è¯•è¿æ¥
            self.redis_client.ping()
            logger.info(f"âœ… Redisç¼“å­˜å·²è¿æ¥: {self.cache_config.host}:{self.cache_config.port}")
        except redis.ConnectionError as e:
            logger.warning(f"âš ï¸  Redisè¿æ¥å¤±è´¥ï¼Œç¼“å­˜åŠŸèƒ½å°†è¢«ç¦ç”¨: {e}")
            self.redis_client = None
        except Exception as e:
            logger.error(f"Redisåˆå§‹åŒ–é”™è¯¯: {e}")
            self.redis_client = None
    
    def _generate_cache_key(self, *args, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # åˆ›å»ºå”¯ä¸€çš„é”®
        key_parts = [str(arg) for arg in args]
        key_parts.extend([f"{k}={v}" for k, v in sorted(kwargs.items())])
        key_string = "|".join(key_parts)
        
        # ä½¿ç”¨MD5ç”Ÿæˆå“ˆå¸Œ
        key_hash = hashlib.md5(key_string.encode()).hexdigest()
        return f"{self.cache_config.prefix}{key_hash}"
    
    def _get_from_cache(self, cache_key: str) -> Optional[Any]:
        """ä»ç¼“å­˜è·å–æ•°æ®"""
        # å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
        if cache_key in self._memory_cache:
            logger.debug(f"ğŸ’¾ ä»å†…å­˜ç¼“å­˜å‘½ä¸­: {cache_key[:20]}...")
            return self._memory_cache[cache_key]
        
        # å†æ£€æŸ¥Redisç¼“å­˜
        if self.redis_client:
            try:
                cached_data = self.redis_client.get(cache_key)
                if cached_data:
                    data = pickle.loads(cached_data)
                    # æ›´æ–°åˆ°å†…å­˜ç¼“å­˜
                    self._update_memory_cache(cache_key, data)
                    logger.debug(f"ğŸ’¾ ä»Redisç¼“å­˜å‘½ä¸­: {cache_key[:20]}...")
                    return data
            except Exception as e:
                logger.error(f"ä»Redisè¯»å–ç¼“å­˜å¤±è´¥: {e}")
        
        return None
    
    def _set_to_cache(self, cache_key: str, data: Any, ttl: Optional[int] = None):
        """å­˜å‚¨æ•°æ®åˆ°ç¼“å­˜"""
        ttl = ttl or self.cache_config.default_ttl
        
        # å­˜å‚¨åˆ°å†…å­˜ç¼“å­˜
        self._update_memory_cache(cache_key, data)
        
        # å­˜å‚¨åˆ°Redis
        if self.redis_client:
            try:
                serialized_data = pickle.dumps(data)
                self.redis_client.setex(cache_key, ttl, serialized_data)
                logger.debug(f"ğŸ’¾ æ•°æ®å·²ç¼“å­˜ (TTL={ttl}s): {cache_key[:20]}...")
            except Exception as e:
                logger.error(f"å­˜å‚¨åˆ°Rediså¤±è´¥: {e}")
    
    def _update_memory_cache(self, key: str, data: Any):
        """æ›´æ–°å†…å­˜ç¼“å­˜ï¼ˆLRUï¼‰"""
        if len(self._memory_cache) >= self._memory_cache_size:
            # ç§»é™¤æœ€æ—§çš„é¡¹
            oldest_key = next(iter(self._memory_cache))
            del self._memory_cache[oldest_key]
        
        self._memory_cache[key] = data
    
    def clear_cache(self, pattern: Optional[str] = None):
        """
        æ¸…é™¤ç¼“å­˜
        
        Args:
            pattern: åŒ¹é…æ¨¡å¼ï¼Œå¦‚ "qilin:data:*"
        """
        # æ¸…é™¤å†…å­˜ç¼“å­˜
        self._memory_cache.clear()
        
        # æ¸…é™¤Redisç¼“å­˜
        if self.redis_client:
            try:
                if pattern:
                    keys = self.redis_client.keys(pattern)
                    if keys:
                        self.redis_client.delete(*keys)
                        logger.info(f"âœ… å·²æ¸…é™¤ {len(keys)} ä¸ªç¼“å­˜é¡¹")
                else:
                    # æ¸…é™¤æ‰€æœ‰qilinç›¸å…³çš„ç¼“å­˜
                    keys = self.redis_client.keys(f"{self.cache_config.prefix}*")
                    if keys:
                        self.redis_client.delete(*keys)
                        logger.info(f"âœ… å·²æ¸…é™¤æ‰€æœ‰ç¼“å­˜ ({len(keys)} é¡¹)")
            except Exception as e:
                logger.error(f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {e}")
    
    def get_stock_data(
        self,
        symbols: List[str],
        start_date: str,
        end_date: str,
        fields: Optional[List[str]] = None,
        use_cache: bool = True
    ) -> pd.DataFrame:
        """
        è·å–è‚¡ç¥¨æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰
        
        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ YYYY-MM-DD
            end_date: ç»“æŸæ—¥æœŸ YYYY-MM-DD
            fields: å­—æ®µåˆ—è¡¨
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            æ•°æ®DataFrame
        """
        cache_key = self._generate_cache_key(
            'stock_data',
            symbols=','.join(sorted(symbols)),
            start_date=start_date,
            end_date=end_date,
            fields=','.join(sorted(fields)) if fields else 'all'
        )
        
        # å°è¯•ä»ç¼“å­˜è·å–
        if use_cache:
            cached_data = self._get_from_cache(cache_key)
            if cached_data is not None:
                return cached_data
        
        # ä»æ•°æ®æºåŠ è½½
        logger.info(f"ğŸ“¥ æ­£åœ¨åŠ è½½è‚¡ç¥¨æ•°æ®: {len(symbols)} åªè‚¡ç¥¨")
        data = self._load_from_source(symbols, start_date, end_date, fields)
        
        # å­˜å‚¨åˆ°ç¼“å­˜
        if use_cache:
            self._set_to_cache(cache_key, data)
        
        return data
    
    def _load_from_source(
        self,
        symbols: List[str],
        start_date: str,
        end_date: str,
        fields: Optional[List[str]] = None
    ) -> pd.DataFrame:
        """ä»æ•°æ®æºåŠ è½½æ•°æ®"""
        try:
            # å°è¯•ä½¿ç”¨Qlib
            import qlib
            from qlib.data import D
            
            fields = fields or ['$open', '$high', '$low', '$close', '$volume', '$factor']
            
            # æ‰¹é‡åŠ è½½æ•°æ®
            data_dict = {}
            for symbol in symbols:
                try:
                    df = D.features(
                        [symbol],
                        fields,
                        start_date,
                        end_date
                    )
                    if not df.empty:
                        data_dict[symbol] = df
                except Exception as e:
                    logger.warning(f"åŠ è½½ {symbol} å¤±è´¥: {e}")
            
            if not data_dict:
                logger.warning("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®")
                return pd.DataFrame()
            
            # åˆå¹¶æ•°æ®
            result = pd.concat(data_dict, names=['symbol', 'datetime'])
            return result
            
        except ImportError:
            logger.warning("Qlibæœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            return self._generate_mock_data(symbols, start_date, end_date)
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _generate_mock_data(
        self,
        symbols: List[str],
        start_date: str,
        end_date: str
    ) -> pd.DataFrame:
        """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®"""
        dates = pd.date_range(start=start_date, end=end_date, freq='B')
        
        data_list = []
        for symbol in symbols:
            # ç”Ÿæˆéšæœºä»·æ ¼æ•°æ®
            base_price = 10 + np.random.randn() * 2
            prices = base_price + np.cumsum(np.random.randn(len(dates)) * 0.1)
            
            df = pd.DataFrame({
                'symbol': symbol,
                'datetime': dates,
                'open': prices * (1 + np.random.randn(len(dates)) * 0.01),
                'high': prices * (1 + abs(np.random.randn(len(dates))) * 0.02),
                'low': prices * (1 - abs(np.random.randn(len(dates))) * 0.02),
                'close': prices,
                'volume': np.random.randint(1000, 10000, len(dates)),
                'factor': 1.0
            })
            data_list.append(df)
        
        result = pd.concat(data_list, ignore_index=False)
        result.set_index(['symbol', 'datetime'], inplace=True)
        return result
    
    def batch_load(
        self,
        tasks: List[Dict[str, Any]],
        use_cache: bool = True,
        parallel: bool = True
    ) -> Dict[str, pd.DataFrame]:
        """
        æ‰¹é‡åŠ è½½æ•°æ®
        
        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«symbols, start_date, end_dateç­‰
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            parallel: æ˜¯å¦å¹¶è¡ŒåŠ è½½
            
        Returns:
            ç»“æœå­—å…¸ {task_id: DataFrame}
        """
        results = {}
        
        if parallel:
            # å¹¶è¡ŒåŠ è½½
            from concurrent.futures import ThreadPoolExecutor, as_completed
            
            with ThreadPoolExecutor(max_workers=4) as executor:
                future_to_task = {
                    executor.submit(
                        self.get_stock_data,
                        task['symbols'],
                        task['start_date'],
                        task['end_date'],
                        task.get('fields'),
                        use_cache
                    ): task.get('task_id', f"task_{i}")
                    for i, task in enumerate(tasks)
                }
                
                for future in as_completed(future_to_task):
                    task_id = future_to_task[future]
                    try:
                        results[task_id] = future.result()
                    except Exception as e:
                        logger.error(f"ä»»åŠ¡ {task_id} å¤±è´¥: {e}")
                        results[task_id] = pd.DataFrame()
        else:
            # é¡ºåºåŠ è½½
            for i, task in enumerate(tasks):
                task_id = task.get('task_id', f"task_{i}")
                try:
                    results[task_id] = self.get_stock_data(
                        task['symbols'],
                        task['start_date'],
                        task['end_date'],
                        task.get('fields'),
                        use_cache
                    )
                except Exception as e:
                    logger.error(f"ä»»åŠ¡ {task_id} å¤±è´¥: {e}")
                    results[task_id] = pd.DataFrame()
        
        return results
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'memory_cache_size': len(self._memory_cache),
            'memory_cache_max': self._memory_cache_size,
            'redis_enabled': self.redis_client is not None
        }
        
        if self.redis_client:
            try:
                info = self.redis_client.info('stats')
                keys_count = len(self.redis_client.keys(f"{self.cache_config.prefix}*"))
                stats['redis_keys'] = keys_count
                stats['redis_hits'] = info.get('keyspace_hits', 0)
                stats['redis_misses'] = info.get('keyspace_misses', 0)
                if stats['redis_hits'] + stats['redis_misses'] > 0:
                    stats['redis_hit_rate'] = stats['redis_hits'] / (stats['redis_hits'] + stats['redis_misses'])
            except Exception as e:
                logger.error(f"è·å–Redisç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        
        return stats


def cached_data_loader(ttl: Optional[int] = None):
    """
    æ•°æ®åŠ è½½è£…é¥°å™¨ï¼ˆå¸¦ç¼“å­˜ï¼‰
    
    Args:
        ttl: ç¼“å­˜æ—¶é—´ï¼ˆç§’ï¼‰
    """
    def decorator(func):
        loader = OptimizedDataLoader()
        
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = loader._generate_cache_key(
                func.__name__,
                *args,
                **kwargs
            )
            
            # å°è¯•ä»ç¼“å­˜è·å–
            cached_data = loader._get_from_cache(cache_key)
            if cached_data is not None:
                return cached_data
            
            # è°ƒç”¨åŸå‡½æ•°
            result = func(*args, **kwargs)
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            loader._set_to_cache(cache_key, result, ttl)
            
            return result
        
        return wrapper
    return decorator


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    loader = OptimizedDataLoader()
    
    print("\nğŸ“Š æµ‹è¯•ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨\n")
    
    # æµ‹è¯•å•æ¬¡åŠ è½½
    symbols = ['000001.SZ', '600000.SH', '600519.SH']
    start_date = '2024-01-01'
    end_date = '2024-03-01'
    
    print(f"ç¬¬ä¸€æ¬¡åŠ è½½ ({len(symbols)} åªè‚¡ç¥¨)...")
    import time
    start = time.time()
    data1 = loader.get_stock_data(symbols, start_date, end_date)
    elapsed1 = time.time() - start
    print(f"âœ… å®Œæˆï¼Œè€—æ—¶: {elapsed1:.2f}ç§’")
    print(f"   æ•°æ®shape: {data1.shape}")
    
    print(f"\nç¬¬äºŒæ¬¡åŠ è½½ï¼ˆåº”è¯¥å‘½ä¸­ç¼“å­˜ï¼‰...")
    start = time.time()
    data2 = loader.get_stock_data(symbols, start_date, end_date)
    elapsed2 = time.time() - start
    print(f"âœ… å®Œæˆï¼Œè€—æ—¶: {elapsed2:.2f}ç§’")
    print(f"   åŠ é€Ÿæ¯”: {elapsed1/elapsed2:.1f}x")
    
    # ç¼“å­˜ç»Ÿè®¡
    print("\nğŸ“ˆ ç¼“å­˜ç»Ÿè®¡:")
    stats = loader.get_cache_stats()
    for key, value in stats.items():
        print(f"   {key}: {value}")
    
    # æ¸…é™¤ç¼“å­˜
    print("\nğŸ§¹ æ¸…é™¤ç¼“å­˜...")
    loader.clear_cache()
    print("âœ… å®Œæˆ")
