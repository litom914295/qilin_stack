#!/usr/bin/env python
"""
å›°éš¾æ¡ˆä¾‹æŒ–æ˜è®­ç»ƒå™¨
è®©AIåœ¨é”™è¯¯ä¸­æˆé•¿ï¼ŒæŒç»­è¿›åŒ–
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
import json
from pathlib import Path


class HardCaseMining:
    """å›°éš¾æ¡ˆä¾‹æŒ–æ˜ - å¾ªç¯è¿›åŒ–è®­ç»ƒ"""
    
    def __init__(self, base_model=None):
        self.model = base_model
        self.hard_cases = []
        self.training_history = []
        self.iteration_count = 0
    
    def iterative_training(
        self, 
        historical_data: pd.DataFrame, 
        max_iterations: int = 10,
        convergence_threshold: float = 0.85,
        min_hard_cases: int = 50
    ) -> Dict:
        """
        è¿­ä»£è®­ç»ƒæµç¨‹
        
        Args:
            historical_data: å†å²æ•°æ®
            max_iterations: æœ€å¤§è¿­ä»£è½®æ•°
            convergence_threshold: æ”¶æ•›å‡†ç¡®ç‡é˜ˆå€¼
            min_hard_cases: æœ€å°‘å›°éš¾æ¡ˆä¾‹æ•°ï¼ˆä½äºæ­¤æ•°å³æ”¶æ•›ï¼‰
        
        Returns:
            è®­ç»ƒç»“æœç»Ÿè®¡
        """
        
        print(f"å¼€å§‹å›°éš¾æ¡ˆä¾‹æŒ–æ˜è®­ç»ƒï¼Œæœ€å¤š{max_iterations}è½®")
        
        results = {
            'iterations': [],
            'final_accuracy': 0,
            'total_hard_cases': 0,
            'converged': False
        }
        
        for iteration in range(max_iterations):
            print(f"\n{'='*50}")
            print(f"ç¬¬ {iteration + 1}/{max_iterations} è½®è®­ç»ƒ")
            print(f"{'='*50}")
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            if iteration == 0:
                # ç¬¬1è½®ï¼šå…¨é‡è®­ç»ƒ
                train_data = historical_data.copy()
                print("ä½¿ç”¨å…¨é‡æ•°æ®è¿›è¡Œåˆå§‹è®­ç»ƒ")
            else:
                # åç»­è½®æ¬¡ï¼šé‡ç‚¹è®­ç»ƒå›°éš¾æ¡ˆä¾‹
                train_data = self._prepare_hard_case_training_set(
                    historical_data, 
                    iteration
                )
                print(f"ä½¿ç”¨å›°éš¾æ¡ˆä¾‹å¢å¼ºè®­ç»ƒé›†ï¼ˆ{len(train_data)}æ ·æœ¬ï¼‰")
            
            # æ¨¡æ‹Ÿè®­ç»ƒï¼ˆå®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºçœŸå®æ¨¡å‹è®­ç»ƒï¼‰
            training_result = self._train_one_iteration(train_data)
            
            # è¯„ä¼°å¹¶æ‰¾å‡ºæ–°çš„å›°éš¾æ¡ˆä¾‹
            predictions = self._predict_all(historical_data)
            new_hard_cases = self._identify_hard_cases(
                historical_data, 
                predictions
            )
            
            print(f"å‘ç° {len(new_hard_cases)} ä¸ªæ–°å›°éš¾æ¡ˆä¾‹")
            
            # ç´¯ç§¯å›°éš¾æ¡ˆä¾‹
            self.hard_cases.extend(new_hard_cases)
            
            # è®¡ç®—å‡†ç¡®ç‡
            accuracy = self._calculate_accuracy(predictions, historical_data)
            
            # è®°å½•æœ¬è½®ç»“æœ
            iteration_result = {
                'iteration': iteration + 1,
                'accuracy': accuracy,
                'new_hard_cases': len(new_hard_cases),
                'total_hard_cases': len(self.hard_cases),
                'training_time': training_result.get('time', 0)
            }
            
            results['iterations'].append(iteration_result)
            
            print(f"âœ… ç¬¬{iteration + 1}è½®å®Œæˆ")
            print(f"   å‡†ç¡®ç‡: {accuracy:.2%}")
            print(f"   æ–°å›°éš¾æ¡ˆä¾‹: {len(new_hard_cases)}")
            print(f"   ç´¯è®¡å›°éš¾æ¡ˆä¾‹: {len(self.hard_cases)}")
            
            # æ”¶æ•›åˆ¤æ–­
            if accuracy >= convergence_threshold and len(new_hard_cases) < min_hard_cases:
                print(f"\nğŸ‰ è®­ç»ƒæ”¶æ•›ï¼")
                print(f"   æœ€ç»ˆå‡†ç¡®ç‡: {accuracy:.2%}")
                print(f"   å›°éš¾æ¡ˆä¾‹æ•°: {len(self.hard_cases)}")
                results['converged'] = True
                break
        
        # æ±‡æ€»ç»“æœ
        results['final_accuracy'] = accuracy
        results['total_hard_cases'] = len(self.hard_cases)
        results['iteration_count'] = iteration + 1
        
        self.iteration_count = iteration + 1
        self.training_history = results
        
        return results
    
    def _identify_hard_cases(
        self, 
        data: pd.DataFrame, 
        predictions: np.ndarray
    ) -> List[Dict]:
        """è¯†åˆ«å›°éš¾æ¡ˆä¾‹"""
        
        hard_cases = []
        
        for i in range(len(data)):
            case_info = {'index': i}
            
            # è·å–çœŸå®æ ‡ç­¾å’Œé¢„æµ‹
            true_label = data.iloc[i].get('main_label', 0)
            pred_label = predictions[i] if isinstance(predictions[i], (int, float)) else predictions[i].get('label', 0)
            pred_confidence = predictions[i].get('confidence', 0.5) if isinstance(predictions[i], dict) else 0.5
            
            # ç±»å‹1: é¢„æµ‹é”™è¯¯çš„æ¡ˆä¾‹
            if true_label != pred_label:
                case_info.update({
                    'type': 'wrong_prediction',
                    'true_label': true_label,
                    'pred_label': pred_label,
                    'confidence': pred_confidence,
                    'weight': 3.0  # é¢„æµ‹é”™è¯¯ - é«˜æƒé‡
                })
                hard_cases.append(case_info)
                continue
            
            # ç±»å‹2: ä½ç½®ä¿¡åº¦çš„æ­£ç¡®æ¡ˆä¾‹ï¼ˆè¾¹ç•Œæ¡ˆä¾‹ï¼‰
            if pred_confidence < 0.6:
                case_info.update({
                    'type': 'low_confidence',
                    'true_label': true_label,
                    'confidence': pred_confidence,
                    'weight': 2.0  # ä½ç½®ä¿¡åº¦ - ä¸­æƒé‡
                })
                hard_cases.append(case_info)
                continue
            
            # ç±»å‹3: åç›´è§‰æ¡ˆä¾‹
            if self._is_counter_intuitive(data.iloc[i]):
                case_info.update({
                    'type': 'counter_intuitive',
                    'reason': self._get_counter_intuitive_reason(data.iloc[i]),
                    'weight': 3.0  # åç›´è§‰ - é«˜æƒé‡
                })
                hard_cases.append(case_info)
        
        return hard_cases
    
    def _is_counter_intuitive(self, case: pd.Series) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºåç›´è§‰æ¡ˆä¾‹"""
        
        seal_strength = case.get('seal_strength', case.get('å°æ¿å¼ºåº¦', 0))
        return_1d = case.get('return_1d', 0)
        
        # åç›´è§‰æ¡ˆä¾‹ç¤ºä¾‹ï¼š
        
        # 1. å¼ºå°æ¿ä½†æ¬¡æ—¥ä¸‹è·Œ
        if seal_strength > 90 and return_1d < 0:
            return True
        
        # 2. å¼±å°æ¿ä½†æ¬¡æ—¥æ¶¨åœ
        if seal_strength < 60 and return_1d >= 0.095:
            return True
        
        # 3. é«˜ä½æ¶¨åœä½†æŒç»­ä¸Šæ¶¨
        price_position = case.get('price_position', 0.5)
        return_5d = case.get('return_5d', 0)
        if price_position > 0.9 and return_5d > 0.2:
            return True
        
        # 4. æƒ…ç»ªä½è¿·ä½†ä¸ªè‚¡èµ°å¼º
        market_sentiment = case.get('market_sentiment', 'neutral')
        if market_sentiment in ['weak', 'poor'] and return_1d > 0.05:
            return True
        
        return False
    
    def _get_counter_intuitive_reason(self, case: pd.Series) -> str:
        """è·å–åç›´è§‰åŸå› """
        
        seal_strength = case.get('seal_strength', case.get('å°æ¿å¼ºåº¦', 0))
        return_1d = case.get('return_1d', 0)
        
        if seal_strength > 90 and return_1d < 0:
            return "å¼ºå°æ¿ä½†æ¬¡æ—¥ä¸‹è·Œï¼ˆè¯±å¤šé™·é˜±ï¼‰"
        
        if seal_strength < 60 and return_1d >= 0.095:
            return "å¼±å°æ¿ä½†æ¬¡æ—¥æ¶¨åœï¼ˆéšè—æœºä¼šï¼‰"
        
        return "å…¶ä»–åç›´è§‰æƒ…å†µ"
    
    def _prepare_hard_case_training_set(
        self, 
        historical_data: pd.DataFrame, 
        iteration: int
    ) -> pd.DataFrame:
        """å‡†å¤‡å›°éš¾æ¡ˆä¾‹è®­ç»ƒé›†"""
        
        # æå–å›°éš¾æ¡ˆä¾‹
        hard_case_indices = list(set([case['index'] for case in self.hard_cases]))
        hard_data = historical_data.iloc[hard_case_indices].copy()
        
        # é‡‡æ ·æ­£å¸¸æ¡ˆä¾‹ï¼ˆä¿æŒå¹³è¡¡ï¼‰
        normal_indices = [i for i in range(len(historical_data)) 
                         if i not in hard_case_indices]
        
        if len(normal_indices) > 0:
            sample_size = min(len(hard_data) * 2, len(normal_indices))
            normal_sample = np.random.choice(
                normal_indices, 
                size=sample_size,
                replace=False
            )
            normal_data = historical_data.iloc[normal_sample].copy()
        else:
            normal_data = pd.DataFrame()
        
        # åˆå¹¶æ•°æ®
        if len(normal_data) > 0:
            train_data = pd.concat([hard_data, normal_data], ignore_index=True)
        else:
            train_data = hard_data.copy()
        
        # è®¾ç½®æƒé‡
        train_data['sample_weight'] = 1.0
        
        # å›°éš¾æ¡ˆä¾‹æƒé‡æ˜ å°„
        for case in self.hard_cases:
            idx = case['index']
            if idx in hard_case_indices:
                # åœ¨train_dataä¸­æ‰¾åˆ°å¯¹åº”è¡Œ
                mask = train_data.index.isin([idx])
                if mask.any():
                    train_data.loc[mask, 'sample_weight'] = case.get('weight', 3.0)
        
        return train_data
    
    def _train_one_iteration(self, train_data: pd.DataFrame) -> Dict:
        """è®­ç»ƒä¸€è½®ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        
        # å®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºçœŸå®æ¨¡å‹è®­ç»ƒ
        import time
        start_time = time.time()
        
        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        time.sleep(0.1)
        
        training_time = time.time() - start_time
        
        return {
            'time': training_time,
            'samples': len(train_data)
        }
    
    def _predict_all(self, data: pd.DataFrame) -> np.ndarray:
        """é¢„æµ‹æ‰€æœ‰æ•°æ®ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        
        # å®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºçœŸå®æ¨¡å‹é¢„æµ‹
        predictions = []
        
        for i in range(len(data)):
            # æ¨¡æ‹Ÿé¢„æµ‹ç»“æœ
            pred = {
                'label': np.random.choice([0, 1, 2, 3]),
                'confidence': np.random.uniform(0.3, 0.95)
            }
            predictions.append(pred)
        
        return np.array(predictions)
    
    def _calculate_accuracy(
        self, 
        predictions: np.ndarray, 
        data: pd.DataFrame
    ) -> float:
        """è®¡ç®—å‡†ç¡®ç‡"""
        
        correct = 0
        total = len(predictions)
        
        for i in range(total):
            true_label = data.iloc[i].get('main_label', 0)
            pred_label = predictions[i]['label'] if isinstance(predictions[i], dict) else predictions[i]
            
            if true_label == pred_label:
                correct += 1
        
        return correct / total if total > 0 else 0
    
    def get_hard_cases_summary(self) -> pd.DataFrame:
        """è·å–å›°éš¾æ¡ˆä¾‹æ‘˜è¦"""
        
        if not self.hard_cases:
            return pd.DataFrame()
        
        # ç»Ÿè®¡å„ç±»å‹å›°éš¾æ¡ˆä¾‹
        type_counts = {}
        for case in self.hard_cases:
            case_type = case.get('type', 'unknown')
            type_counts[case_type] = type_counts.get(case_type, 0) + 1
        
        summary = pd.DataFrame([
            {'case_type': k, 'count': v, 'percentage': v / len(self.hard_cases)}
            for k, v in type_counts.items()
        ])
        
        return summary
    
    def save(self, save_dir: Path):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        
        save_dir = Path(save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜å›°éš¾æ¡ˆä¾‹
        with open(save_dir / 'hard_cases.json', 'w', encoding='utf-8') as f:
            json.dump(self.hard_cases[:1000], f, ensure_ascii=False, indent=2)  # åªä¿å­˜å‰1000ä¸ª
        
        # ä¿å­˜è®­ç»ƒå†å²
        with open(save_dir / 'training_history.json', 'w', encoding='utf-8') as f:
            json.dump(self.training_history, f, ensure_ascii=False, indent=2)
    
    def load(self, save_dir: Path):
        """åŠ è½½è®­ç»ƒç»“æœ"""
        
        save_dir = Path(save_dir)
        
        if (save_dir / 'hard_cases.json').exists():
            with open(save_dir / 'hard_cases.json', 'r', encoding='utf-8') as f:
                self.hard_cases = json.load(f)
        
        if (save_dir / 'training_history.json').exists():
            with open(save_dir / 'training_history.json', 'r', encoding='utf-8') as f:
                self.training_history = json.load(f)


def demo():
    """æ¼”ç¤ºç”¨æ³•"""
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    data = pd.DataFrame({
        'code': ['000001'] * 100,
        'main_label': np.random.choice([0, 1, 2, 3], 100),
        'seal_strength': np.random.uniform(50, 95, 100),
        'return_1d': np.random.normal(0.03, 0.05, 100),
        'return_5d': np.random.normal(0.08, 0.12, 100)
    })
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = HardCaseMining()
    
    # è¿­ä»£è®­ç»ƒ
    results = trainer.iterative_training(
        data, 
        max_iterations=5,
        convergence_threshold=0.80
    )
    
    print("\n" + "="*50)
    print("è®­ç»ƒå®Œæˆï¼")
    print("="*50)
    print(f"æœ€ç»ˆå‡†ç¡®ç‡: {results['final_accuracy']:.2%}")
    print(f"æ€»å›°éš¾æ¡ˆä¾‹: {results['total_hard_cases']}")
    print(f"è¿­ä»£è½®æ•°: {results['iteration_count']}")
    print(f"æ˜¯å¦æ”¶æ•›: {results['converged']}")


if __name__ == '__main__':
    demo()
