"""
æ•°æ®è´¨é‡ç›‘æ§å™¨ (Data Quality Monitor)
å¤šç»´åº¦ç›‘æ§æ•°æ®è´¨é‡å¹¶å®æ—¶å‘Šè­¦

æ ¸å¿ƒç›‘æ§ç»´åº¦ï¼š
1. å®Œæ•´æ€§ï¼šç¼ºå¤±å€¼æ£€æµ‹
2. å‡†ç¡®æ€§ï¼šå¼‚å¸¸å€¼æ£€æµ‹
3. ä¸€è‡´æ€§ï¼šé€»è¾‘ä¸€è‡´æ€§æ ¡éªŒ
4. æ—¶æ•ˆæ€§ï¼šæ•°æ®å»¶è¿Ÿç›‘æ§
5. åˆè§„æ€§ï¼šæ•°æ®è§„èŒƒæ ¡éªŒ
"""

from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from enum import Enum
import numpy as np
import pandas as pd


class QualityDimension(Enum):
    """æ•°æ®è´¨é‡ç»´åº¦"""
    COMPLETENESS = "å®Œæ•´æ€§"
    ACCURACY = "å‡†ç¡®æ€§"
    CONSISTENCY = "ä¸€è‡´æ€§"
    TIMELINESS = "æ—¶æ•ˆæ€§"
    VALIDITY = "åˆè§„æ€§"


class SeverityLevel(Enum):
    """ä¸¥é‡ç­‰çº§"""
    INFO = "ä¿¡æ¯"
    WARNING = "è­¦å‘Š"
    ERROR = "é”™è¯¯"
    CRITICAL = "ä¸¥é‡"


@dataclass
class QualityIssue:
    """è´¨é‡é—®é¢˜"""
    dimension: QualityDimension
    severity: SeverityLevel
    field: str
    description: str
    timestamp: datetime
    value: Any = None
    expected: Any = None
    suggestion: str = ""


@dataclass
class QualityMetrics:
    """è´¨é‡æŒ‡æ ‡"""
    # å®Œæ•´æ€§æŒ‡æ ‡
    completeness_rate: float          # å®Œæ•´ç‡
    missing_fields: List[str]         # ç¼ºå¤±å­—æ®µ
    
    # å‡†ç¡®æ€§æŒ‡æ ‡
    accuracy_rate: float              # å‡†ç¡®ç‡
    outlier_count: int                # å¼‚å¸¸å€¼æ•°é‡
    outlier_fields: List[str]         # å¼‚å¸¸å­—æ®µ
    
    # ä¸€è‡´æ€§æŒ‡æ ‡
    consistency_rate: float           # ä¸€è‡´æ€§ç‡
    inconsistency_count: int          # ä¸ä¸€è‡´æ•°é‡
    inconsistent_fields: List[str]    # ä¸ä¸€è‡´å­—æ®µ
    
    # æ—¶æ•ˆæ€§æŒ‡æ ‡
    avg_latency_ms: float             # å¹³å‡å»¶è¿Ÿ
    max_latency_ms: float             # æœ€å¤§å»¶è¿Ÿ
    stale_data_count: int             # è¿‡æœŸæ•°æ®æ•°
    
    # åˆè§„æ€§æŒ‡æ ‡
    validity_rate: float              # åˆè§„ç‡
    invalid_count: int                # ä¸åˆè§„æ•°é‡
    invalid_fields: List[str]         # ä¸åˆè§„å­—æ®µ
    
    # æ±‡æ€»
    overall_score: float              # æ€»ä½“å¾—åˆ†(0-100)
    total_issues: int                 # æ€»é—®é¢˜æ•°
    issues: List[QualityIssue] = field(default_factory=list)


class DataQualityRule:
    """æ•°æ®è´¨é‡è§„åˆ™æŠ½è±¡åŸºç±»"""
    
    def __init__(self, dimension: QualityDimension, severity: SeverityLevel):
        self.dimension = dimension
        self.severity = severity
    
    def check(self, data: pd.DataFrame) -> List[QualityIssue]:
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        raise NotImplementedError


class CompletenessRule(DataQualityRule):
    """å®Œæ•´æ€§è§„åˆ™"""
    
    def __init__(self, required_fields: List[str], 
                 missing_threshold: float = 0.01,
                 severity: SeverityLevel = SeverityLevel.WARNING):
        super().__init__(QualityDimension.COMPLETENESS, severity)
        self.required_fields = required_fields
        self.missing_threshold = missing_threshold  # å…è®¸çš„ç¼ºå¤±ç‡
    
    def check(self, data: pd.DataFrame) -> List[QualityIssue]:
        """æ£€æŸ¥å®Œæ•´æ€§"""
        issues = []
        
        for field in self.required_fields:
            if field not in data.columns:
                issues.append(QualityIssue(
                    dimension=self.dimension,
                    severity=SeverityLevel.CRITICAL,
                    field=field,
                    description=f"å­—æ®µ'{field}'å®Œå…¨ç¼ºå¤±",
                    timestamp=datetime.now(),
                    suggestion=f"æ£€æŸ¥æ•°æ®æºæ˜¯å¦æ­£ç¡®æä¾›'{field}'å­—æ®µ"
                ))
            else:
                missing_rate = data[field].isna().sum() / len(data)
                if missing_rate > self.missing_threshold:
                    issues.append(QualityIssue(
                        dimension=self.dimension,
                        severity=self.severity,
                        field=field,
                        description=f"å­—æ®µ'{field}'ç¼ºå¤±ç‡è¿‡é«˜: {missing_rate:.2%}",
                        timestamp=datetime.now(),
                        value=missing_rate,
                        expected=self.missing_threshold,
                        suggestion=f"å¡«å……ç¼ºå¤±å€¼æˆ–ç§»é™¤'{field}'å­—æ®µ"
                    ))
        
        return issues


class AccuracyRule(DataQualityRule):
    """å‡†ç¡®æ€§è§„åˆ™ï¼ˆå¼‚å¸¸å€¼æ£€æµ‹ï¼‰"""
    
    def __init__(self, field: str, 
                 min_value: Optional[float] = None,
                 max_value: Optional[float] = None,
                 use_iqr: bool = True,
                 iqr_multiplier: float = 1.5,
                 severity: SeverityLevel = SeverityLevel.WARNING):
        super().__init__(QualityDimension.ACCURACY, severity)
        self.field = field
        self.min_value = min_value
        self.max_value = max_value
        self.use_iqr = use_iqr
        self.iqr_multiplier = iqr_multiplier
    
    def check(self, data: pd.DataFrame) -> List[QualityIssue]:
        """æ£€æŸ¥å‡†ç¡®æ€§"""
        issues = []
        
        if self.field not in data.columns:
            return issues
        
        values = data[self.field].dropna()
        
        if len(values) == 0:
            return issues
        
        # æ–¹æ³•1: ç¡¬æ€§è¾¹ç•Œæ£€æŸ¥
        if self.min_value is not None:
            outliers = values[values < self.min_value]
            if len(outliers) > 0:
                issues.append(QualityIssue(
                    dimension=self.dimension,
                    severity=self.severity,
                    field=self.field,
                    description=f"æ£€æµ‹åˆ°{len(outliers)}ä¸ªä½äºä¸‹é™({self.min_value})çš„å¼‚å¸¸å€¼",
                    timestamp=datetime.now(),
                    value=outliers.min(),
                    expected=self.min_value,
                    suggestion=f"æ£€æŸ¥'{self.field}'æ•°æ®æ¥æºæ˜¯å¦æ­£ç¡®"
                ))
        
        if self.max_value is not None:
            outliers = values[values > self.max_value]
            if len(outliers) > 0:
                issues.append(QualityIssue(
                    dimension=self.dimension,
                    severity=self.severity,
                    field=self.field,
                    description=f"æ£€æµ‹åˆ°{len(outliers)}ä¸ªé«˜äºä¸Šé™({self.max_value})çš„å¼‚å¸¸å€¼",
                    timestamp=datetime.now(),
                    value=outliers.max(),
                    expected=self.max_value,
                    suggestion=f"æ£€æŸ¥'{self.field}'æ•°æ®æ¥æºæ˜¯å¦æ­£ç¡®"
                ))
        
        # æ–¹æ³•2: IQRæ–¹æ³•æ£€æµ‹ç¦»ç¾¤å€¼
        if self.use_iqr and len(values) >= 10:
            Q1 = values.quantile(0.25)
            Q3 = values.quantile(0.75)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - self.iqr_multiplier * IQR
            upper_bound = Q3 + self.iqr_multiplier * IQR
            
            outliers = values[(values < lower_bound) | (values > upper_bound)]
            outlier_rate = len(outliers) / len(values)
            
            if outlier_rate > 0.05:  # ç¦»ç¾¤å€¼è¶…è¿‡5%
                issues.append(QualityIssue(
                    dimension=self.dimension,
                    severity=self.severity,
                    field=self.field,
                    description=f"IQRæ–¹æ³•æ£€æµ‹åˆ°{len(outliers)}ä¸ªå¼‚å¸¸å€¼({outlier_rate:.2%})",
                    timestamp=datetime.now(),
                    value=outlier_rate,
                    expected=0.05,
                    suggestion=f"ä½¿ç”¨Z-scoreæˆ–Winsorizeæ–¹æ³•å¤„ç†'{self.field}'çš„å¼‚å¸¸å€¼"
                ))
        
        return issues


class ConsistencyRule(DataQualityRule):
    """ä¸€è‡´æ€§è§„åˆ™"""
    
    def __init__(self, checks: List[Dict[str, Any]],
                 severity: SeverityLevel = SeverityLevel.ERROR):
        super().__init__(QualityDimension.CONSISTENCY, severity)
        self.checks = checks
    
    def check(self, data: pd.DataFrame) -> List[QualityIssue]:
        """æ£€æŸ¥ä¸€è‡´æ€§"""
        issues = []
        
        for check in self.checks:
            check_type = check.get("type")
            
            # æ£€æŸ¥ç±»å‹1: ä»·æ ¼å…³ç³»
            if check_type == "price_relation":
                fields = check.get("fields", [])
                if all(f in data.columns for f in fields):
                    # ä¾‹å¦‚ï¼šhigh >= close >= low
                    high_col, close_col, low_col = fields
                    
                    invalid = data[
                        (data[high_col] < data[close_col]) | 
                        (data[close_col] < data[low_col])
                    ]
                    
                    if len(invalid) > 0:
                        issues.append(QualityIssue(
                            dimension=self.dimension,
                            severity=self.severity,
                            field=", ".join(fields),
                            description=f"æ£€æµ‹åˆ°{len(invalid)}è¡Œä»·æ ¼å…³ç³»ä¸ä¸€è‡´(high >= close >= low)",
                            timestamp=datetime.now(),
                            value=len(invalid),
                            suggestion="æ£€æŸ¥æ•°æ®æºæ˜¯å¦å­˜åœ¨é”™è¯¯æˆ–æ•°æ®æ±¡æŸ“"
                        ))
            
            # æ£€æŸ¥ç±»å‹2: æˆäº¤é‡-é‡‘é¢ä¸€è‡´æ€§
            elif check_type == "volume_amount":
                volume_field = check.get("volume_field", "volume")
                amount_field = check.get("amount_field", "amount")
                price_field = check.get("price_field", "close")
                
                if all(f in data.columns for f in [volume_field, amount_field, price_field]):
                    # è®¡ç®—é¢„æœŸé‡‘é¢
                    expected_amount = data[volume_field] * data[price_field]
                    actual_amount = data[amount_field]
                    
                    # å…è®¸2%çš„è¯¯å·®
                    diff_rate = abs((actual_amount - expected_amount) / expected_amount)
                    invalid = data[diff_rate > 0.02]
                    
                    if len(invalid) > 0:
                        issues.append(QualityIssue(
                            dimension=self.dimension,
                            severity=self.severity,
                            field=f"{volume_field}, {amount_field}, {price_field}",
                            description=f"æ£€æµ‹åˆ°{len(invalid)}è¡Œæˆäº¤é‡ä¸é‡‘é¢ä¸ä¸€è‡´",
                            timestamp=datetime.now(),
                            value=len(invalid),
                            suggestion="éªŒè¯æˆäº¤é‡‘é¢ = æˆäº¤é‡ Ã— ä»·æ ¼æ˜¯å¦æˆç«‹"
                        ))
        
        return issues


class TimelinessRule(DataQualityRule):
    """æ—¶æ•ˆæ€§è§„åˆ™"""
    
    def __init__(self, timestamp_field: str = "timestamp",
                 max_delay_seconds: float = 60,
                 severity: SeverityLevel = SeverityLevel.WARNING):
        super().__init__(QualityDimension.TIMELINESS, severity)
        self.timestamp_field = timestamp_field
        self.max_delay_seconds = max_delay_seconds
    
    def check(self, data: pd.DataFrame) -> List[QualityIssue]:
        """æ£€æŸ¥æ—¶æ•ˆæ€§"""
        issues = []
        
        if self.timestamp_field not in data.columns:
            issues.append(QualityIssue(
                dimension=self.dimension,
                severity=SeverityLevel.ERROR,
                field=self.timestamp_field,
                description=f"ç¼ºå°‘æ—¶é—´æˆ³å­—æ®µ'{self.timestamp_field}'",
                timestamp=datetime.now(),
                suggestion=f"ç¡®ä¿æ•°æ®ä¸­åŒ…å«'{self.timestamp_field}'å­—æ®µ"
            ))
            return issues
        
        # è½¬æ¢ä¸ºdatetime
        timestamps = pd.to_datetime(data[self.timestamp_field], errors='coerce')
        now = pd.Timestamp.now()
        
        # è®¡ç®—å»¶è¿Ÿ
        delays = (now - timestamps).dt.total_seconds()
        stale_data = delays[delays > self.max_delay_seconds]
        
        if len(stale_data) > 0:
            max_delay = delays.max()
            avg_delay = delays.mean()
            
            severity = self.severity
            if max_delay > self.max_delay_seconds * 10:
                severity = SeverityLevel.CRITICAL
            
            issues.append(QualityIssue(
                dimension=self.dimension,
                severity=severity,
                field=self.timestamp_field,
                description=f"æ£€æµ‹åˆ°{len(stale_data)}æ¡æ•°æ®å»¶è¿Ÿè¶…è¿‡{self.max_delay_seconds}ç§’",
                timestamp=datetime.now(),
                value=max_delay,
                expected=self.max_delay_seconds,
                suggestion=f"å¹³å‡å»¶è¿Ÿ{avg_delay:.1f}ç§’ï¼Œæœ€å¤§å»¶è¿Ÿ{max_delay:.1f}ç§’ï¼Œæ£€æŸ¥æ•°æ®é‡‡é›†é“¾è·¯"
            ))
        
        return issues


class ValidityRule(DataQualityRule):
    """åˆè§„æ€§è§„åˆ™"""
    
    def __init__(self, field: str, valid_values: Optional[List] = None,
                 regex_pattern: Optional[str] = None,
                 severity: SeverityLevel = SeverityLevel.ERROR):
        super().__init__(QualityDimension.VALIDITY, severity)
        self.field = field
        self.valid_values = valid_values
        self.regex_pattern = regex_pattern
    
    def check(self, data: pd.DataFrame) -> List[QualityIssue]:
        """æ£€æŸ¥åˆè§„æ€§"""
        issues = []
        
        if self.field not in data.columns:
            return issues
        
        values = data[self.field].dropna()
        
        # æ£€æŸ¥æ–¹æ³•1: æšä¸¾å€¼æ ¡éªŒ
        if self.valid_values is not None:
            invalid = values[~values.isin(self.valid_values)]
            if len(invalid) > 0:
                issues.append(QualityIssue(
                    dimension=self.dimension,
                    severity=self.severity,
                    field=self.field,
                    description=f"æ£€æµ‹åˆ°{len(invalid)}ä¸ªä¸åˆè§„å€¼",
                    timestamp=datetime.now(),
                    value=invalid.unique().tolist()[:5],
                    expected=self.valid_values,
                    suggestion=f"'{self.field}'çš„å€¼å¿…é¡»åœ¨{self.valid_values}ä¸­"
                ))
        
        # æ£€æŸ¥æ–¹æ³•2: æ­£åˆ™è¡¨è¾¾å¼æ ¡éªŒ
        if self.regex_pattern is not None:
            invalid = values[~values.astype(str).str.match(self.regex_pattern)]
            if len(invalid) > 0:
                issues.append(QualityIssue(
                    dimension=self.dimension,
                    severity=self.severity,
                    field=self.field,
                    description=f"æ£€æµ‹åˆ°{len(invalid)}ä¸ªæ ¼å¼ä¸åˆè§„å€¼",
                    timestamp=datetime.now(),
                    value=invalid.unique().tolist()[:5],
                    expected=f"åŒ¹é…æ­£åˆ™: {self.regex_pattern}",
                    suggestion=f"'{self.field}'çš„å€¼å¿…é¡»ç¬¦åˆæ ¼å¼è¦æ±‚"
                ))
        
        return issues


class DataQualityMonitor:
    """æ•°æ®è´¨é‡ç›‘æ§å™¨"""
    
    def __init__(self):
        self.rules: List[DataQualityRule] = []
        self.history: List[QualityMetrics] = []
    
    def add_rule(self, rule: DataQualityRule):
        """æ·»åŠ è´¨é‡è§„åˆ™"""
        self.rules.append(rule)
        print(f"âœ… æ·»åŠ è§„åˆ™: {rule.dimension.value} - {rule.__class__.__name__}")
    
    def check(self, data: pd.DataFrame) -> QualityMetrics:
        """æ‰§è¡Œè´¨é‡æ£€æŸ¥"""
        all_issues = []
        
        # æ‰§è¡Œæ‰€æœ‰è§„åˆ™
        for rule in self.rules:
            try:
                issues = rule.check(data)
                all_issues.extend(issues)
            except Exception as e:
                print(f"è§„åˆ™æ‰§è¡Œå¤±è´¥: {rule.__class__.__name__} - {e}")
        
        # ç»Ÿè®¡å„ç»´åº¦æŒ‡æ ‡
        metrics = self._calculate_metrics(data, all_issues)
        
        # ä¿å­˜å†å²
        self.history.append(metrics)
        
        return metrics
    
    def _calculate_metrics(self, data: pd.DataFrame, 
                           issues: List[QualityIssue]) -> QualityMetrics:
        """è®¡ç®—è´¨é‡æŒ‡æ ‡"""
        # æŒ‰ç»´åº¦åˆ†ç»„é—®é¢˜
        dimension_issues = {dim: [] for dim in QualityDimension}
        for issue in issues:
            dimension_issues[issue.dimension].append(issue)
        
        # å®Œæ•´æ€§æŒ‡æ ‡
        completeness_issues = dimension_issues[QualityDimension.COMPLETENESS]
        missing_fields = [i.field for i in completeness_issues]
        completeness_rate = 1.0 - len(missing_fields) / max(len(data.columns), 1)
        
        # å‡†ç¡®æ€§æŒ‡æ ‡
        accuracy_issues = dimension_issues[QualityDimension.ACCURACY]
        outlier_fields = [i.field for i in accuracy_issues]
        accuracy_rate = 1.0 - len(outlier_fields) / max(len(data.columns), 1)
        
        # ä¸€è‡´æ€§æŒ‡æ ‡
        consistency_issues = dimension_issues[QualityDimension.CONSISTENCY]
        inconsistent_fields = [i.field for i in consistency_issues]
        consistency_rate = 1.0 - len(consistency_issues) / max(len(data), 1)
        
        # æ—¶æ•ˆæ€§æŒ‡æ ‡
        timeliness_issues = dimension_issues[QualityDimension.TIMELINESS]
        avg_latency = sum(i.value for i in timeliness_issues if i.value) / max(len(timeliness_issues), 1)
        max_latency = max((i.value for i in timeliness_issues if i.value), default=0)
        stale_data_count = sum(1 for i in timeliness_issues)
        
        # åˆè§„æ€§æŒ‡æ ‡
        validity_issues = dimension_issues[QualityDimension.VALIDITY]
        invalid_fields = [i.field for i in validity_issues]
        validity_rate = 1.0 - len(invalid_fields) / max(len(data.columns), 1)
        
        # æ€»ä½“å¾—åˆ†ï¼ˆåŠ æƒå¹³å‡ï¼‰
        weights = {
            QualityDimension.COMPLETENESS: 0.3,
            QualityDimension.ACCURACY: 0.25,
            QualityDimension.CONSISTENCY: 0.20,
            QualityDimension.TIMELINESS: 0.15,
            QualityDimension.VALIDITY: 0.10
        }
        
        overall_score = (
            completeness_rate * weights[QualityDimension.COMPLETENESS] +
            accuracy_rate * weights[QualityDimension.ACCURACY] +
            consistency_rate * weights[QualityDimension.CONSISTENCY] +
            (1 - min(avg_latency / 100, 1)) * weights[QualityDimension.TIMELINESS] +
            validity_rate * weights[QualityDimension.VALIDITY]
        ) * 100
        
        return QualityMetrics(
            completeness_rate=completeness_rate,
            missing_fields=missing_fields,
            accuracy_rate=accuracy_rate,
            outlier_count=len(accuracy_issues),
            outlier_fields=outlier_fields,
            consistency_rate=consistency_rate,
            inconsistency_count=len(consistency_issues),
            inconsistent_fields=inconsistent_fields,
            avg_latency_ms=avg_latency,
            max_latency_ms=max_latency,
            stale_data_count=stale_data_count,
            validity_rate=validity_rate,
            invalid_count=len(validity_issues),
            invalid_fields=invalid_fields,
            overall_score=overall_score,
            total_issues=len(issues),
            issues=issues
        )
    
    def print_report(self, metrics: QualityMetrics):
        """æ‰“å°è´¨é‡æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š æ•°æ®è´¨é‡æŠ¥å‘Š")
        print("="*60)
        
        # æ€»ä½“è¯„åˆ†
        score = metrics.overall_score
        if score >= 90:
            grade = "ä¼˜ç§€ âœ…"
        elif score >= 80:
            grade = "è‰¯å¥½ âš ï¸"
        elif score >= 70:
            grade = "åˆæ ¼ âš ï¸"
        else:
            grade = "ä¸åˆæ ¼ âŒ"
        
        print(f"\næ€»ä½“å¾—åˆ†: {score:.1f}/100 - {grade}")
        print(f"é—®é¢˜æ€»æ•°: {metrics.total_issues}")
        
        # å„ç»´åº¦è¯¦æƒ…
        print(f"\nğŸ“¦ å®Œæ•´æ€§: {metrics.completeness_rate:.1%}")
        if metrics.missing_fields:
            print(f"   ç¼ºå¤±å­—æ®µ({len(metrics.missing_fields)}): {', '.join(metrics.missing_fields[:5])}")
        
        print(f"\nğŸ¯ å‡†ç¡®æ€§: {metrics.accuracy_rate:.1%}")
        if metrics.outlier_fields:
            print(f"   å¼‚å¸¸å­—æ®µ({len(metrics.outlier_fields)}): {', '.join(metrics.outlier_fields[:5])}")
        
        print(f"\nğŸ”„ ä¸€è‡´æ€§: {metrics.consistency_rate:.1%}")
        if metrics.inconsistent_fields:
            print(f"   ä¸ä¸€è‡´å­—æ®µ({len(metrics.inconsistent_fields)}): {', '.join(metrics.inconsistent_fields[:5])}")
        
        print(f"\nâ±ï¸  æ—¶æ•ˆæ€§:")
        print(f"   å¹³å‡å»¶è¿Ÿ: {metrics.avg_latency_ms:.1f}ms")
        print(f"   æœ€å¤§å»¶è¿Ÿ: {metrics.max_latency_ms:.1f}ms")
        print(f"   è¿‡æœŸæ•°æ®: {metrics.stale_data_count}")
        
        print(f"\nâœ… åˆè§„æ€§: {metrics.validity_rate:.1%}")
        if metrics.invalid_fields:
            print(f"   ä¸åˆè§„å­—æ®µ({len(metrics.invalid_fields)}): {', '.join(metrics.invalid_fields[:5])}")
        
        # é—®é¢˜è¯¦æƒ…
        if metrics.issues:
            print(f"\nâš ï¸  é—®é¢˜è¯¦æƒ… (å…±{len(metrics.issues)}ä¸ª):")
            
            # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
            critical = [i for i in metrics.issues if i.severity == SeverityLevel.CRITICAL]
            errors = [i for i in metrics.issues if i.severity == SeverityLevel.ERROR]
            warnings = [i for i in metrics.issues if i.severity == SeverityLevel.WARNING]
            
            if critical:
                print(f"\nâŒ ä¸¥é‡é—®é¢˜ ({len(critical)}ä¸ª):")
                for issue in critical[:3]:
                    print(f"   - [{issue.dimension.value}] {issue.field}: {issue.description}")
                    if issue.suggestion:
                        print(f"     ğŸ’¡ {issue.suggestion}")
            
            if errors:
                print(f"\nâš ï¸  é”™è¯¯ ({len(errors)}ä¸ª):")
                for issue in errors[:3]:
                    print(f"   - [{issue.dimension.value}] {issue.field}: {issue.description}")
                    if issue.suggestion:
                        print(f"     ğŸ’¡ {issue.suggestion}")
            
            if warnings:
                print(f"\nâ„¹ï¸  è­¦å‘Š ({len(warnings)}ä¸ª):")
                for issue in warnings[:3]:
                    print(f"   - [{issue.dimension.value}] {issue.field}: {issue.description}")
        
        print("\n" + "="*60 + "\n")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºç›‘æ§å™¨
    monitor = DataQualityMonitor()
    
    # æ·»åŠ è§„åˆ™
    print("ğŸ”§ é…ç½®è´¨é‡è§„åˆ™...\n")
    
    # 1. å®Œæ•´æ€§è§„åˆ™
    monitor.add_rule(CompletenessRule(
        required_fields=["symbol", "open", "high", "low", "close", "volume"],
        missing_threshold=0.01
    ))
    
    # 2. å‡†ç¡®æ€§è§„åˆ™
    monitor.add_rule(AccuracyRule(
        field="close",
        min_value=0.01,
        max_value=10000,
        use_iqr=True
    ))
    
    monitor.add_rule(AccuracyRule(
        field="volume",
        min_value=0,
        use_iqr=True
    ))
    
    # 3. ä¸€è‡´æ€§è§„åˆ™
    monitor.add_rule(ConsistencyRule(checks=[
        {
            "type": "price_relation",
            "fields": ["high", "close", "low"]
        }
    ]))
    
    # 4. æ—¶æ•ˆæ€§è§„åˆ™
    monitor.add_rule(TimelinessRule(
        timestamp_field="timestamp",
        max_delay_seconds=60
    ))
    
    # 5. åˆè§„æ€§è§„åˆ™
    monitor.add_rule(ValidityRule(
        field="symbol",
        regex_pattern=r"^\d{6}\.(SZ|SH)$"
    ))
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    print("\nğŸ“¦ ç”Ÿæˆæµ‹è¯•æ•°æ®...\n")
    test_data = pd.DataFrame({
        "symbol": ["000001.SZ", "600000.SH", "INVALID", "000002.SZ"],
        "open": [10.0, 20.0, np.nan, 15.0],
        "high": [10.5, 21.0, 16.0, 15.5],
        "close": [10.3, 20.5, 15.8, 15.2],
        "low": [9.8, 19.5, 15.5, 14.8],
        "volume": [1000000, 2000000, -500, 1500000],  # å¼‚å¸¸å€¼: -500
        "timestamp": [
            datetime.now(),
            datetime.now() - timedelta(seconds=30),
            datetime.now() - timedelta(seconds=120),  # è¿‡æœŸæ•°æ®
            datetime.now() - timedelta(seconds=10)
        ]
    })
    
    # æ‰§è¡Œæ£€æŸ¥
    print("ğŸ” æ‰§è¡Œè´¨é‡æ£€æŸ¥...\n")
    metrics = monitor.check(test_data)
    
    # æ‰“å°æŠ¥å‘Š
    monitor.print_report(metrics)
    
    print("âœ… å®Œæˆ")
