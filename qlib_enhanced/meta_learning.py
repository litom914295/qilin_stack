"""
P2-8: å…ƒå­¦ä¹ ä¸è¿ç§»å­¦ä¹ æ¨¡å— (Meta-Learning & Transfer Learning)
å®ç°MAMLã€æ¨¡å‹é€‚é…ã€è·¨å¸‚åœºè¿ç§»ç­‰åŠŸèƒ½
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from copy import deepcopy
import logging

logger = logging.getLogger(__name__)


@dataclass
class MetaLearningConfig:
    """å…ƒå­¦ä¹ é…ç½®"""
    inner_lr: float = 0.01          # å†…å¾ªç¯å­¦ä¹ ç‡
    outer_lr: float = 0.001         # å¤–å¾ªç¯å­¦ä¹ ç‡
    num_inner_steps: int = 5        # å†…å¾ªç¯æ›´æ–°æ­¥æ•°
    meta_batch_size: int = 4        # å…ƒæ‰¹æ¬¡å¤§å°
    num_epochs: int = 100           # è®­ç»ƒè½®æ•°


class SimpleNet(nn.Module):
    """ç®€å•ç¥ç»ç½‘ç»œæ¨¡å‹"""
    
    def __init__(self, input_dim: int = 10, hidden_dim: int = 64, output_dim: int = 1):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
    
    def forward(self, x):
        return self.network(x)


class MAMLTrainer:
    """Model-Agnostic Meta-Learning (MAML) è®­ç»ƒå™¨"""
    
    def __init__(self, model: nn.Module, config: MetaLearningConfig):
        """
        åˆå§‹åŒ–MAMLè®­ç»ƒå™¨
        
        Args:
            model: åŸºç¡€æ¨¡å‹
            config: å…ƒå­¦ä¹ é…ç½®
        """
        self.model = model
        self.config = config
        self.meta_optimizer = optim.Adam(
            self.model.parameters(), 
            lr=config.outer_lr
        )
        self.loss_fn = nn.MSELoss()
        
        logger.info(f"MAMLè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def inner_loop(self, support_x: torch.Tensor, support_y: torch.Tensor) -> nn.Module:
        """
        å†…å¾ªç¯æ›´æ–°ï¼ˆä»»åŠ¡ç‰¹å®šçš„å¿«é€Ÿé€‚é…ï¼‰
        
        Args:
            support_x: æ”¯æŒé›†è¾“å…¥
            support_y: æ”¯æŒé›†æ ‡ç­¾
        
        Returns:
            é€‚é…åçš„æ¨¡å‹
        """
        # å¤åˆ¶æ¨¡å‹
        adapted_model = deepcopy(self.model)
        optimizer = optim.SGD(
            adapted_model.parameters(), 
            lr=self.config.inner_lr
        )
        
        # åœ¨æ”¯æŒé›†ä¸Šå¿«é€Ÿé€‚é…
        for _ in range(self.config.num_inner_steps):
            pred = adapted_model(support_x)
            loss = self.loss_fn(pred, support_y)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        return adapted_model
    
    def meta_train_step(self, tasks: List[Tuple[torch.Tensor, torch.Tensor, 
                                                  torch.Tensor, torch.Tensor]]) -> float:
        """
        å…ƒè®­ç»ƒæ­¥éª¤ï¼ˆå¤–å¾ªç¯æ›´æ–°ï¼‰
        
        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å«(support_x, support_y, query_x, query_y)
        
        Returns:
            å…ƒæŸå¤±
        """
        meta_loss = 0.0
        
        self.meta_optimizer.zero_grad()
        
        for support_x, support_y, query_x, query_y in tasks:
            # å†…å¾ªç¯ï¼šåœ¨æ”¯æŒé›†ä¸Šå¿«é€Ÿé€‚é…
            adapted_model = self.inner_loop(support_x, support_y)
            
            # åœ¨æŸ¥è¯¢é›†ä¸Šè¯„ä¼°
            query_pred = adapted_model(query_x)
            task_loss = self.loss_fn(query_pred, query_y)
            
            meta_loss += task_loss
        
        # å¤–å¾ªç¯ï¼šæ›´æ–°å…ƒæ¨¡å‹
        meta_loss = meta_loss / len(tasks)
        meta_loss.backward()
        self.meta_optimizer.step()
        
        return meta_loss.item()
    
    def adapt(self, support_x: torch.Tensor, support_y: torch.Tensor) -> nn.Module:
        """
        å¿«é€Ÿé€‚é…åˆ°æ–°ä»»åŠ¡
        
        Args:
            support_x: æ–°ä»»åŠ¡çš„æ”¯æŒé›†è¾“å…¥
            support_y: æ–°ä»»åŠ¡çš„æ”¯æŒé›†æ ‡ç­¾
        
        Returns:
            é€‚é…åçš„æ¨¡å‹
        """
        return self.inner_loop(support_x, support_y)


class TransferLearner:
    """è¿ç§»å­¦ä¹ å™¨"""
    
    def __init__(self, source_model: nn.Module, target_task_dim: int):
        """
        åˆå§‹åŒ–è¿ç§»å­¦ä¹ å™¨
        
        Args:
            source_model: æºåŸŸé¢„è®­ç»ƒæ¨¡å‹
            target_task_dim: ç›®æ ‡ä»»åŠ¡ç»´åº¦
        """
        self.source_model = source_model
        self.target_model = self._build_transfer_model(target_task_dim)
        
        logger.info("è¿ç§»å­¦ä¹ å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _build_transfer_model(self, target_dim: int) -> nn.Module:
        """æ„å»ºè¿ç§»æ¨¡å‹ï¼ˆå†»ç»“æºæ¨¡å‹ç‰¹å¾å±‚ï¼‰"""
        # ç®€åŒ–ç‰ˆï¼šå¤åˆ¶æºæ¨¡å‹å¹¶æ·»åŠ é€‚é…å±‚
        transfer_model = deepcopy(self.source_model)
        
        # å†»ç»“å‰é¢çš„å±‚
        for param in list(transfer_model.parameters())[:-2]:
            param.requires_grad = False
        
        return transfer_model
    
    def fine_tune(self, x: torch.Tensor, y: torch.Tensor, 
                  epochs: int = 10, lr: float = 0.001) -> Dict[str, float]:
        """
        åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šå¾®è°ƒ
        
        Args:
            x: ç›®æ ‡ä»»åŠ¡è¾“å…¥
            y: ç›®æ ‡ä»»åŠ¡æ ‡ç­¾
            epochs: è®­ç»ƒè½®æ•°
            lr: å­¦ä¹ ç‡
        
        Returns:
            è®­ç»ƒå†å²
        """
        optimizer = optim.Adam(
            filter(lambda p: p.requires_grad, self.target_model.parameters()),
            lr=lr
        )
        loss_fn = nn.MSELoss()
        
        history = {'loss': []}
        
        for epoch in range(epochs):
            pred = self.target_model(x)
            loss = loss_fn(pred, y)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            history['loss'].append(loss.item())
        
        return history


class FewShotLearner:
    """å°‘æ ·æœ¬å­¦ä¹ å™¨"""
    
    def __init__(self, model: nn.Module, metric: str = 'euclidean'):
        """
        åˆå§‹åŒ–å°‘æ ·æœ¬å­¦ä¹ å™¨
        
        Args:
            model: ç‰¹å¾æå–æ¨¡å‹
            metric: è·ç¦»åº¦é‡æ–¹å¼
        """
        self.model = model
        self.metric = metric
        self.prototypes = {}
        
        logger.info(f"å°‘æ ·æœ¬å­¦ä¹ å™¨åˆå§‹åŒ– (metric={metric})")
    
    def compute_prototypes(self, support_x: torch.Tensor, 
                          support_y: torch.Tensor) -> Dict[int, torch.Tensor]:
        """
        è®¡ç®—åŸå‹ï¼ˆæ¯ä¸ªç±»çš„ç‰¹å¾ä¸­å¿ƒï¼‰
        
        Args:
            support_x: æ”¯æŒé›†è¾“å…¥
            support_y: æ”¯æŒé›†æ ‡ç­¾
        
        Returns:
            åŸå‹å­—å…¸
        """
        with torch.no_grad():
            features = self.model(support_x)
        
        prototypes = {}
        for label in torch.unique(support_y):
            mask = (support_y == label).squeeze()
            class_features = features[mask]
            prototypes[label.item()] = class_features.mean(dim=0)
        
        return prototypes
    
    def predict(self, query_x: torch.Tensor, 
                support_x: torch.Tensor, 
                support_y: torch.Tensor) -> torch.Tensor:
        """
        åŸºäºåŸå‹ç½‘ç»œé¢„æµ‹
        
        Args:
            query_x: æŸ¥è¯¢æ ·æœ¬
            support_x: æ”¯æŒé›†è¾“å…¥
            support_y: æ”¯æŒé›†æ ‡ç­¾
        
        Returns:
            é¢„æµ‹ç»“æœ
        """
        # è®¡ç®—åŸå‹
        prototypes = self.compute_prototypes(support_x, support_y)
        
        # æå–æŸ¥è¯¢ç‰¹å¾
        with torch.no_grad():
            query_features = self.model(query_x)
        
        # è®¡ç®—åˆ°æ¯ä¸ªåŸå‹çš„è·ç¦»
        distances = {}
        for label, prototype in prototypes.items():
            if self.metric == 'euclidean':
                dist = torch.cdist(query_features, prototype.unsqueeze(0))
            else:
                dist = 1 - torch.cosine_similarity(
                    query_features, 
                    prototype.unsqueeze(0), 
                    dim=-1
                )
            distances[label] = dist
        
        # è¿”å›æœ€è¿‘çš„åŸå‹
        predictions = []
        for i in range(len(query_x)):
            min_label = min(distances.keys(), 
                          key=lambda l: distances[l][i])
            predictions.append(min_label)
        
        return torch.tensor(predictions)


def create_sample_tasks(num_tasks: int = 10, 
                       task_size: int = 20) -> List[Tuple]:
    """
    åˆ›å»ºç¤ºä¾‹å…ƒå­¦ä¹ ä»»åŠ¡
    
    Args:
        num_tasks: ä»»åŠ¡æ•°é‡
        task_size: æ¯ä¸ªä»»åŠ¡çš„æ ·æœ¬æ•°
    
    Returns:
        ä»»åŠ¡åˆ—è¡¨
    """
    tasks = []
    
    for i in range(num_tasks):
        # ç”Ÿæˆä»»åŠ¡ç‰¹å®šçš„æ•°æ®åˆ†å¸ƒ
        np.random.seed(i)
        
        # æ”¯æŒé›†
        support_x = torch.randn(task_size // 2, 10)
        support_y = torch.randn(task_size // 2, 1)
        
        # æŸ¥è¯¢é›†
        query_x = torch.randn(task_size // 2, 10)
        query_y = torch.randn(task_size // 2, 1)
        
        tasks.append((support_x, support_y, query_x, query_y))
    
    return tasks


def main():
    """ç¤ºä¾‹: å…ƒå­¦ä¹ ä¸è¿ç§»å­¦ä¹ """
    print("=" * 80)
    print("P2-8: å…ƒå­¦ä¹ ä¸è¿ç§»å­¦ä¹  - ç¤ºä¾‹")
    print("=" * 80)
    
    # 1. MAMLå…ƒå­¦ä¹ 
    print("\nğŸ“š MAMLå…ƒå­¦ä¹ è®­ç»ƒ...")
    
    config = MetaLearningConfig(
        inner_lr=0.01,
        outer_lr=0.001,
        num_inner_steps=5,
        meta_batch_size=4,
        num_epochs=10
    )
    
    model = SimpleNet(input_dim=10, hidden_dim=32, output_dim=1)
    maml_trainer = MAMLTrainer(model, config)
    
    # åˆ›å»ºå…ƒå­¦ä¹ ä»»åŠ¡
    all_tasks = create_sample_tasks(num_tasks=20, task_size=20)
    
    # å…ƒè®­ç»ƒ
    for epoch in range(5):
        batch_tasks = all_tasks[:config.meta_batch_size]
        meta_loss = maml_trainer.meta_train_step(batch_tasks)
        print(f"  Epoch {epoch+1}/5 - Meta Loss: {meta_loss:.4f}")
    
    print("âœ… MAMLè®­ç»ƒå®Œæˆ")
    
    # 2. å¿«é€Ÿé€‚é…åˆ°æ–°ä»»åŠ¡
    print("\nğŸ¯ å¿«é€Ÿé€‚é…åˆ°æ–°ä»»åŠ¡...")
    new_task = all_tasks[-1]
    support_x, support_y, query_x, query_y = new_task
    
    adapted_model = maml_trainer.adapt(support_x, support_y)
    
    with torch.no_grad():
        pred = adapted_model(query_x)
        test_loss = nn.MSELoss()(pred, query_y)
    
    print(f"  é€‚é…åæµ‹è¯•æŸå¤±: {test_loss:.4f}")
    print("âœ… å¿«é€Ÿé€‚é…å®Œæˆ")
    
    # 3. è¿ç§»å­¦ä¹ 
    print("\nğŸ”„ è¿ç§»å­¦ä¹ ç¤ºä¾‹...")
    
    source_model = SimpleNet(input_dim=10, hidden_dim=32, output_dim=1)
    transfer_learner = TransferLearner(source_model, target_task_dim=1)
    
    # åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šå¾®è°ƒ
    target_x = torch.randn(50, 10)
    target_y = torch.randn(50, 1)
    
    history = transfer_learner.fine_tune(target_x, target_y, epochs=5)
    print(f"  å¾®è°ƒæŸå¤±: {history['loss'][-1]:.4f}")
    print("âœ… è¿ç§»å­¦ä¹ å®Œæˆ")
    
    # 4. å°‘æ ·æœ¬å­¦ä¹ 
    print("\nğŸ² å°‘æ ·æœ¬å­¦ä¹ ç¤ºä¾‹...")
    
    feature_model = SimpleNet(input_dim=10, hidden_dim=32, output_dim=8)
    few_shot = FewShotLearner(feature_model, metric='euclidean')
    
    # 5-way 5-shot ä»»åŠ¡
    support_x = torch.randn(25, 10)  # 5ç±» Ã— 5æ ·æœ¬
    support_y = torch.tensor([[i] for i in range(5) for _ in range(5)])
    query_x = torch.randn(10, 10)
    
    predictions = few_shot.predict(query_x, support_x, support_y)
    print(f"  é¢„æµ‹ç»“æœ: {predictions.tolist()}")
    print("âœ… å°‘æ ·æœ¬å­¦ä¹ å®Œæˆ")
    
    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰å…ƒå­¦ä¹ åŠŸèƒ½æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 80)


if __name__ == '__main__':
    main()
