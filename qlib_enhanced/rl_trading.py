"""
å¼ºåŒ–å­¦ä¹ äº¤æ˜“ç­–ç•¥æ¨¡å—
æ”¯æŒDQNã€PPOã€A2Cç­‰ç®—æ³•è¿›è¡Œè‚¡ç¥¨äº¤æ˜“ç­–ç•¥è®­ç»ƒ
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
import logging
from dataclasses import dataclass
from enum import Enum
import pickle

logger = logging.getLogger(__name__)


# ============================================================================
# äº¤æ˜“åŠ¨ä½œå®šä¹‰
# ============================================================================

class TradingAction(Enum):
    """äº¤æ˜“åŠ¨ä½œ"""
    HOLD = 0      # æŒæœ‰
    BUY = 1       # ä¹°å…¥
    SELL = 2      # å–å‡º


@dataclass
class TradeExecution:
    """äº¤æ˜“æ‰§è¡Œè®°å½•"""
    timestamp: datetime
    symbol: str
    action: TradingAction
    quantity: int
    price: float
    value: float
    commission: float


# ============================================================================
# äº¤æ˜“ç¯å¢ƒ
# ============================================================================

class TradingEnvironment:
    """
    å¼ºåŒ–å­¦ä¹ äº¤æ˜“ç¯å¢ƒ
    æ¨¡æ‹ŸçœŸå®è‚¡ç¥¨å¸‚åœºäº¤æ˜“
    """
    
    def __init__(self,
                 data: pd.DataFrame,
                 initial_balance: float = 100000.0,
                 commission_rate: float = 0.001,
                 max_position: int = 1000):
        """
        åˆå§‹åŒ–äº¤æ˜“ç¯å¢ƒ
        
        Args:
            data: å†å²ä»·æ ¼æ•°æ® (columns: open, high, low, close, volume)
            initial_balance: åˆå§‹èµ„é‡‘
            commission_rate: æ‰‹ç»­è´¹ç‡
            max_position: æœ€å¤§æŒä»“æ•°é‡
        """
        self.data = data
        self.initial_balance = initial_balance
        self.commission_rate = commission_rate
        self.max_position = max_position
        
        # ç¯å¢ƒçŠ¶æ€
        self.current_step = 0
        self.balance = initial_balance
        self.position = 0  # å½“å‰æŒä»“
        self.total_value = initial_balance
        self.trades = []
        
        # æ€§èƒ½æŒ‡æ ‡
        self.max_value = initial_balance
        self.max_drawdown = 0.0
        
        logger.info(f"äº¤æ˜“ç¯å¢ƒåˆå§‹åŒ–: åˆå§‹èµ„é‡‘={initial_balance}, æ•°æ®é•¿åº¦={len(data)}")
    
    def reset(self) -> np.ndarray:
        """é‡ç½®ç¯å¢ƒ"""
        self.current_step = 0
        self.balance = self.initial_balance
        self.position = 0
        self.total_value = self.initial_balance
        self.trades = []
        self.max_value = self.initial_balance
        self.max_drawdown = 0.0
        
        return self._get_state()
    
    def _get_state(self) -> np.ndarray:
        """
        è·å–å½“å‰çŠ¶æ€
        
        Returns:
            çŠ¶æ€å‘é‡: [ä»·æ ¼ç‰¹å¾, æŒä»“ä¿¡æ¯, è´¦æˆ·ä¿¡æ¯]
        """
        if self.current_step >= len(self.data):
            return np.zeros(20)  # é»˜è®¤çŠ¶æ€ç»´åº¦
        
        # ä»·æ ¼ç‰¹å¾ (æœ€è¿‘5å¤©)
        lookback = 5
        start_idx = max(0, self.current_step - lookback + 1)
        recent_data = self.data.iloc[start_idx:self.current_step + 1]
        
        price_features = []
        for col in ['close', 'volume']:
            if col in recent_data.columns:
                values = recent_data[col].values
                # å½’ä¸€åŒ–
                if len(values) > 0:
                    normalized = (values - values.mean()) / (values.std() + 1e-8)
                    # å¡«å……åˆ°å›ºå®šé•¿åº¦
                    if len(normalized) < lookback:
                        normalized = np.pad(normalized, (lookback - len(normalized), 0))
                    price_features.extend(normalized[-lookback:])
        
        # æŒä»“ä¿¡æ¯
        current_price = self.data.iloc[self.current_step]['close']
        position_ratio = self.position / self.max_position if self.max_position > 0 else 0
        position_value = self.position * current_price
        position_features = [
            position_ratio,
            position_value / self.initial_balance if self.initial_balance > 0 else 0
        ]
        
        # è´¦æˆ·ä¿¡æ¯
        total_value = self.balance + position_value
        account_features = [
            self.balance / self.initial_balance if self.initial_balance > 0 else 0,
            total_value / self.initial_balance if self.initial_balance > 0 else 0,
            (total_value - self.initial_balance) / self.initial_balance if self.initial_balance > 0 else 0
        ]
        
        # ç»„åˆçŠ¶æ€
        state = np.array(price_features + position_features + account_features, dtype=np.float32)
        
        # ç¡®ä¿å›ºå®šç»´åº¦
        if len(state) < 20:
            state = np.pad(state, (0, 20 - len(state)))
        elif len(state) > 20:
            state = state[:20]
        
        return state
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:
        """
        æ‰§è¡ŒåŠ¨ä½œ
        
        Args:
            action: 0=æŒæœ‰, 1=ä¹°å…¥, 2=å–å‡º
        
        Returns:
            next_state, reward, done, info
        """
        current_price = self.data.iloc[self.current_step]['close']
        trading_action = TradingAction(action)
        
        # æ‰§è¡Œäº¤æ˜“
        reward = 0.0
        trade_info = None
        
        if trading_action == TradingAction.BUY and self.balance > 0:
            # ä¹°å…¥
            max_buy = int(self.balance / (current_price * (1 + self.commission_rate)))
            buy_quantity = min(max_buy, self.max_position - self.position)
            
            if buy_quantity > 0:
                cost = buy_quantity * current_price * (1 + self.commission_rate)
                self.balance -= cost
                self.position += buy_quantity
                
                trade_info = TradeExecution(
                    timestamp=self.data.index[self.current_step],
                    symbol='stock',
                    action=TradingAction.BUY,
                    quantity=buy_quantity,
                    price=current_price,
                    value=cost,
                    commission=buy_quantity * current_price * self.commission_rate
                )
                self.trades.append(trade_info)
        
        elif trading_action == TradingAction.SELL and self.position > 0:
            # å–å‡ºå…¨éƒ¨æŒä»“
            sell_value = self.position * current_price * (1 - self.commission_rate)
            self.balance += sell_value
            
            trade_info = TradeExecution(
                timestamp=self.data.index[self.current_step],
                symbol='stock',
                action=TradingAction.SELL,
                quantity=self.position,
                price=current_price,
                value=sell_value,
                commission=self.position * current_price * self.commission_rate
            )
            self.trades.append(trade_info)
            self.position = 0
        
        # ç§»åŠ¨åˆ°ä¸‹ä¸€æ­¥
        self.current_step += 1
        
        # è®¡ç®—å¥–åŠ±
        if self.current_step < len(self.data):
            next_price = self.data.iloc[self.current_step]['close']
            position_value = self.position * next_price
            new_total_value = self.balance + position_value
            
            # å¥–åŠ± = æ€»èµ„äº§å˜åŒ–ç‡
            reward = (new_total_value - self.total_value) / self.total_value if self.total_value > 0 else 0
            
            self.total_value = new_total_value
            
            # æ›´æ–°æœ€å¤§å›æ’¤
            if self.total_value > self.max_value:
                self.max_value = self.total_value
            drawdown = (self.max_value - self.total_value) / self.max_value if self.max_value > 0 else 0
            self.max_drawdown = max(self.max_drawdown, drawdown)
        
        # åˆ¤æ–­æ˜¯å¦ç»“æŸ
        done = self.current_step >= len(self.data) - 1
        
        # ä¸‹ä¸€ä¸ªçŠ¶æ€
        next_state = self._get_state() if not done else np.zeros(20)
        
        # ä¿¡æ¯
        info = {
            'balance': self.balance,
            'position': self.position,
            'total_value': self.total_value,
            'max_drawdown': self.max_drawdown,
            'trade': trade_info
        }
        
        return next_state, reward, done, info
    
    def get_performance(self) -> Dict[str, float]:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        total_return = (self.total_value - self.initial_balance) / self.initial_balance
        num_trades = len(self.trades)
        
        # è®¡ç®—å¤æ™®æ¯”ç‡
        if len(self.trades) > 1:
            returns = []
            for i in range(1, len(self.trades)):
                returns.append((self.trades[i].value - self.trades[i-1].value) / self.trades[i-1].value)
            
            if len(returns) > 0:
                sharpe = np.mean(returns) / (np.std(returns) + 1e-8) * np.sqrt(252)
            else:
                sharpe = 0.0
        else:
            sharpe = 0.0
        
        return {
            'total_return': total_return,
            'total_value': self.total_value,
            'max_drawdown': self.max_drawdown,
            'num_trades': num_trades,
            'sharpe_ratio': sharpe
        }


# ============================================================================
# DQNæ™ºèƒ½ä½“
# ============================================================================

class DQNAgent:
    """
    Deep Q-Network (DQN) äº¤æ˜“æ™ºèƒ½ä½“
    """
    
    def __init__(self,
                 state_dim: int = 20,
                 action_dim: int = 3,
                 learning_rate: float = 0.001,
                 gamma: float = 0.99,
                 epsilon: float = 1.0,
                 epsilon_decay: float = 0.995,
                 epsilon_min: float = 0.01):
        """
        åˆå§‹åŒ–DQNæ™ºèƒ½ä½“
        
        Args:
            state_dim: çŠ¶æ€ç©ºé—´ç»´åº¦
            action_dim: åŠ¨ä½œç©ºé—´ç»´åº¦
            learning_rate: å­¦ä¹ ç‡
            gamma: æŠ˜æ‰£å› å­
            epsilon: æ¢ç´¢ç‡
            epsilon_decay: æ¢ç´¢ç‡è¡°å‡
            epsilon_min: æœ€å°æ¢ç´¢ç‡
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        
        # Qç½‘ç»œ (ç®€åŒ–ç‰ˆï¼Œå®é™…åº”ä½¿ç”¨ç¥ç»ç½‘ç»œ)
        self.q_table = {}
        
        # ç»éªŒå›æ”¾
        self.memory = []
        self.memory_size = 10000
        
        logger.info(f"DQNæ™ºèƒ½ä½“åˆå§‹åŒ–: state_dim={state_dim}, action_dim={action_dim}")
    
    def _state_to_key(self, state: np.ndarray) -> str:
        """å°†çŠ¶æ€è½¬æ¢ä¸ºå­—å…¸é”®"""
        # ç¦»æ•£åŒ–çŠ¶æ€
        discretized = (state * 10).astype(int)
        return str(discretized.tolist())
    
    def choose_action(self, state: np.ndarray, training: bool = True) -> int:
        """
        é€‰æ‹©åŠ¨ä½œ
        
        Args:
            state: å½“å‰çŠ¶æ€
            training: æ˜¯å¦è®­ç»ƒæ¨¡å¼
        
        Returns:
            åŠ¨ä½œç´¢å¼•
        """
        if training and np.random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ
            return np.random.randint(0, self.action_dim)
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
            state_key = self._state_to_key(state)
            if state_key not in self.q_table:
                return np.random.randint(0, self.action_dim)
            
            q_values = self.q_table[state_key]
            return np.argmax(q_values)
    
    def store_experience(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        self.memory.append((state, action, reward, next_state, done))
        if len(self.memory) > self.memory_size:
            self.memory.pop(0)
    
    def train(self, batch_size: int = 32):
        """è®­ç»ƒQç½‘ç»œ"""
        if len(self.memory) < batch_size:
            return 0.0
        
        # éšæœºé‡‡æ ·
        indices = np.random.choice(len(self.memory), batch_size, replace=False)
        batch = [self.memory[i] for i in indices]
        
        total_loss = 0.0
        
        for state, action, reward, next_state, done in batch:
            state_key = self._state_to_key(state)
            next_state_key = self._state_to_key(next_state)
            
            # åˆå§‹åŒ–Qå€¼
            if state_key not in self.q_table:
                self.q_table[state_key] = np.zeros(self.action_dim)
            if next_state_key not in self.q_table:
                self.q_table[next_state_key] = np.zeros(self.action_dim)
            
            # Q-learningæ›´æ–°
            current_q = self.q_table[state_key][action]
            if done:
                target_q = reward
            else:
                target_q = reward + self.gamma * np.max(self.q_table[next_state_key])
            
            # æ›´æ–°Qå€¼
            self.q_table[state_key][action] += self.learning_rate * (target_q - current_q)
            
            total_loss += abs(target_q - current_q)
        
        # è¡°å‡æ¢ç´¢ç‡
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        return total_loss / batch_size
    
    def save(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        with open(filepath, 'wb') as f:
            pickle.dump({
                'q_table': self.q_table,
                'epsilon': self.epsilon
            }, f)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜: {filepath}")
    
    def load(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
            self.q_table = data['q_table']
            self.epsilon = data['epsilon']
        logger.info(f"æ¨¡å‹å·²åŠ è½½: {filepath}")


# ============================================================================
# è®­ç»ƒå™¨
# ============================================================================

class RLTrainer:
    """å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self,
                 env: TradingEnvironment,
                 agent: DQNAgent):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            env: äº¤æ˜“ç¯å¢ƒ
            agent: RLæ™ºèƒ½ä½“
        """
        self.env = env
        self.agent = agent
        self.training_history = []
    
    def train(self, num_episodes: int = 100, batch_size: int = 32) -> List[Dict]:
        """
        è®­ç»ƒæ™ºèƒ½ä½“
        
        Args:
            num_episodes: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
        
        Returns:
            è®­ç»ƒå†å²
        """
        logger.info(f"å¼€å§‹è®­ç»ƒ: {num_episodes} è½®")
        
        for episode in range(num_episodes):
            state = self.env.reset()
            total_reward = 0.0
            done = False
            steps = 0
            
            while not done:
                # é€‰æ‹©åŠ¨ä½œ
                action = self.agent.choose_action(state, training=True)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, info = self.env.step(action)
                
                # å­˜å‚¨ç»éªŒ
                self.agent.store_experience(state, action, reward, next_state, done)
                
                # è®­ç»ƒ
                loss = self.agent.train(batch_size)
                
                total_reward += reward
                state = next_state
                steps += 1
            
            # è®°å½•è®­ç»ƒå†å²
            performance = self.env.get_performance()
            history_entry = {
                'episode': episode + 1,
                'total_reward': total_reward,
                'steps': steps,
                'epsilon': self.agent.epsilon,
                **performance
            }
            self.training_history.append(history_entry)
            
            if (episode + 1) % 10 == 0:
                logger.info(f"Episode {episode + 1}/{num_episodes}: "
                          f"Return={performance['total_return']:.2%}, "
                          f"Value={performance['total_value']:.2f}, "
                          f"Epsilon={self.agent.epsilon:.3f}")
        
        logger.info("è®­ç»ƒå®Œæˆï¼")
        return self.training_history
    
    def backtest(self) -> Dict[str, Any]:
        """å›æµ‹æ™ºèƒ½ä½“"""
        logger.info("å¼€å§‹å›æµ‹...")
        
        state = self.env.reset()
        done = False
        actions_taken = []
        
        while not done:
            action = self.agent.choose_action(state, training=False)
            next_state, reward, done, info = self.env.step(action)
            
            actions_taken.append({
                'step': self.env.current_step - 1,
                'action': TradingAction(action).name,
                'balance': info['balance'],
                'position': info['position'],
                'total_value': info['total_value']
            })
            
            state = next_state
        
        performance = self.env.get_performance()
        
        logger.info(f"å›æµ‹å®Œæˆ: æ€»æ”¶ç›Š={performance['total_return']:.2%}")
        
        return {
            'performance': performance,
            'actions': actions_taken,
            'trades': self.env.trades
        }


# ============================================================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================================================

def create_sample_data(days: int = 252) -> pd.DataFrame:
    """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®"""
    np.random.seed(42)
    dates = pd.date_range(start='2023-01-01', periods=days, freq='D')
    
    # æ¨¡æ‹Ÿä»·æ ¼èµ°åŠ¿
    prices = 100 * np.exp(np.cumsum(np.random.randn(days) * 0.02))
    
    data = pd.DataFrame({
        'open': prices * (1 + np.random.randn(days) * 0.01),
        'high': prices * (1 + np.abs(np.random.randn(days) * 0.02)),
        'low': prices * (1 - np.abs(np.random.randn(days) * 0.02)),
        'close': prices,
        'volume': np.random.randint(1000000, 10000000, days)
    }, index=dates)
    
    return data


def main():
    """ç¤ºä¾‹ï¼šè®­ç»ƒDQNäº¤æ˜“ç­–ç•¥"""
    print("=" * 80)
    print("å¼ºåŒ–å­¦ä¹ äº¤æ˜“ç­–ç•¥ - DQNè®­ç»ƒç¤ºä¾‹")
    print("=" * 80)
    
    # 1. åˆ›å»ºç¯å¢ƒ
    print("\nğŸ“Š åˆ›å»ºäº¤æ˜“ç¯å¢ƒ...")
    data = create_sample_data(days=252)
    env = TradingEnvironment(data, initial_balance=100000.0)
    
    # 2. åˆ›å»ºæ™ºèƒ½ä½“
    print("ğŸ¤– åˆ›å»ºDQNæ™ºèƒ½ä½“...")
    agent = DQNAgent(state_dim=20, action_dim=3)
    
    # 3. è®­ç»ƒ
    print("\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
    trainer = RLTrainer(env, agent)
    history = trainer.train(num_episodes=50, batch_size=32)
    
    # 4. å›æµ‹
    print("\nğŸ“ˆ å›æµ‹ç­–ç•¥...")
    backtest_results = trainer.backtest()
    
    # 5. æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ“Š è®­ç»ƒç»“æœ")
    print("=" * 80)
    
    final_performance = history[-1]
    print(f"æ€»æ”¶ç›Šç‡: {final_performance['total_return']:.2%}")
    print(f"æœ€ç»ˆèµ„äº§: Â¥{final_performance['total_value']:,.2f}")
    print(f"æœ€å¤§å›æ’¤: {final_performance['max_drawdown']:.2%}")
    print(f"å¤æ™®æ¯”ç‡: {final_performance['sharpe_ratio']:.2f}")
    print(f"äº¤æ˜“æ¬¡æ•°: {final_performance['num_trades']}")
    
    print("\n" + "=" * 80)
    print("âœ… å®Œæˆï¼")
    print("=" * 80)


if __name__ == '__main__':
    from qilin_stack.app.core.logging_setup import setup_logging
    setup_logging()
    main()
