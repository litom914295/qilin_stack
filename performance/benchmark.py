"""
æ€§èƒ½åŸºå‡†æµ‹è¯• - å¯¹æ¯”å¹¶å‘ä¼˜åŒ–æ•ˆæžœ
"""
import asyncio
import time
import logging
from typing import List, Dict
from dataclasses import dataclass
import statistics
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from decision_engine.core import DecisionEngine

logger = logging.getLogger(__name__)


@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æžœ"""
    mode: str
    total_time: float
    avg_time: float
    min_time: float
    max_time: float
    std_dev: float
    throughput: float  # å†³ç­–æ•°/ç§’


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self, num_runs: int = 10):
        self.num_runs = num_runs
        self.test_symbols = [
            '000001.SZ', '000002.SZ', '600000.SH', '600036.SH',
            '000651.SZ', '600519.SH', '601318.SH', '000858.SZ',
            '002594.SZ', '601398.SH'
        ]
        self.test_date = '2024-06-30'
    
    async def run_benchmark(self) -> Dict[str, BenchmarkResult]:
        """è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•"""
        logger.info("=" * 70)
        logger.info("ðŸš€ Qilin Stack æ€§èƒ½åŸºå‡†æµ‹è¯•")
        logger.info("=" * 70)
        logger.info("é…ç½®:")
        logger.info(f"  - æµ‹è¯•è‚¡ç¥¨æ•°: {len(self.test_symbols)}")
        logger.info(f"  - æµ‹è¯•è½®æ¬¡: {self.num_runs}")
        logger.info(f"  - æµ‹è¯•æ—¥æœŸ: {self.test_date}")
        
        results = {}
        
        # 1. ä¸²è¡Œæ¨¡å¼æµ‹è¯•
        logger.info("ðŸ“Š æµ‹è¯•1: ä¸²è¡Œæ¨¡å¼ï¼ˆæ— ä¼˜åŒ–ï¼‰")
        logger.info("-" * 70)
        results['sequential'] = await self._benchmark_mode(enable_performance=False)
        self._print_result(results['sequential'])
        
        # 2. å¹¶è¡Œæ¨¡å¼æµ‹è¯•
        logger.info("ðŸ“Š æµ‹è¯•2: å¹¶è¡Œæ¨¡å¼ï¼ˆå¹¶å‘ä¼˜åŒ–ï¼‰")
        logger.info("-" * 70)
        results['parallel'] = await self._benchmark_mode(enable_performance=True)
        self._print_result(results['parallel'])
        
        # 3. å¯¹æ¯”åˆ†æž
        logger.info("ðŸ“ˆ æ€§èƒ½å¯¹æ¯”åˆ†æž")
        logger.info("=" * 70)
        self._compare_results(results['sequential'], results['parallel'])
        
        return results
    
    async def _benchmark_mode(self, enable_performance: bool) -> BenchmarkResult:
        """æµ‹è¯•ç‰¹å®šæ¨¡å¼"""
        mode = "å¹¶è¡Œ" if enable_performance else "ä¸²è¡Œ"
        times = []
        
        for i in range(self.num_runs):
            # åˆ›å»ºæ–°å¼•æ“Žå®žä¾‹
            engine = DecisionEngine(enable_performance=enable_performance)
            
            # æµ‹é‡æ—¶é—´
            start_time = time.time()
            decisions = await engine.make_decisions(self.test_symbols, self.test_date)
            elapsed_time = time.time() - start_time
            
            times.append(elapsed_time)
            
            # æ˜¾ç¤ºè¿›åº¦
            logger.info(f"  è½®æ¬¡ {i+1}/{self.num_runs}: {elapsed_time:.3f}ç§’ ({len(decisions)}ä¸ªå†³ç­–)")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_time = sum(times)
        avg_time = statistics.mean(times)
        min_time = min(times)
        max_time = max(times)
        std_dev = statistics.stdev(times) if len(times) > 1 else 0.0
        throughput = len(self.test_symbols) / avg_time
        
        return BenchmarkResult(
            mode=mode,
            total_time=total_time,
            avg_time=avg_time,
            min_time=min_time,
            max_time=max_time,
            std_dev=std_dev,
            throughput=throughput
        )
    
    def _print_result(self, result: BenchmarkResult):
        """æ‰“å°å•ä¸ªç»“æžœ"""
        lines = [
            f"æ¨¡å¼: {result.mode}",
            f"æ€»æ—¶é—´: {result.total_time:.3f}ç§’",
            f"å¹³å‡æ—¶é—´: {result.avg_time:.3f}ç§’",
            f"æœ€å°æ—¶é—´: {result.min_time:.3f}ç§’",
            f"æœ€å¤§æ—¶é—´: {result.max_time:.3f}ç§’",
            f"æ ‡å‡†å·®: {result.std_dev:.3f}ç§’",
            f"åžåé‡: {result.throughput:.2f} å†³ç­–/ç§’",
        ]
        logger.info("\n".join(lines))
    
    def _compare_results(self, seq: BenchmarkResult, par: BenchmarkResult):
        """å¯¹æ¯”ç»“æžœ"""
        speedup = seq.avg_time / par.avg_time
        time_saved = seq.avg_time - par.avg_time
        improvement = (1 - par.avg_time / seq.avg_time) * 100
        
        logger.info("ä¸²è¡Œæ¨¡å¼:")
        logger.info(f"  å¹³å‡æ—¶é—´: {seq.avg_time:.3f}ç§’")
        logger.info(f"  åžåé‡: {seq.throughput:.2f} å†³ç­–/ç§’")
        
        logger.info("å¹¶è¡Œæ¨¡å¼:")
        logger.info(f"  å¹³å‡æ—¶é—´: {par.avg_time:.3f}ç§’")
        logger.info(f"  åžåé‡: {par.throughput:.2f} å†³ç­–/ç§’")
        
        logger.info("æ€§èƒ½æå‡:")
        logger.info(f"  âš¡ åŠ é€Ÿæ¯”: {speedup:.2f}x")
        logger.info(f"  â±ï¸  èŠ‚çœæ—¶é—´: {time_saved:.3f}ç§’ ({improvement:.1f}%)")
        logger.info(f"  ðŸ“Š åžåé‡æå‡: {(par.throughput/seq.throughput-1)*100:.1f}%")
        
        # åˆ¤æ–­
        if speedup >= 2.5:
            emoji = "ðŸ†"
            comment = "å“è¶Šï¼å¹¶å‘ä¼˜åŒ–æ•ˆæžœæ˜¾è‘—"
        elif speedup >= 1.5:
            emoji = "âœ…"
            comment = "è‰¯å¥½ï¼Œå¹¶å‘ä¼˜åŒ–æœ‰æ˜Žæ˜¾æ•ˆæžœ"
        elif speedup >= 1.2:
            emoji = "ðŸ‘"
            comment = "ä¸é”™ï¼Œæœ‰ä¸€å®šä¼˜åŒ–æ•ˆæžœ"
        else:
            emoji = "âš ï¸"
            comment = "ä¼˜åŒ–æ•ˆæžœæœ‰é™ï¼Œå¯èƒ½å—IOé™åˆ¶"
        
        logger.info(f"{emoji} è¯„ä»·: {comment}")


async def quick_test():
    """å¿«é€Ÿæµ‹è¯•"""
    logger.info("ðŸ”¬ å¿«é€Ÿæ€§èƒ½æµ‹è¯•ï¼ˆ3è½®ï¼‰")
    
    benchmark = PerformanceBenchmark(num_runs=3)
    await benchmark.run_benchmark()


async def full_test():
    """å®Œæ•´æµ‹è¯•"""
    logger.info("ðŸ”¬ å®Œæ•´æ€§èƒ½æµ‹è¯•ï¼ˆ10è½®ï¼‰")
    
    benchmark = PerformanceBenchmark(num_runs=10)
    await benchmark.run_benchmark()


async def stress_test():
    """åŽ‹åŠ›æµ‹è¯•"""
    logger.info("ðŸ’ª åŽ‹åŠ›æµ‹è¯•ï¼ˆå¤§é‡è‚¡ç¥¨ï¼‰")
    
    # ç”Ÿæˆ100åªè‚¡ç¥¨
    symbols = [f"{i:06d}.SZ" for i in range(1, 101)]
    
    logger.info(f"æµ‹è¯•{len(symbols)}åªè‚¡ç¥¨...")
    
    # ä¸²è¡Œæ¨¡å¼
    engine_seq = DecisionEngine(enable_performance=False)
    start = time.time()
    decisions_seq = await engine_seq.make_decisions(symbols, '2024-06-30')
    time_seq = time.time() - start
    logger.info(f"ä¸²è¡Œæ¨¡å¼: {time_seq:.2f}ç§’")
    
    # å¹¶è¡Œæ¨¡å¼
    engine_par = DecisionEngine(enable_performance=True)
    start = time.time()
    decisions_par = await engine_par.make_decisions(symbols, '2024-06-30')
    time_par = time.time() - start
    logger.info(f"å¹¶è¡Œæ¨¡å¼: {time_par:.2f}ç§’")
    
    # å¯¹æ¯”
    speedup = time_seq / time_par
    logger.info(f"åŠ é€Ÿæ¯”: {speedup:.2f}x")


if __name__ == '__main__':
    import sys
    
    if len(sys.argv) > 1:
        mode = sys.argv[1]
        if mode == 'quick':
            asyncio.run(quick_test())
        elif mode == 'full':
            asyncio.run(full_test())
        elif mode == 'stress':
            asyncio.run(stress_test())
        else:
            from app.core.logging_setup import setup_logging
            setup_logging()
            logger.info("ç”¨æ³•: python benchmark.py [quick|full|stress]")
    else:
        # é»˜è®¤å¿«é€Ÿæµ‹è¯•
        from app.core.logging_setup import setup_logging
        setup_logging()
        asyncio.run(quick_test())
