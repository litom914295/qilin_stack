#!/usr/bin/env python
"""
æ£€æŸ¥å¹¶å»ºè®®ä¿®å¤ RD-Agent çš„ç¯å¢ƒå˜é‡é…ç½®
"""

import os
from pathlib import Path
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
env_path = Path("G:/test/qilin_stack/.env")
load_dotenv(env_path)


def check_rdagent_env():
    """æ£€æŸ¥ RD-Agent æ‰€éœ€çš„ç¯å¢ƒå˜é‡"""
    
    print("=" * 70)
    print("ğŸ” æ£€æŸ¥ RD-Agent ç¯å¢ƒå˜é‡é…ç½®")
    print("=" * 70)
    
    # RD-Agent éœ€è¦çš„ç¯å¢ƒå˜é‡
    required_vars = {
        "CHAT_MODEL": "èŠå¤©æ¨¡å‹ (å¿…éœ€)",
        "EMBEDDING_MODEL": "åµŒå…¥æ¨¡å‹ (å¿…éœ€)",
    }
    
    # DeepSeek ç›¸å…³ç¯å¢ƒå˜é‡
    deepseek_vars = {
        "DEEPSEEK_API_KEY": "DeepSeek APIå¯†é’¥",
        "OPENAI_API_KEY": "OpenAI APIå¯†é’¥ (æˆ–DeepSeekå¯†é’¥)",
        "OPENAI_API_BASE": "APIåŸºç¡€URL",
    }
    
    # å…¶ä»–å¯é€‰ç¯å¢ƒå˜é‡
    optional_vars = {
        "LITELLM_PROXY_API_KEY": "Embeddingä»£ç†å¯†é’¥ (å¯é€‰)",
        "LITELLM_PROXY_API_BASE": "Embeddingä»£ç†URL (å¯é€‰)",
        "REASONING_THINK_RM": "æ¨ç†æ€è€ƒæ¨¡å¼ (å¯é€‰)",
    }
    
    print("\nğŸ“‹ å½“å‰ç¯å¢ƒå˜é‡çŠ¶æ€:\n")
    
    # æ£€æŸ¥å¿…éœ€å˜é‡
    print("ğŸ”´ å¿…éœ€å˜é‡:")
    missing_required = []
    for var, desc in required_vars.items():
        value = os.getenv(var)
        if value:
            print(f"  âœ… {var}: {desc}")
            print(f"     å½“å‰å€¼: {value}")
        else:
            print(f"  âŒ {var}: {desc} - æœªè®¾ç½®")
            missing_required.append(var)
    
    # æ£€æŸ¥ DeepSeek å˜é‡
    print("\nğŸŸ¡ DeepSeek ç›¸å…³å˜é‡:")
    for var, desc in deepseek_vars.items():
        value = os.getenv(var)
        if value and value != f"your_{var.lower()}_here":
            # éƒ¨åˆ†éšè—å¯†é’¥
            if "KEY" in var and len(value) > 8:
                display_value = value[:8] + "..." + value[-4:]
            else:
                display_value = value
            print(f"  âœ… {var}: {desc}")
            print(f"     å½“å‰å€¼: {display_value}")
        else:
            print(f"  âš ï¸  {var}: {desc} - æœªè®¾ç½®æˆ–ä¸ºé»˜è®¤å€¼")
    
    # æ£€æŸ¥å¯é€‰å˜é‡
    print("\nğŸŸ¢ å¯é€‰å˜é‡:")
    for var, desc in optional_vars.items():
        value = os.getenv(var)
        if value:
            print(f"  âœ… {var}: {desc}")
            print(f"     å½“å‰å€¼: {value}")
        else:
            print(f"  âšª {var}: {desc} - æœªè®¾ç½® (å¯é€‰)")
    
    # åˆ†æé…ç½®é—®é¢˜
    print("\n" + "=" * 70)
    print("ğŸ“Š é…ç½®åˆ†æ")
    print("=" * 70)
    
    issues = []
    suggestions = []
    
    # æ£€æŸ¥ CHAT_MODEL
    chat_model = os.getenv("CHAT_MODEL")
    if not chat_model:
        issues.append("âŒ ç¼ºå°‘ CHAT_MODEL")
        suggestions.append("éœ€è¦è®¾ç½® CHAT_MODEL=deepseek/deepseek-chat")
    elif chat_model == "deepseek-chat":
        issues.append("âš ï¸  CHAT_MODEL æ ¼å¼ä¸æ­£ç¡®")
        suggestions.append("RD-Agent ä½¿ç”¨ LiteLLMï¼Œéœ€è¦æ”¹ä¸º: CHAT_MODEL=deepseek/deepseek-chat")
    
    # æ£€æŸ¥ EMBEDDING_MODEL
    embedding_model = os.getenv("EMBEDDING_MODEL")
    if not embedding_model:
        issues.append("âŒ ç¼ºå°‘ EMBEDDING_MODEL")
        suggestions.append("DeepSeek æ²¡æœ‰ embedding æ¨¡å‹ï¼Œéœ€è¦ä½¿ç”¨ç¬¬ä¸‰æ–¹")
        suggestions.append("æ¨è: EMBEDDING_MODEL=litellm_proxy/BAAI/bge-m3")
        suggestions.append("å¹¶é…ç½® LITELLM_PROXY_API_KEY å’Œ LITELLM_PROXY_API_BASE")
    
    # æ£€æŸ¥ DEEPSEEK_API_KEY
    deepseek_key = os.getenv("DEEPSEEK_API_KEY")
    openai_key = os.getenv("OPENAI_API_KEY")
    
    if not deepseek_key:
        if openai_key and openai_key.startswith("sk-"):
            issues.append("âš ï¸  ä½¿ç”¨ OPENAI_API_KEY å­˜å‚¨ DeepSeek å¯†é’¥")
            suggestions.append("å»ºè®®æ”¹ä¸ºä½¿ç”¨ DEEPSEEK_API_KEY æ›´æ¸…æ™°")
        else:
            issues.append("âŒ ç¼ºå°‘ DeepSeek API å¯†é’¥")
    
    # è¾“å‡ºé—®é¢˜å’Œå»ºè®®
    if issues:
        print("\nâš ï¸  å‘ç°çš„é—®é¢˜:")
        for issue in issues:
            print(f"  {issue}")
    
    if suggestions:
        print("\nğŸ’¡ ä¿®å¤å»ºè®®:")
        for i, suggestion in enumerate(suggestions, 1):
            print(f"  {i}. {suggestion}")
    
    # ç”Ÿæˆæ¨èé…ç½®
    print("\n" + "=" * 70)
    print("ğŸ“ æ¨èçš„ .env é…ç½® (é’ˆå¯¹ RD-Agent + DeepSeek)")
    print("=" * 70)
    print("""
# RD-Agent Chat Model (ä½¿ç”¨ DeepSeek)
CHAT_MODEL=deepseek/deepseek-chat
DEEPSEEK_API_KEY=sk-04104c2d50864c30b307e6f6cfdf8fb4

# RD-Agent Embedding Model (DeepSeekæ²¡æœ‰embeddingï¼Œä½¿ç”¨SiliconFlow)
EMBEDDING_MODEL=litellm_proxy/BAAI/bge-m3
LITELLM_PROXY_API_KEY=<ä½ çš„SiliconFlowå¯†é’¥>
LITELLM_PROXY_API_BASE=https://api.siliconflow.cn/v1

# DeepSeek æ¨ç†æ¨¡å‹è®¾ç½® (å¦‚æœä½¿ç”¨ deepseek-reasoner)
# REASONING_THINK_RM=True

# Clash ä»£ç†é…ç½® (å¦‚éœ€è¦)
HTTP_PROXY=http://127.0.0.1:7890
HTTPS_PROXY=http://127.0.0.1:7890
ALL_PROXY=http://127.0.0.1:7890

# ä¿ç•™åŸæœ‰é…ç½®ç”¨äºå…¶ä»–ç³»ç»Ÿ
OPENAI_API_KEY=sk-04104c2d50864c30b307e6f6cfdf8fb4
OPENAI_API_BASE=https://api.deepseek.com
LLM_PROVIDER=openai
LLM_MODEL=deepseek-chat
""")
    
    print("\n" + "=" * 70)
    print("âš ï¸  é‡è¦è¯´æ˜")
    print("=" * 70)
    print("""
1. RD-Agent ä½¿ç”¨ LiteLLM åç«¯ï¼Œéœ€è¦ç‰¹å®šçš„æ¨¡å‹æ ¼å¼:
   - DeepSeek: deepseek/deepseek-chat (ä¸æ˜¯ deepseek-chat)
   - OpenAI: gpt-4o (ç›´æ¥å†™æ¨¡å‹å)

2. DeepSeek æ²¡æœ‰ embedding æ¨¡å‹ï¼Œéœ€è¦é…ç½®ç¬¬ä¸‰æ–¹:
   - æ¨èä½¿ç”¨ SiliconFlow çš„ BAAI/bge-m3 æ¨¡å‹
   - éœ€è¦æ³¨å†Œ SiliconFlow å¹¶è·å– API key

3. å½“å‰é…ç½®ä¿ç•™äº†åŸæœ‰å˜é‡ï¼Œä¸ä¼šå½±å“å…¶ä»–ç³»ç»Ÿä½¿ç”¨

4. æ·»åŠ æ–°é…ç½®åï¼Œè¿è¡Œæµ‹è¯•:
   rdagent health_check
""")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ SiliconFlow å¯†é’¥
    if not os.getenv("LITELLM_PROXY_API_KEY"):
        print("\nğŸ”— è·å– SiliconFlow API Key:")
        print("   è®¿é—®: https://cloud.siliconflow.cn/")
        print("   æ³¨å†Œå¹¶åœ¨æ§åˆ¶å°è·å– API Key")


if __name__ == '__main__':
    check_rdagent_env()
